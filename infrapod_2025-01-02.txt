
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:06.780]   - Well, welcome to the InfraPod.
[00:00:06.780 --> 00:00:09.460]   It's Tim from S&P.C and Ian, let's go.
[00:00:09.460 --> 00:00:12.380]   - This is the Livingston Angel Investor,
[00:00:12.380 --> 00:00:15.660]   pretender of knowing, of knowledge about Infra,
[00:00:15.660 --> 00:00:18.340]   and just overall lover of dev tools.
[00:00:18.340 --> 00:00:19.980]   In the future of how we build apps,
[00:00:19.980 --> 00:00:24.500]   Tim, how excited are you to do our year in review recap?
[00:00:24.500 --> 00:00:26.560]   Both of the podcasts, but just Infra
[00:00:26.560 --> 00:00:29.380]   and all of the fun that's occurred in the last year.
[00:00:29.380 --> 00:00:32.140]   - Yeah, you know, I don't think we ever really plan out
[00:00:32.140 --> 00:00:34.020]   our episodes at all, right?
[00:00:34.020 --> 00:00:36.860]   But the themes that we go talk about
[00:00:36.860 --> 00:00:38.860]   and the things that we just keep recurring
[00:00:38.860 --> 00:00:39.980]   in all our conversations,
[00:00:39.980 --> 00:00:42.820]   seems to get to pretty much like a pattern
[00:00:42.820 --> 00:00:45.100]   I would have even observing, so.
[00:00:45.100 --> 00:00:46.980]   I'm excited, you know?
[00:00:46.980 --> 00:00:49.580]   We're excited, let's just do this, yeah?
[00:00:49.580 --> 00:00:51.140]   - Do this thing, cool.
[00:00:51.140 --> 00:00:53.780]   Well, on this episode, we're gonna do,
[00:00:53.780 --> 00:00:56.020]   we'll talk a little bit about our favorite moments,
[00:00:56.020 --> 00:00:58.580]   but more importantly, our takeaways from the last year.
[00:00:58.580 --> 00:00:59.660]   Like what have we learned?
[00:00:59.660 --> 00:01:00.500]   What have we noticed?
[00:01:00.500 --> 00:01:01.340]   What are the big trends?
[00:01:01.340 --> 00:01:03.020]   What are the things we think that are cool?
[00:01:03.020 --> 00:01:05.620]   And we'll finish you off with a little prediction
[00:01:05.620 --> 00:01:07.740]   on what we think is gonna happen within front apps
[00:01:07.740 --> 00:01:10.060]   and everything in 2025 based on our purview,
[00:01:10.060 --> 00:01:10.900]   and where we end up.
[00:01:10.900 --> 00:01:13.660]   And, you know, I think that's all pretty spicy
[00:01:13.660 --> 00:01:16.460]   and a nice little holiday snack
[00:01:16.460 --> 00:01:18.260]   for all of our great listeners out there.
[00:01:18.260 --> 00:01:21.340]   So Tim, what's your favorite moment over the last year?
[00:01:21.340 --> 00:01:23.940]   I think we've put out like somehow 20 episodes
[00:01:23.940 --> 00:01:26.100]   or something, where are we sitting?
[00:01:26.100 --> 00:01:28.180]   Face source from your favorite moments.
[00:01:28.180 --> 00:01:33.180]   My favorite moment so far is when we got Guy
[00:01:33.180 --> 00:01:36.980]   to talk about what he's doing and Tesla.
[00:01:36.980 --> 00:01:40.700]   Like to me, like the death of what he is able to describe.
[00:01:40.700 --> 00:01:44.420]   And sort of like the both things he's doing,
[00:01:44.420 --> 00:01:46.820]   like it's just so memorable for me, you know?
[00:01:46.820 --> 00:01:49.500]   'Cause getting Guy on, of course, he has a company
[00:01:49.500 --> 00:01:53.500]   that raises a lot of money, but on the flip side,
[00:01:53.500 --> 00:01:56.780]   seeing somebody with that background
[00:01:56.780 --> 00:01:59.580]   and sort of like the ability to kind of change the world.
[00:01:59.580 --> 00:02:02.460]   In that way, you believe there might be something
[00:02:02.460 --> 00:02:04.300]   really happening, right?
[00:02:04.300 --> 00:02:05.780]   And not saying that all the other guests are not,
[00:02:05.780 --> 00:02:07.540]   but they're all building something really interesting,
[00:02:07.540 --> 00:02:10.860]   but like he's yet to sit on some new journey here.
[00:02:10.860 --> 00:02:12.620]   And so that's pretty memorable for me,
[00:02:12.620 --> 00:02:14.980]   'cause I remember we were chatting about like, hey,
[00:02:14.980 --> 00:02:17.300]   you know, let's get a little bit more prepared.
[00:02:17.300 --> 00:02:18.660]   (laughs)
[00:02:18.660 --> 00:02:21.220]   Get a little bit more ready.
[00:02:21.220 --> 00:02:23.140]   For something that becomes more memorable, right?
[00:02:23.140 --> 00:02:25.460]   Or like, okay, this is a much bigger deal.
[00:02:25.460 --> 00:02:26.700]   Let's get ready to do it.
[00:02:26.700 --> 00:02:28.060]   That's why I didn't disappoint at all,
[00:02:28.060 --> 00:02:29.740]   like talking to Guy in that episode.
[00:02:29.740 --> 00:02:32.100]   It was the longest episode we had.
[00:02:32.100 --> 00:02:35.420]   Like it was an hour fully, we recorded longer than an hour,
[00:02:35.420 --> 00:02:36.260]   right?
[00:02:36.260 --> 00:02:38.020]   We cut it down to like 50ish minutes.
[00:02:38.020 --> 00:02:39.500]   Time flew fast on that one.
[00:02:39.500 --> 00:02:42.060]   I didn't feel like it was a long chatter.
[00:02:42.060 --> 00:02:42.900]   - Not at all.
[00:02:42.900 --> 00:02:44.740]   I mean, Guy's phenomenal storyteller.
[00:02:44.740 --> 00:02:45.740]   I think with Ona Dote,
[00:02:45.740 --> 00:02:48.660]   one of the best storytellers we have
[00:02:48.660 --> 00:02:50.580]   in like DevTools infrastructure
[00:02:50.580 --> 00:02:51.740]   in terms of the way that he thinks
[00:02:51.740 --> 00:02:54.100]   and that democracy creates his ability to communicate it.
[00:02:54.100 --> 00:02:55.060]   I love that episode.
[00:02:55.060 --> 00:02:57.140]   And I think the thing that's so interesting
[00:02:57.140 --> 00:02:59.140]   with what Guy is doing, what he's really talking about
[00:02:59.140 --> 00:03:01.580]   is like the interface change
[00:03:01.580 --> 00:03:04.020]   and the trust required as a result
[00:03:04.020 --> 00:03:06.460]   that like enable the interface change to occur
[00:03:06.460 --> 00:03:10.580]   and what that interface change from the IDE to the specs
[00:03:10.580 --> 00:03:12.500]   or from a little higher level abstraction
[00:03:12.500 --> 00:03:14.300]   that comes from moving away from structured code
[00:03:14.300 --> 00:03:17.540]   to something that's less formal, more readable,
[00:03:17.540 --> 00:03:19.660]   more human digestible,
[00:03:19.660 --> 00:03:21.180]   or just sort of this idea of the specs
[00:03:21.180 --> 00:03:22.900]   and this current composite system to specs.
[00:03:22.900 --> 00:03:24.260]   And with AI that read the spec
[00:03:24.260 --> 00:03:25.940]   and then maintain the system for you,
[00:03:25.940 --> 00:03:28.860]   it is both just like radically different
[00:03:28.860 --> 00:03:31.020]   than the way things work,
[00:03:31.020 --> 00:03:34.740]   but also is a very democratizing mission if true.
[00:03:34.740 --> 00:03:35.580]   And so I agree with you.
[00:03:35.580 --> 00:03:37.380]   That was like a pretty great episode
[00:03:37.380 --> 00:03:40.740]   and very interesting endeavor in infrastructure
[00:03:40.740 --> 00:03:42.700]   and very bold and something you totally expect
[00:03:42.700 --> 00:03:44.140]   from a third time founder.
[00:03:44.140 --> 00:03:44.980]   - Yeah.
[00:03:44.980 --> 00:03:46.660]   And what's your favorite movement then?
[00:03:46.660 --> 00:03:49.380]   - Mine is, it's hard to follow up on Guy.
[00:03:49.380 --> 00:03:51.460]   So, but look, I think one of my favorite episodes
[00:03:51.460 --> 00:03:54.780]   was when we talked to Akshay Shah, CTO of Ba.
[00:03:54.780 --> 00:03:57.660]   You know, it was such a great episode.
[00:03:57.660 --> 00:04:01.220]   It had a very specific view on the world
[00:04:01.220 --> 00:04:03.420]   and why the world should be the way that it is.
[00:04:03.420 --> 00:04:05.380]   And it was just one of the episodes
[00:04:05.380 --> 00:04:07.100]   where I got a lot of feedback from people who were like,
[00:04:07.100 --> 00:04:07.940]   "This is incredible."
[00:04:07.940 --> 00:04:09.260]   Like this guy gets it.
[00:04:09.260 --> 00:04:10.620]   It was so clearly delivered.
[00:04:10.620 --> 00:04:11.460]   It was interesting.
[00:04:11.460 --> 00:04:12.620]   It was intriguing.
[00:04:12.620 --> 00:04:14.220]   And I think what we've learned,
[00:04:14.220 --> 00:04:15.220]   and we were just going through
[00:04:15.220 --> 00:04:17.460]   what all did we even talk about this year,
[00:04:17.460 --> 00:04:19.140]   what we've learned is like the best guests
[00:04:19.140 --> 00:04:20.260]   and we said this on another episode.
[00:04:20.260 --> 00:04:21.700]   The best guests are ones that just come in
[00:04:21.700 --> 00:04:24.780]   with something that's really bold, opinionated,
[00:04:24.780 --> 00:04:28.500]   but backed up with experience and story, incredible data
[00:04:28.500 --> 00:04:29.660]   based on their experience
[00:04:29.660 --> 00:04:32.140]   about why they believe the future is the way that it should be.
[00:04:32.140 --> 00:04:35.260]   And so that's by far one of my most favorite moments.
[00:04:35.260 --> 00:04:37.100]   - I remember that episode is,
[00:04:37.100 --> 00:04:39.900]   he said this is very first time doing a podcast, right?
[00:04:39.900 --> 00:04:41.700]   - I don't actually believe that.
[00:04:41.700 --> 00:04:43.500]   - I was like, I never done a podcast before.
[00:04:43.500 --> 00:04:45.220]   I'm like, I can't tell, right?
[00:04:45.220 --> 00:04:49.340]   He was pretty poised and didn't really have any like retakes
[00:04:49.340 --> 00:04:50.260]   or edits needed.
[00:04:50.260 --> 00:04:52.020]   He just gone through it.
[00:04:52.020 --> 00:04:53.660]   I guess he's just a natural, right?
[00:04:53.660 --> 00:04:55.620]   No, rehearsing or practice needed at all
[00:04:55.620 --> 00:04:59.100]   to be a good podcast guest or host if you wanna do it.
[00:04:59.100 --> 00:05:00.540]   Anyway, he's amazing.
[00:05:00.540 --> 00:05:03.340]   I think people that really spend a lot of time
[00:05:03.340 --> 00:05:05.940]   digging into like deep-ended infrastructure.
[00:05:05.940 --> 00:05:08.380]   When you ask them questions that we're asking,
[00:05:08.380 --> 00:05:09.580]   they get more and more excited
[00:05:09.580 --> 00:05:11.300]   and easier to answer all this stuff
[00:05:11.300 --> 00:05:12.700]   that we're workers about to.
[00:05:12.700 --> 00:05:17.700]   So yeah, that's my memorable thing about that episode at least.
[00:05:18.620 --> 00:05:20.500]   - Should we dig into our takeaways?
[00:05:20.500 --> 00:05:23.380]   Like the non full stop, like prognosticating,
[00:05:23.380 --> 00:05:25.060]   how amazing we think our podcast is
[00:05:25.060 --> 00:05:26.500]   and start talking about what we think we learned
[00:05:26.500 --> 00:05:28.020]   or as we learned.
[00:05:28.020 --> 00:05:31.620]   - I guess we had to drop some stuff we learned, right?
[00:05:31.620 --> 00:05:32.620]   - Yeah.
[00:05:32.620 --> 00:05:34.140]   - So I'll start then, I'll start.
[00:05:34.140 --> 00:05:37.020]   So I think, you know, we're gonna each share two things, right?
[00:05:37.020 --> 00:05:40.100]   And I'll probably start with this first.
[00:05:40.100 --> 00:05:42.540]   I wanna leave probably the more interesting one
[00:05:42.540 --> 00:05:43.860]   and the later one.
[00:05:43.860 --> 00:05:45.740]   And it's not no surprise, right?
[00:05:45.740 --> 00:05:48.580]   Obviously the people we talked through this year,
[00:05:48.580 --> 00:05:51.860]   and even last year has been so much about the future
[00:05:51.860 --> 00:05:55.140]   of a data infrastructure around how S3
[00:05:55.140 --> 00:05:56.940]   is becoming so much of the standard place
[00:05:56.940 --> 00:05:58.660]   where all the data is stored.
[00:05:58.660 --> 00:05:59.780]   And so there's nothing new.
[00:05:59.780 --> 00:06:02.140]   We remember like, you know, this year,
[00:06:02.140 --> 00:06:05.580]   even talking to modal, you know, talking to LANs
[00:06:05.580 --> 00:06:08.620]   and obviously talking to Tigris data.
[00:06:08.620 --> 00:06:12.020]   Two years ago, we're talking to Warpstream, you know,
[00:06:12.020 --> 00:06:17.020]   CDRDB and we got Chris materialized view on all surrounded
[00:06:17.340 --> 00:06:21.860]   on the future of data is no longer,
[00:06:21.860 --> 00:06:26.340]   has to be stored in these separate databases and silo.
[00:06:26.340 --> 00:06:30.900]   We're gonna see data actually much, much more standardized
[00:06:30.900 --> 00:06:33.740]   in the cloud data lake themselves.
[00:06:33.740 --> 00:06:36.300]   And so the future of the data infrastructure
[00:06:36.300 --> 00:06:37.900]   is looking at all these companies.
[00:06:37.900 --> 00:06:40.780]   Like most of the computer doesn't really care where data is,
[00:06:40.780 --> 00:06:43.580]   but so much of the customers are directly going to S3.
[00:06:43.580 --> 00:06:46.940]   LANs is an S3 native data store, Warpstream,
[00:06:46.940 --> 00:06:48.260]   Tigris building a new S3.
[00:06:48.260 --> 00:06:50.140]   Like all this is just assuming.
[00:06:50.140 --> 00:06:52.140]   Next time I still need to use my data
[00:06:52.140 --> 00:06:56.980]   and I do like a export, postgres, whatever thingy
[00:06:56.980 --> 00:06:58.980]   to finally go to a Kafka, whatever.
[00:06:58.980 --> 00:07:00.940]   Like that whole world of like,
[00:07:00.940 --> 00:07:05.860]   I need thousands of ETL and Kafkas to do anything.
[00:07:05.860 --> 00:07:07.220]   It's probably changing.
[00:07:07.220 --> 00:07:09.940]   And it's quite cool to actually see
[00:07:09.940 --> 00:07:13.380]   that our guests is pretty much assumed it's gonna happen.
[00:07:13.380 --> 00:07:15.620]   If you wanna actually build a company in this space,
[00:07:15.620 --> 00:07:18.700]   you kinda have to be assuming certain things gonna happen
[00:07:18.700 --> 00:07:20.180]   and bet on them, right?
[00:07:20.180 --> 00:07:21.980]   Anyway, this is not even a new topic,
[00:07:21.980 --> 00:07:24.220]   but it's really cool to confirm
[00:07:24.220 --> 00:07:27.380]   people are now wanting data not to go everywhere.
[00:07:27.380 --> 00:07:30.060]   We wanted to have a much more simpler stack.
[00:07:30.060 --> 00:07:32.780]   The simpler stack requires, unfortunately,
[00:07:32.780 --> 00:07:33.860]   a new data infrastructure
[00:07:33.860 --> 00:07:38.300]   'cause all the older stuff can't go interoperable at all.
[00:07:38.300 --> 00:07:40.140]   So that's kinda fun for me.
[00:07:40.140 --> 00:07:40.980]   - It's super fun.
[00:07:40.980 --> 00:07:42.180]   I mean, it's fun for me too.
[00:07:42.180 --> 00:07:43.700]   I think one of the things that's cool
[00:07:43.700 --> 00:07:45.060]   is you kinda have like a prescription
[00:07:45.060 --> 00:07:47.580]   of how to actually build data infrastructure
[00:07:47.580 --> 00:07:51.180]   in a way that like customers will be capable of adopting it,
[00:07:51.180 --> 00:07:52.020]   right?
[00:07:52.020 --> 00:07:55.500]   Like even with Warpstream, Kafka compatible APIs and S3,
[00:07:55.500 --> 00:07:58.140]   there are starting to become recipes
[00:07:58.140 --> 00:08:00.500]   for what does it look like
[00:08:00.500 --> 00:08:02.500]   to build data infrastructure and sell it?
[00:08:02.500 --> 00:08:03.740]   You know, it's gotta be S3.
[00:08:03.740 --> 00:08:05.700]   The data plane has to be in their cloud.
[00:08:05.700 --> 00:08:07.300]   Then you have no site control pipeline.
[00:08:07.300 --> 00:08:10.180]   That's cool, but like a lot of these topics,
[00:08:10.180 --> 00:08:12.380]   it's becoming known whereas, you know,
[00:08:12.380 --> 00:08:15.340]   three, five years ago wasn't clear
[00:08:15.340 --> 00:08:16.980]   what was gonna work and what wasn't.
[00:08:16.980 --> 00:08:19.660]   Do you do SaaS first and have it all run inside yours, right?
[00:08:19.660 --> 00:08:21.900]   Like we're starting to get recipes at work,
[00:08:21.900 --> 00:08:24.700]   which means that like broadly the cloud is maturing.
[00:08:24.700 --> 00:08:25.940]   And you know, to be honest,
[00:08:25.940 --> 00:08:28.220]   kind of flows into my next topic,
[00:08:28.220 --> 00:08:31.420]   which is at the end of the day, my biggest takeaway,
[00:08:31.420 --> 00:08:33.140]   and this isn't directly from an episode,
[00:08:33.140 --> 00:08:35.580]   but it's something I've definitely like realized
[00:08:35.580 --> 00:08:38.140]   as a result of having all these different conversations,
[00:08:38.140 --> 00:08:41.940]   is that, you know, the moat that the major cloud vendors have,
[00:08:41.940 --> 00:08:45.340]   which is IAM and the VPC, it's eroding away.
[00:08:45.340 --> 00:08:47.660]   You know, they still got data egress fees for sure, right?
[00:08:47.660 --> 00:08:49.540]   So the data is gonna say in S3.
[00:08:49.540 --> 00:08:52.820]   But like the idea that IAM or the VPC locks you in
[00:08:52.820 --> 00:08:56.580]   to their cloud, which was their original real moat,
[00:08:56.580 --> 00:08:58.340]   in terms of, oh, you know, 'cause security was like,
[00:08:58.340 --> 00:08:59.940]   oh, it has to be an IAM, that's VPC,
[00:08:59.940 --> 00:09:01.380]   that's starting to fall apart.
[00:09:01.380 --> 00:09:03.300]   Because, you know, at the end of the day,
[00:09:03.300 --> 00:09:06.180]   some of these systems, you're giving cross access
[00:09:06.180 --> 00:09:08.940]   into your core systems, cross cloud account, right?
[00:09:08.940 --> 00:09:10.060]   So it's not holistically
[00:09:10.060 --> 00:09:11.500]   owned by your cloud account anymore.
[00:09:11.500 --> 00:09:13.940]   And sometimes those things are, you know,
[00:09:13.940 --> 00:09:17.100]   are runtimes that exist outside AWS or GCP.
[00:09:17.100 --> 00:09:18.380]   They're running on their own thing, right?
[00:09:18.380 --> 00:09:20.660]   We've seen the rise of vendors like Fly,
[00:09:20.660 --> 00:09:23.540]   things like Modal, which are like just absolute rocket ships.
[00:09:23.540 --> 00:09:25.580]   You know, Vercel is gonna be IPO-ing next year,
[00:09:25.580 --> 00:09:26.860]   another absolute rocket ship,
[00:09:26.860 --> 00:09:29.060]   all of that compute, all of that runtime,
[00:09:29.060 --> 00:09:32.140]   lives outside your IAM or your VPC.
[00:09:32.140 --> 00:09:33.900]   And these companies are starting to eat
[00:09:33.900 --> 00:09:35.660]   into the enterprise as well.
[00:09:35.660 --> 00:09:37.900]   And so a big takeaway for me was like,
[00:09:37.900 --> 00:09:40.220]   oh, actually, over the next three to five years,
[00:09:40.220 --> 00:09:41.740]   the way that we build and sell software,
[00:09:41.740 --> 00:09:45.020]   all the things that were like preventative earlier on
[00:09:45.020 --> 00:09:46.940]   in terms of like adopting the way
[00:09:46.940 --> 00:09:47.980]   that I think about architecture,
[00:09:47.980 --> 00:09:50.420]   that's all gonna shift some truly more
[00:09:50.420 --> 00:09:52.700]   of a composite set of vendors.
[00:09:52.700 --> 00:09:55.300]   And there's gonna be a bunch of norms and substrates
[00:09:55.300 --> 00:09:57.380]   and expected practice and how you connect it together.
[00:09:57.380 --> 00:09:59.980]   And so that's one of my big takeaways from this year
[00:09:59.980 --> 00:10:02.860]   is, oh, the pieces are actually starting to fit together.
[00:10:02.860 --> 00:10:05.740]   And we have a clear driver as to why this is occurring.
[00:10:05.740 --> 00:10:09.580]   And it's all about AI, AI, AI, which is more to say,
[00:10:09.580 --> 00:10:12.700]   you wanna add a little bit of intelligence to your app.
[00:10:12.700 --> 00:10:15.460]   We now have, you know, LLMs are easy to,
[00:10:15.460 --> 00:10:17.540]   for anyone to pick up and use and build some
[00:10:17.540 --> 00:10:20.460]   chat-based features from natural language-based feature.
[00:10:20.460 --> 00:10:22.660]   And you wanna build that into your app.
[00:10:22.660 --> 00:10:25.540]   And you wanna do it, whether it's support chatbot
[00:10:25.540 --> 00:10:28.180]   or you're building some type of net new native experience,
[00:10:28.180 --> 00:10:30.580]   you wanna do it because now we have to compete on it.
[00:10:30.580 --> 00:10:32.900]   And so that's the reason for all these enterprises
[00:10:32.900 --> 00:10:34.900]   for the business to make big investment cycles
[00:10:34.900 --> 00:10:36.260]   and to actually adopting these vendors.
[00:10:36.260 --> 00:10:38.060]   So I think that's really exciting.
[00:10:38.060 --> 00:10:42.020]   And also a reason for investment into new infrastructure.
[00:10:42.020 --> 00:10:46.100]   - Yeah, yeah, well, it's a pretty good teaser, sir.
[00:10:46.100 --> 00:10:49.140]   I think this is so interesting
[00:10:49.140 --> 00:10:51.700]   because people assumed back in the day,
[00:10:51.700 --> 00:10:54.260]   either all this stuff, either there's a feature
[00:10:54.260 --> 00:10:57.820]   in the platform or the clouds are just gonna be a couple.
[00:10:57.820 --> 00:11:00.020]   But like I said, like if we're gonna see,
[00:11:00.020 --> 00:11:01.860]   like the not just a versatile modals at the world,
[00:11:01.860 --> 00:11:03.900]   fly at the world, but like specialized clouds
[00:11:03.900 --> 00:11:05.820]   and all the different platforms growing,
[00:11:05.820 --> 00:11:09.660]   even talking to guests today or even mentioning Lambda,
[00:11:09.660 --> 00:11:12.820]   you know, serverless is becoming like a default option.
[00:11:12.820 --> 00:11:15.860]   It's so hard to embed all the old-school identity stuff
[00:11:15.860 --> 00:11:18.740]   everywhere, you know, not even just VPCIMs, right?
[00:11:18.740 --> 00:11:23.740]   Like it's all going to be changed in some way.
[00:11:23.740 --> 00:11:26.460]   I think maybe that will be something we can predict
[00:11:26.460 --> 00:11:28.140]   on how it will change and when.
[00:11:28.140 --> 00:11:31.140]   Talk about AI, AI, AI.
[00:11:31.140 --> 00:11:32.900]   All right, I'll jump into what I wanna,
[00:11:32.900 --> 00:11:35.340]   I think it's something I've really been thinking about
[00:11:35.340 --> 00:11:37.580]   recently, just given the guy's episode
[00:11:37.580 --> 00:11:38.460]   and all the kind of stuff.
[00:11:38.460 --> 00:11:41.180]   Because I think even just like summarizing the episodes,
[00:11:41.180 --> 00:11:43.940]   we talk about AI, but which is pretty much every single
[00:11:43.940 --> 00:11:47.100]   damn one at this point, but right,
[00:11:47.100 --> 00:11:49.740]   the last one we have Modern, we talk about how AI
[00:11:49.740 --> 00:11:51.580]   is changing code refactoring.
[00:11:51.580 --> 00:11:53.940]   And their differentiation was the semantic, right?
[00:11:53.940 --> 00:11:55.660]   The loss of semantic tree, they're able to actually
[00:11:55.660 --> 00:11:58.020]   capture a lot of context to actually generate AI.
[00:11:58.020 --> 00:12:01.380]   Versus just me looking at code and just doing all kind
[00:12:01.380 --> 00:12:05.340]   of code refactoring without all the compiler context, right?
[00:12:05.340 --> 00:12:08.780]   Guy is also kind of arguing, we need a new spec.
[00:12:08.780 --> 00:12:12.380]   We need a new spec to help guide the LLM to know exactly
[00:12:12.380 --> 00:12:14.780]   what is the thing to produce and how to validate them, right?
[00:12:14.780 --> 00:12:16.700]   And all the necessary pieces into it.
[00:12:16.700 --> 00:12:20.260]   Even GPT scripts is trying to add more context
[00:12:20.260 --> 00:12:23.260]   than just me generating just random code.
[00:12:23.260 --> 00:12:26.300]   We need descriptions on how to like guide
[00:12:26.300 --> 00:12:29.420]   and test and add all the functions and stuff like that.
[00:12:29.420 --> 00:12:33.580]   And so like all this conversations one way or another,
[00:12:33.580 --> 00:12:35.700]   my biggest takeaway is like, I think AI,
[00:12:35.700 --> 00:12:39.140]   we all been focusing so much on the LLMs in self these days,
[00:12:39.140 --> 00:12:42.740]   like how big is my parameters, how much context window,
[00:12:42.740 --> 00:12:45.300]   what kind of fancy inference, whatever,
[00:12:45.300 --> 00:12:48.020]   fancy new architecture, all this kind of stuff.
[00:12:48.020 --> 00:12:51.100]   But then the actual thing you wanted to do really,
[00:12:51.100 --> 00:12:53.460]   really well, where you bring into code,
[00:12:53.460 --> 00:12:55.980]   you bring into security, you bring into development,
[00:12:55.980 --> 00:12:57.540]   you bring to whatever, you know,
[00:12:57.540 --> 00:12:59.940]   or natural language answering questions.
[00:12:59.940 --> 00:13:02.100]   What I'm hearing, not even just on our podcast,
[00:13:02.100 --> 00:13:04.740]   but even just outside talking to a bunch of founders.
[00:13:04.740 --> 00:13:09.260]   The thing that really empowers the thing to work with LLMs
[00:13:09.260 --> 00:13:12.780]   is really almost like data structures that's around it.
[00:13:12.780 --> 00:13:16.900]   And these data, these contexts or whatever we think we call them
[00:13:16.900 --> 00:13:21.020]   are the hardest thing to really able to generate and maintain.
[00:13:21.020 --> 00:13:22.460]   We don't even know what it looks like.
[00:13:22.460 --> 00:13:24.300]   We don't really have AI systems everywhere yet.
[00:13:24.300 --> 00:13:26.260]   So we don't even know what a spec looks like, right?
[00:13:26.260 --> 00:13:27.660]   You know, guide, you talked about,
[00:13:27.660 --> 00:13:29.620]   like we probably have a bunch of things there.
[00:13:29.620 --> 00:13:31.380]   It's a community effort, right?
[00:13:31.380 --> 00:13:34.740]   There's probably not gonna be a single, one single file,
[00:13:34.740 --> 00:13:36.220]   you know, that holds everything.
[00:13:36.220 --> 00:13:39.820]   There's gonna be a brand new level of definition
[00:13:39.820 --> 00:13:43.020]   and data structures required to make AI to work.
[00:13:43.020 --> 00:13:47.380]   But just so cool to kind of hear that we look at how AI
[00:13:47.380 --> 00:13:49.980]   actually would become true everywhere,
[00:13:49.980 --> 00:13:52.580]   is that the LLMs will continue to be smaller and cheaper
[00:13:52.580 --> 00:13:54.340]   or whatever they're doing.
[00:13:54.340 --> 00:13:57.780]   But the most important part beyond just the LLM vendors
[00:13:57.780 --> 00:14:00.340]   doing their thing and selling more GPUs
[00:14:00.340 --> 00:14:02.460]   is we actually need to go figure out
[00:14:02.460 --> 00:14:04.220]   for the problem we're solving.
[00:14:04.220 --> 00:14:06.860]   What is this data structure?
[00:14:06.860 --> 00:14:09.820]   We need to start to able to contribute and write to
[00:14:09.820 --> 00:14:12.340]   that can actually able to do way better job
[00:14:12.340 --> 00:14:14.020]   of what we want, right?
[00:14:14.020 --> 00:14:17.380]   Because we need validation or guiding principles
[00:14:17.380 --> 00:14:22.020]   or something context related to help let the LLM see
[00:14:22.020 --> 00:14:24.380]   that this is all the relevant information,
[00:14:24.380 --> 00:14:26.700]   almost like RAG in some level.
[00:14:26.700 --> 00:14:29.860]   But that RAG isn't just stuff all my notion
[00:14:29.860 --> 00:14:31.100]   in Google Docs somewhere, right?
[00:14:31.100 --> 00:14:33.660]   It has to be much, much more specific.
[00:14:33.660 --> 00:14:38.220]   And that information, that context,
[00:14:38.220 --> 00:14:41.980]   will require a very different culture, process,
[00:14:41.980 --> 00:14:45.740]   technology, different compiler that's compiling code
[00:14:45.740 --> 00:14:47.180]   with that getting that data.
[00:14:47.180 --> 00:14:49.860]   It requires a different company, complete week,
[00:14:49.860 --> 00:14:51.700]   trying to come up with a new standard,
[00:14:51.700 --> 00:14:55.060]   requires some level of information or knowledge graph
[00:14:55.060 --> 00:14:57.380]   or whatever thing people have been trying to figure out with.
[00:14:57.380 --> 00:14:58.900]   That is a fun part to me.
[00:14:58.900 --> 00:15:01.340]   Talking to all the practitioners,
[00:15:01.340 --> 00:15:02.980]   it's just a little more refreshing
[00:15:02.980 --> 00:15:04.460]   that we're not just reducing our problems
[00:15:04.460 --> 00:15:06.220]   that you just LLM's, you know?
[00:15:06.220 --> 00:15:09.700]   'Cause I think that's just so boring in my mind.
[00:15:09.700 --> 00:15:12.180]   I just want to keep thinking about like 70 parameters,
[00:15:12.180 --> 00:15:14.660]   800 parameters, and like NeurIPS is the only thing
[00:15:14.660 --> 00:15:15.980]   you should attend and nothing else, right?
[00:15:15.980 --> 00:15:18.260]   Like it seems like our world just reduces down to like
[00:15:18.260 --> 00:15:19.660]   'cause two conferences on earth need to go.
[00:15:19.660 --> 00:15:21.180]   NeurIPS is something else, right?
[00:15:21.180 --> 00:15:22.180]   ICML, right?
[00:15:22.180 --> 00:15:24.340]   Nothing else matters on earth anymore, right?
[00:15:24.340 --> 00:15:27.300]   We're just like, you know, we're in our,
[00:15:27.300 --> 00:15:29.020]   whatever, 30, 40, 50s, you know,
[00:15:29.020 --> 00:15:30.900]   still holding on to Kubernetes.
[00:15:30.900 --> 00:15:32.700]   (laughs)
[00:15:32.700 --> 00:15:35.260]   Grumpy, unfra, oldies, you know?
[00:15:35.260 --> 00:15:40.260]   Like the world still is important around LLM's, you know?
[00:15:40.260 --> 00:15:43.420]   It's just, I don't think we talk about it that much.
[00:15:43.420 --> 00:15:45.580]   - I think that's a great take away.
[00:15:45.580 --> 00:15:48.060]   I mean, that episode of Moderna was just so interesting.
[00:15:48.060 --> 00:15:49.460]   And like one of the things that, you know,
[00:15:49.460 --> 00:15:51.460]   you're really calling out there is,
[00:15:51.460 --> 00:15:53.180]   there's a couple of things we learned this year, right?
[00:15:53.180 --> 00:15:55.100]   We basically exhausted all of the public data
[00:15:55.100 --> 00:15:57.060]   available to train, right?
[00:15:57.060 --> 00:15:58.980]   But more importantly, you know,
[00:15:58.980 --> 00:16:01.300]   I feel like we're broadly in like the pets
[00:16:01.300 --> 00:16:02.980]   not cattle stage of AI.
[00:16:02.980 --> 00:16:06.100]   In the sense that like all the models are massive,
[00:16:06.100 --> 00:16:07.860]   the costs to run them is huge.
[00:16:07.860 --> 00:16:11.180]   The systems are very like not repeatable.
[00:16:11.180 --> 00:16:13.460]   Everyone basically building a one off.
[00:16:13.460 --> 00:16:15.660]   The hardware is hyper specialized,
[00:16:15.660 --> 00:16:19.460]   and we continue to like pursue more specialized hardware.
[00:16:19.460 --> 00:16:22.060]   There's lots of new inference hardware chips
[00:16:22.060 --> 00:16:24.420]   and all those other different things coming down the pipe.
[00:16:24.420 --> 00:16:26.700]   And broadly that's because we're trying to pursue
[00:16:26.700 --> 00:16:29.380]   the ability for like compute and prediction
[00:16:29.380 --> 00:16:31.780]   or the use or inference to be cheap, right?
[00:16:31.780 --> 00:16:34.780]   And so that's the next phase is like,
[00:16:34.780 --> 00:16:37.820]   how do we get these AI systems to be cattle instead of pets?
[00:16:37.820 --> 00:16:40.220]   How do we get them to be repeatable instead of pets?
[00:16:40.220 --> 00:16:42.020]   And that's how I've been thinking about it.
[00:16:42.020 --> 00:16:43.500]   The last couple of years have been like,
[00:16:43.500 --> 00:16:46.060]   okay, let's just scale, scale, scale,
[00:16:46.060 --> 00:16:48.620]   specialized hardware, specialized compute, specialized.
[00:16:48.620 --> 00:16:51.260]   Everything was just tune, tune, tune, tune, tune.
[00:16:51.260 --> 00:16:55.980]   And over time, like what happened in the early 2000s,
[00:16:55.980 --> 00:16:58.180]   you know, Google was a good example of this.
[00:16:58.180 --> 00:17:00.260]   Everyone was deploying specialized hardware.
[00:17:00.260 --> 00:17:02.060]   There are a bunch of pets all over the place
[00:17:02.060 --> 00:17:03.140]   and Google bases like, no, no,
[00:17:03.140 --> 00:17:05.460]   we're just gonna rack and stack commodity hardware.
[00:17:05.460 --> 00:17:08.220]   And we're gonna invest in building, you know,
[00:17:08.220 --> 00:17:10.380]   the software layer that makes it possible
[00:17:10.380 --> 00:17:13.220]   for software to be treated like a bunch of cattle.
[00:17:13.220 --> 00:17:15.260]   And if, you know, the hard disk crashes, whatever,
[00:17:15.260 --> 00:17:16.380]   there's like a thousand other ones
[00:17:16.380 --> 00:17:18.300]   that have a portion of the data someplace
[00:17:18.300 --> 00:17:20.780]   or like they're up there serving compute.
[00:17:20.780 --> 00:17:24.900]   And so it feels like with AI or LLMs and more broadly,
[00:17:24.900 --> 00:17:26.740]   we're at the sort of like pets on a cattle stage
[00:17:26.740 --> 00:17:28.060]   and we need to get to the cattle stage.
[00:17:28.060 --> 00:17:30.580]   And then I think the unique thing that you said is like,
[00:17:30.580 --> 00:17:33.980]   the unique value in AI to the next phase
[00:17:33.980 --> 00:17:35.220]   is still just comes down to like,
[00:17:35.220 --> 00:17:36.620]   what unique data do you have?
[00:17:36.620 --> 00:17:38.900]   More importantly, what's the unique data structure
[00:17:38.900 --> 00:17:41.300]   that you have that enables these systems
[00:17:41.300 --> 00:17:43.700]   to actually like interpret what's going on?
[00:17:43.700 --> 00:17:44.820]   Right, we're still on the phase of like,
[00:17:44.820 --> 00:17:47.540]   how do you model the problem, right?
[00:17:47.540 --> 00:17:49.980]   And I think the lossless tree episode
[00:17:49.980 --> 00:17:52.580]   talking about like code migration is like,
[00:17:52.580 --> 00:17:54.540]   actually we still haven't yet discovered
[00:17:54.540 --> 00:17:56.700]   what the right modeling of the problem is
[00:17:56.700 --> 00:17:58.660]   so we can actually solve it.
[00:17:58.660 --> 00:18:00.580]   And I think that's what I learned at least
[00:18:00.580 --> 00:18:01.700]   from Jonathan and Moderna.
[00:18:01.700 --> 00:18:03.180]   It was like, oh, actually this problem space
[00:18:03.180 --> 00:18:05.460]   is really complicated 'cause it's really a context issue
[00:18:05.460 --> 00:18:07.380]   to even under, to have the context understand
[00:18:07.380 --> 00:18:10.460]   like what a potential good solution even looked like
[00:18:10.460 --> 00:18:12.860]   and then how do you even evaluate what a good solution is?
[00:18:12.860 --> 00:18:15.300]   That was really quite fascinating.
[00:18:15.300 --> 00:18:16.180]   - Yeah, I think so too.
[00:18:16.180 --> 00:18:18.860]   And also like just last footnote is I think
[00:18:18.860 --> 00:18:22.020]   it's almost like we used to have no computers before, right?
[00:18:22.020 --> 00:18:24.900]   So we always have to like human labor everywhere
[00:18:24.900 --> 00:18:27.300]   to like do everything related.
[00:18:27.300 --> 00:18:29.580]   But once we have computer scientists,
[00:18:29.580 --> 00:18:33.420]   you know, since we're able to have any compute primitives,
[00:18:33.420 --> 00:18:36.180]   now we have to like start to think of what is the way
[00:18:36.180 --> 00:18:39.820]   we can even turn this into a set of APIs or programs
[00:18:39.820 --> 00:18:42.500]   to solve shipping, you know, phone calls
[00:18:42.500 --> 00:18:44.140]   and all the sort of like related things
[00:18:44.140 --> 00:18:45.740]   that we only do by hand, right?
[00:18:45.740 --> 00:18:48.060]   And I think AI is almost like the next generation of like,
[00:18:48.060 --> 00:18:51.740]   okay, like AI can enable it to solve this now.
[00:18:51.740 --> 00:18:55.420]   But you know, I can't just throw this to the computers,
[00:18:55.420 --> 00:18:57.420]   computers have no idea what the hell you're talking about, right?
[00:18:57.420 --> 00:19:00.140]   This is AI still have no idea what you're talking about either.
[00:19:00.140 --> 00:19:02.740]   So it's like this new abstraction layer
[00:19:02.740 --> 00:19:05.460]   that needs to be required and that's hard.
[00:19:05.460 --> 00:19:07.540]   I don't think we have an easy answer at all.
[00:19:07.540 --> 00:19:10.740]   And that's the opportunity for so many things of the companies
[00:19:10.740 --> 00:19:14.700]   to come in and actually solve a broader set of problems for them.
[00:19:14.700 --> 00:19:17.020]   - That's a great segue into my final sort of takeaway,
[00:19:17.020 --> 00:19:19.700]   which is, you know, now the neurops has occurred.
[00:19:19.700 --> 00:19:21.700]   I wasn't there, but it's, but like, you know,
[00:19:21.700 --> 00:19:23.740]   with the death of pre-training, like we kind of talked about it.
[00:19:23.740 --> 00:19:25.460]   It's like we've run out of public data.
[00:19:25.460 --> 00:19:29.060]   We've got to the point where more compute and more data
[00:19:29.060 --> 00:19:30.380]   doesn't equal better model.
[00:19:30.380 --> 00:19:32.900]   Like we're at diminishing returns, if not more.
[00:19:32.900 --> 00:19:34.500]   And now it's about like, what are the things
[00:19:34.500 --> 00:19:36.540]   we can do on top of the model, right?
[00:19:36.540 --> 00:19:38.940]   So we kind of have the rise of these composite AI systems.
[00:19:38.940 --> 00:19:40.340]   And this is the best example of being like,
[00:19:40.340 --> 00:19:41.820]   "Oh, one from OpenAI."
[00:19:41.820 --> 00:19:43.660]   She's like, "Oh, we're going to do a bunch of stuff
[00:19:43.660 --> 00:19:46.500]   on top of the model, build a bunch of systems,
[00:19:46.500 --> 00:19:48.460]   bunch of RAG systems, bunch of training,
[00:19:48.460 --> 00:19:50.660]   so it can get better at planning."
[00:19:50.660 --> 00:19:52.460]   So we're going to use these different models
[00:19:52.460 --> 00:19:54.020]   in different ways to generate these plans
[00:19:54.020 --> 00:19:56.460]   and then execute through these plans to drive and answer for you.
[00:19:56.460 --> 00:19:57.700]   So we have some concept like,
[00:19:57.700 --> 00:19:59.300]   we're basically taking chain of thought
[00:19:59.300 --> 00:20:00.780]   and we're going to actually build a system
[00:20:00.780 --> 00:20:01.860]   mode of chain of thought, right?
[00:20:01.860 --> 00:20:04.220]   It's basically what O1 is at the end of the day.
[00:20:04.220 --> 00:20:06.180]   And so we're now in this phase of,
[00:20:06.180 --> 00:20:07.700]   the models will still keep getting better,
[00:20:07.700 --> 00:20:09.260]   but they're surely going to get cheaper
[00:20:09.260 --> 00:20:10.540]   faster than they get better now.
[00:20:10.540 --> 00:20:11.940]   And now the question is,
[00:20:11.940 --> 00:20:14.540]   how can we use the models plus the data we have
[00:20:14.540 --> 00:20:17.300]   and build systems that actually result in better results
[00:20:17.300 --> 00:20:18.980]   and can tackle net new problems?
[00:20:18.980 --> 00:20:21.460]   Because unless we have like a breakthrough in architecture
[00:20:21.460 --> 00:20:26.420]   or we somehow stubble across like another massive set of data
[00:20:26.420 --> 00:20:28.580]   like new tokens that are trainable,
[00:20:28.580 --> 00:20:29.940]   like where do we get the next level
[00:20:29.940 --> 00:20:31.900]   of like intelligence bump from?
[00:20:31.900 --> 00:20:34.940]   And so one way to think it was like the rise of AI engineering,
[00:20:34.940 --> 00:20:36.540]   another way to think about it is just the rise of like
[00:20:36.540 --> 00:20:39.340]   the fact that, hey, it's now we're in systems problem land.
[00:20:39.340 --> 00:20:42.020]   We're very much more similar to like the rise of microservices
[00:20:42.020 --> 00:20:43.780]   and service orientated architecture
[00:20:43.780 --> 00:20:46.260]   in, you know, last decade or two decades ago
[00:20:46.260 --> 00:20:48.220]   in terms of like AI compute.
[00:20:48.220 --> 00:20:49.500]   - Yeah, so fascinating.
[00:20:49.500 --> 00:20:54.300]   Like if we continue to able to get models even smaller
[00:20:54.300 --> 00:20:56.900]   and able to have the leverage able to call multiple models
[00:20:56.900 --> 00:20:58.420]   like the MOE theme, right?
[00:20:58.420 --> 00:21:01.940]   But in a much more larger and dynamic scale,
[00:21:01.940 --> 00:21:05.380]   'cause today MOEs are more like a fixed model thing.
[00:21:05.380 --> 00:21:07.460]   Like if I think of models almost like the same way
[00:21:07.460 --> 00:21:09.900]   I think of MPM packages, for example,
[00:21:09.900 --> 00:21:12.420]   I can have a thousand MPM packages in my app.
[00:21:12.420 --> 00:21:14.180]   I'm just using a bunch of these as a calls
[00:21:14.180 --> 00:21:15.700]   or something in that nature.
[00:21:15.700 --> 00:21:17.820]   Maybe one day I'll think of all my models
[00:21:17.820 --> 00:21:19.940]   as like an MPM package installed
[00:21:19.940 --> 00:21:22.460]   and I have a 500 models.
[00:21:22.460 --> 00:21:25.100]   (both laughing)
[00:21:25.100 --> 00:21:27.820]   My app just calls whatever they wants to, right?
[00:21:27.820 --> 00:21:28.700]   - It'll be fascinating.
[00:21:28.700 --> 00:21:31.260]   I don't think we even got to even close to that
[00:21:31.260 --> 00:21:35.260]   sort of like level of like understanding or and from,
[00:21:35.260 --> 00:21:37.540]   but I feel like we can get there soon
[00:21:37.540 --> 00:21:39.980]   where like the models can get so much cheaper.
[00:21:39.980 --> 00:21:41.780]   You don't need a full model everywhere.
[00:21:41.780 --> 00:21:44.460]   Like actually people just need a couple layers.
[00:21:44.460 --> 00:21:46.500]   You know, that's all the research recently is all like,
[00:21:46.500 --> 00:21:47.740]   you know, you just need a small layers
[00:21:47.740 --> 00:21:50.300]   and be activated for most calls.
[00:21:50.300 --> 00:21:51.700]   - Yeah, and this is something we actually learned.
[00:21:51.700 --> 00:21:52.540]   It's so interesting.
[00:21:52.540 --> 00:21:53.860]   It's going through a lot of the same arc
[00:21:53.860 --> 00:21:56.300]   of like we had with like AlexaNet, you know,
[00:21:56.300 --> 00:21:59.180]   in 2017, 2018 is like, oh, we can do all these things.
[00:21:59.180 --> 00:22:00.780]   See if you just have the first couple layers
[00:22:00.780 --> 00:22:03.540]   and you can quantize, you can reduce the encoding.
[00:22:03.540 --> 00:22:05.580]   There were so many options you could do to reduce it
[00:22:05.580 --> 00:22:06.420]   with AlexaNet.
[00:22:06.420 --> 00:22:07.740]   Now we're like, oh, it actually turns out
[00:22:07.740 --> 00:22:09.780]   all those things kind of broadly generalized also
[00:22:09.780 --> 00:22:12.220]   to like this LM Kosker problem.
[00:22:12.220 --> 00:22:13.700]   One point I'd make, by the way,
[00:22:13.700 --> 00:22:15.780]   just to talk about like your MOE point
[00:22:15.780 --> 00:22:17.860]   around like model composition is,
[00:22:17.860 --> 00:22:19.780]   and we're starting to see the beginnings of this,
[00:22:19.780 --> 00:22:22.220]   but not in like a generalized way.
[00:22:22.220 --> 00:22:23.900]   And I think one of the interesting things
[00:22:23.900 --> 00:22:25.140]   that came out near the end of this year
[00:22:25.140 --> 00:22:28.580]   from anthropic and cloud was the model context provider
[00:22:28.580 --> 00:22:29.420]   protocol, which is like,
[00:22:29.420 --> 00:22:32.020]   how do I have a pluggable way to provide contacts
[00:22:32.020 --> 00:22:34.100]   into like the usage of cloud, right?
[00:22:34.100 --> 00:22:36.700]   And so it's certainly not a system.
[00:22:36.700 --> 00:22:38.260]   It's not a full-fledged protocol,
[00:22:38.260 --> 00:22:39.860]   but it's beginnings of thinking about,
[00:22:39.860 --> 00:22:43.100]   well, can we have version zero, zero, zero, one
[00:22:43.100 --> 00:22:46.060]   of what is a protocol that allows you to composite
[00:22:46.060 --> 00:22:48.700]   a bunch of different like intelligence systems together,
[00:22:48.700 --> 00:22:50.180]   both in provide them like context,
[00:22:50.180 --> 00:22:52.860]   but potentially also like guardrails and enforcement.
[00:22:52.860 --> 00:22:54.940]   And that's like a missing piece for missing,
[00:22:54.940 --> 00:22:57.540]   but it's also a sign of the future of these sort of what,
[00:22:57.540 --> 00:23:01.060]   what does this like rise of composite AI systems even mean?
[00:23:01.060 --> 00:23:03.140]   >> Well, one thing I feel like after reading
[00:23:03.140 --> 00:23:05.500]   all this sort of like the protocol
[00:23:05.500 --> 00:23:07.700]   that anthropopically come from,
[00:23:07.700 --> 00:23:09.900]   is not even about the protocol itself.
[00:23:09.900 --> 00:23:12.820]   Like I feel like that is of course interesting.
[00:23:12.820 --> 00:23:14.260]   And it was fun, of course,
[00:23:14.260 --> 00:23:16.060]   you're seeing people adopting that protocol already,
[00:23:16.060 --> 00:23:16.900]   right?
[00:23:16.900 --> 00:23:17.740]   People are building around it,
[00:23:17.740 --> 00:23:19.180]   cloud for everything adopted it.
[00:23:19.180 --> 00:23:23.020]   It's almost like how all the hot AI projects today
[00:23:23.020 --> 00:23:25.860]   have all come from like random places,
[00:23:25.860 --> 00:23:29.380]   random people, but they all caught fire
[00:23:29.380 --> 00:23:31.060]   and some of them keep growing, some of them didn't,
[00:23:31.060 --> 00:23:35.860]   but there's almost like a demand for protocols as well.
[00:23:35.860 --> 00:23:37.740]   We have demanding a new architecture,
[00:23:37.740 --> 00:23:39.860]   but also demanding a new protocols
[00:23:39.860 --> 00:23:41.420]   to solve a bunch of these questions.
[00:23:41.420 --> 00:23:42.740]   And it truly doesn't matter.
[00:23:42.740 --> 00:23:44.580]   You don't have to be only open AI
[00:23:44.580 --> 00:23:45.820]   that can come with this anymore.
[00:23:45.820 --> 00:23:47.340]   You know, that's actually that's the fun part
[00:23:47.340 --> 00:23:50.060]   for start of that opportunity is literally,
[00:23:50.060 --> 00:23:53.220]   if you have a way to solve a real problem
[00:23:53.220 --> 00:23:56.260]   and you don't think of it as just a single vendor thing,
[00:23:56.260 --> 00:23:59.460]   you set up a protocol and you see real applications
[00:23:59.460 --> 00:24:01.260]   around it and you solve them in real,
[00:24:01.260 --> 00:24:03.420]   people will start adopted everywhere.
[00:24:03.420 --> 00:24:04.900]   This is the fascinating part.
[00:24:04.900 --> 00:24:07.740]   AI funding, everything is already crazy busy,
[00:24:07.740 --> 00:24:11.860]   but if you have a real breakthrough, it will go wild
[00:24:11.860 --> 00:24:14.500]   and it doesn't have to come from like a certain big places.
[00:24:14.500 --> 00:24:16.380]   - Absolutely, this is the opportunity.
[00:24:16.380 --> 00:24:18.060]   I think the other thing is like so many people are gonna
[00:24:18.060 --> 00:24:21.260]   like the broadening out of who's actually deploying AI
[00:24:21.260 --> 00:24:22.940]   will drive these needs, right?
[00:24:22.940 --> 00:24:25.940]   Like as we try to drive sort of LMs
[00:24:25.940 --> 00:24:28.060]   and these new interface changes and these new capabilities
[00:24:28.060 --> 00:24:29.620]   and into the broad base of software,
[00:24:29.620 --> 00:24:31.100]   like we're gonna have to develop these things
[00:24:31.100 --> 00:24:33.220]   and it will be, this is opportunity, right?
[00:24:33.220 --> 00:24:35.660]   Like it's not all gonna be captured by open AI.
[00:24:35.660 --> 00:24:37.380]   It's not all gonna be captured by anthropic.
[00:24:37.380 --> 00:24:38.940]   They will capture some of the pie,
[00:24:38.940 --> 00:24:41.660]   but the broad base pie is still gonna be all in the app layer,
[00:24:41.660 --> 00:24:45.380]   which is where like intelligence and data
[00:24:45.380 --> 00:24:47.980]   meets actual human users that create value.
[00:24:47.980 --> 00:24:52.300]   - It's so fascinating to see, like this space is so crowded,
[00:24:52.300 --> 00:24:53.860]   but still so early.
[00:24:53.860 --> 00:24:55.020]   - It's so early.
[00:24:55.020 --> 00:24:59.140]   - Yeah, the real system thinkers has a real opportunity, right?
[00:24:59.140 --> 00:25:00.700]   That's the most exciting part.
[00:25:00.700 --> 00:25:04.100]   So I guess we should move on to our spicy hot takes for,
[00:25:04.100 --> 00:25:09.100]   I don't know, 2025, we're not going like 2030 or 35, I think.
[00:25:13.700 --> 00:25:16.460]   - Do you wanna start, what you think will happen next year?
[00:25:16.460 --> 00:25:17.300]   - Oh, absolutely.
[00:25:17.300 --> 00:25:19.620]   And it's a build on what I think happened last year.
[00:25:19.620 --> 00:25:21.980]   I think AI, doomerism is dead,
[00:25:21.980 --> 00:25:24.420]   and I think it's gonna finally get kicked out the door
[00:25:24.420 --> 00:25:26.660]   at the end of next year.
[00:25:26.660 --> 00:25:28.700]   If there was one thing we started 2024 with,
[00:25:28.700 --> 00:25:31.980]   it was just obscene levels of AI's
[00:25:31.980 --> 00:25:33.660]   gonna need all of us, the world's over.
[00:25:33.660 --> 00:25:34.940]   It's gonna take all our jobs.
[00:25:34.940 --> 00:25:37.940]   It's going to blow up all the nukes and silo.
[00:25:37.940 --> 00:25:41.100]   - You ever like go friends, you know,
[00:25:41.100 --> 00:25:44.500]   all the wives, sell employees, yeah, whatever.
[00:25:44.500 --> 00:25:45.780]   - Exactly, exactly.
[00:25:45.780 --> 00:25:46.820]   Everything's over.
[00:25:46.820 --> 00:25:48.860]   Everyone in Hollywood is gonna get fired.
[00:25:48.860 --> 00:25:50.260]   We're not gonna need writers,
[00:25:50.260 --> 00:25:51.460]   we're not gonna need anything anymore.
[00:25:51.460 --> 00:25:54.140]   And yet, you know, I think if there's one prediction
[00:25:54.140 --> 00:25:55.580]   for next year, I think this is dead.
[00:25:55.580 --> 00:25:57.380]   We stopped really talking about it.
[00:25:57.380 --> 00:25:59.580]   There'll still be people that will be trying
[00:25:59.580 --> 00:26:02.700]   to sell the viewpoint because they make money off of it.
[00:26:02.700 --> 00:26:04.900]   But broadly, I think the rest of the societal conclusion
[00:26:04.900 --> 00:26:06.260]   that those of us who have been spending
[00:26:06.260 --> 00:26:07.740]   any time with these systems, like, oh, actually,
[00:26:07.740 --> 00:26:09.900]   the limitations here are actually quite huge.
[00:26:09.900 --> 00:26:12.180]   We made a thing that's better at predicting
[00:26:12.180 --> 00:26:14.860]   what we were trying to do, which will make us faster,
[00:26:14.860 --> 00:26:18.100]   but it doesn't replace like ingenuity and value of humanity.
[00:26:18.100 --> 00:26:19.660]   And if only it's actually just turns out
[00:26:19.660 --> 00:26:21.220]   to be like a slightly better paintbrush.
[00:26:21.220 --> 00:26:23.220]   And that's pretty fantastic.
[00:26:23.220 --> 00:26:26.020]   So that's my prediction for 2025.
[00:26:26.020 --> 00:26:26.860]   It's kind of spicy.
[00:26:26.860 --> 00:26:27.980]   And I think as a result of that,
[00:26:27.980 --> 00:26:30.260]   it's because more people actually be exposed
[00:26:30.260 --> 00:26:33.620]   to what this incarnation of artificial intelligence is
[00:26:33.620 --> 00:26:36.020]   because they'll be broad-based, more available.
[00:26:36.020 --> 00:26:38.900]   And so as a result of that, they'll see where it's good.
[00:26:38.900 --> 00:26:41.140]   And also, it's extreme weaknesses,
[00:26:41.140 --> 00:26:44.420]   which are many that we haven't talked about,
[00:26:44.420 --> 00:26:45.860]   but they really exist.
[00:26:45.860 --> 00:26:47.500]   How will you tell them? What's your big prediction?
[00:26:47.500 --> 00:26:49.380]   I can't wait to hear.
[00:26:49.380 --> 00:26:51.620]   - I guess maybe this is such a recency bias
[00:26:51.620 --> 00:26:53.780]   because of my episode we recorded today.
[00:26:53.780 --> 00:26:57.540]   But, you know, it's gonna maybe steal a little bit
[00:26:57.540 --> 00:27:00.820]   of what our guest today talks about.
[00:27:00.820 --> 00:27:03.020]   It's basically this idea that Kubernetes
[00:27:03.020 --> 00:27:05.580]   is no longer gonna be like what everyone's attention is.
[00:27:05.580 --> 00:27:08.340]   But I don't think it has been an attention at all anymore,
[00:27:08.340 --> 00:27:11.300]   but really it's like 2025.
[00:27:11.300 --> 00:27:12.860]   I feel like even 2024,
[00:27:12.860 --> 00:27:15.740]   I already kind of sensed that people have been moving away
[00:27:15.740 --> 00:27:17.740]   of really wanting to even dig
[00:27:17.740 --> 00:27:20.740]   and even want to know what's happening in that layer, right?
[00:27:20.740 --> 00:27:23.580]   Like, it's just a thing to run my software.
[00:27:23.580 --> 00:27:25.500]   It is not really that perfectly able
[00:27:25.500 --> 00:27:27.660]   to like hide all complexity anyways.
[00:27:27.660 --> 00:27:28.900]   But there's no better tooling out there
[00:27:28.900 --> 00:27:30.420]   so I'm just stuck there.
[00:27:30.420 --> 00:27:33.340]   So I feel like Infra, you know, next year,
[00:27:33.340 --> 00:27:36.140]   it's almost like the rise of Gen Z Infra, you know what?
[00:27:36.140 --> 00:27:36.980]   (laughs)
[00:27:36.980 --> 00:27:39.660]   And the way I describe it, I don't know.
[00:27:39.660 --> 00:27:40.660]   It's not a better term.
[00:27:40.660 --> 00:27:42.500]   I don't even know how to describe it, right?
[00:27:42.500 --> 00:27:45.100]   It's like there's a sort of age group associated
[00:27:45.100 --> 00:27:46.820]   with like all the infrastructure they would care like.
[00:27:46.820 --> 00:27:48.420]   It has nothing to do with Kubernetes.
[00:27:48.420 --> 00:27:49.980]   There's nothing to do with it.
[00:27:49.980 --> 00:27:52.860]   I bet they'd maybe not have logged into Amazon themselves,
[00:27:52.860 --> 00:27:53.700]   you know?
[00:27:53.700 --> 00:27:56.260]   Like they haven't even seen an Amazon dashboard at all.
[00:27:56.260 --> 00:27:57.660]   It's so fascinating, you know?
[00:27:57.660 --> 00:27:58.500]   It's so fascinating.
[00:27:58.500 --> 00:28:02.940]   Like these folks have largely just reduced their problem
[00:28:02.940 --> 00:28:05.780]   to a single, even a CLI is already complicated.
[00:28:05.780 --> 00:28:07.060]   Like deploy.
[00:28:07.060 --> 00:28:07.900]   Oh, wow.
[00:28:07.900 --> 00:28:09.540]   And you still need to open up my little Unix,
[00:28:09.540 --> 00:28:11.140]   like little terminal.
[00:28:11.140 --> 00:28:12.620]   It's as far as they will go.
[00:28:12.620 --> 00:28:14.660]   (laughs)
[00:28:14.660 --> 00:28:16.620]   Everything is just a GUI
[00:28:16.620 --> 00:28:20.060]   and some sort of dashboardy thing and that's it, right?
[00:28:20.060 --> 00:28:24.020]   And the promise of all these nicer, simpler abstraction
[00:28:24.020 --> 00:28:27.100]   and problems and they try to fit everything they could
[00:28:27.100 --> 00:28:28.180]   in that world.
[00:28:28.180 --> 00:28:30.660]   I just thought it was fascinating, you know?
[00:28:30.660 --> 00:28:32.620]   Like the old school of us, I feel like, you know,
[00:28:32.620 --> 00:28:33.900]   you gotta learn more.
[00:28:33.900 --> 00:28:36.700]   You gotta know what's Kubernetes, right?
[00:28:36.700 --> 00:28:38.620]   I remember when back in the day, we're an engineer,
[00:28:38.620 --> 00:28:40.180]   we always have to answer this question,
[00:28:40.180 --> 00:28:41.820]   interview question, right?
[00:28:41.820 --> 00:28:43.580]   The most famous one I think everybody asks is like,
[00:28:43.580 --> 00:28:46.140]   tell me what happens when you go to a URL or something, right?
[00:28:46.140 --> 00:28:46.980]   You know?
[00:28:46.980 --> 00:28:48.540]   (laughs)
[00:28:48.540 --> 00:28:51.340]   You know, it used to be real interview questions, right?
[00:28:51.340 --> 00:28:53.380]   Tell me as much details, right?
[00:28:53.380 --> 00:28:56.020]   The router has to do this, look up, right?
[00:28:56.020 --> 00:28:58.380]   You know, my browser has a DNS cache.
[00:28:58.380 --> 00:29:00.020]   It's like, I don't know, man.
[00:29:00.020 --> 00:29:02.900]   Like, you know, it just feels like we're just like grumpy.
[00:29:02.900 --> 00:29:04.100]   (laughs)
[00:29:04.100 --> 00:29:06.660]   People still care about our hardware circuits work, right?
[00:29:06.660 --> 00:29:09.380]   On gates and logics and stuff, like, you know?
[00:29:09.380 --> 00:29:11.860]   Real computer science, I don't even care about them anymore.
[00:29:11.860 --> 00:29:14.940]   And to some degree, like, I think 2025,
[00:29:14.940 --> 00:29:17.620]   it's almost like the sort of moving away
[00:29:17.620 --> 00:29:19.580]   from that lower level.
[00:29:19.580 --> 00:29:22.460]   And we had to start embracing the higher levels anymore.
[00:29:22.460 --> 00:29:24.940]   'Cause that's where people mostly feel like,
[00:29:24.940 --> 00:29:27.660]   all these platforms are getting so much mature now,
[00:29:27.660 --> 00:29:29.060]   right, for cells of the world.
[00:29:29.060 --> 00:29:31.180]   They're getting quite mature and do quite a lot of stuff now.
[00:29:31.180 --> 00:29:34.620]   And so they're gonna be companies that are fully running for cell,
[00:29:34.620 --> 00:29:36.020]   fully run on cloud for work,
[00:29:36.020 --> 00:29:38.300]   but fully run on some of these managed providers.
[00:29:38.300 --> 00:29:40.780]   And they don't really even wanna care about anything else.
[00:29:40.780 --> 00:29:42.900]   In that world, I don't know, man.
[00:29:42.900 --> 00:29:44.660]   I don't know what's gonna happen, to be honest.
[00:29:44.660 --> 00:29:49.740]   But 2025, I feel like we're gonna see more people
[00:29:49.740 --> 00:29:53.060]   just doesn't even want to think about that layer.
[00:29:53.060 --> 00:29:54.780]   And therefore, we're gonna see more people
[00:29:54.780 --> 00:29:56.460]   really just wanna bet the whole company
[00:29:56.460 --> 00:29:57.780]   around this stuff, right?
[00:29:57.780 --> 00:29:58.700]   It's just fascinating.
[00:29:58.700 --> 00:30:01.020]   I didn't really wanna admit this is gonna happen.
[00:30:01.020 --> 00:30:04.020]   Not soon, but I just guess it's coming, right?
[00:30:04.020 --> 00:30:04.860]   - I love it.
[00:30:04.860 --> 00:30:07.620]   Gen Z infrastructure, it's totally true, though.
[00:30:07.620 --> 00:30:10.420]   I mean, I think in, I was having a conversation,
[00:30:10.420 --> 00:30:12.620]   actually Tim was involved, but one of the things I said
[00:30:12.620 --> 00:30:14.700]   is like, I feel like infrastructure is always generational,
[00:30:14.700 --> 00:30:16.420]   but it's more consumer than we realize.
[00:30:16.420 --> 00:30:18.220]   Like we always talk with developer experience
[00:30:18.220 --> 00:30:19.780]   being like this key thing.
[00:30:19.780 --> 00:30:22.580]   And that selling to developers is like a consumer-like sale.
[00:30:22.580 --> 00:30:24.140]   Like initially, at the grassroots, it totally is.
[00:30:24.140 --> 00:30:26.740]   You're selling taste, you're selling identity.
[00:30:26.740 --> 00:30:28.900]   Like one of the things that made GitHub very popular,
[00:30:28.900 --> 00:30:31.900]   you know, I was 18 when GitHub came around
[00:30:31.900 --> 00:30:33.620]   and I got a GitHub mug and it was my,
[00:30:33.620 --> 00:30:35.060]   I carried around the GitHub mug
[00:30:35.060 --> 00:30:36.380]   and it was my entire identity.
[00:30:36.380 --> 00:30:39.420]   And there was a 10-year period where I was hyper-producing
[00:30:39.420 --> 00:30:41.100]   tons of code and writing tons of code
[00:30:41.100 --> 00:30:43.460]   and driving in the bus and then eventually I became
[00:30:43.460 --> 00:30:47.820]   not a code monkey as much and more of like manager,
[00:30:47.820 --> 00:30:50.460]   more of like leader, more of different things, right?
[00:30:50.460 --> 00:30:52.860]   And so one of the things is like, yeah,
[00:30:52.860 --> 00:30:54.100]   there it is generational.
[00:30:54.100 --> 00:30:56.180]   And what each generation wants from the tools
[00:30:56.180 --> 00:30:57.780]   that they use is different.
[00:30:57.780 --> 00:31:00.220]   And old men like Tim and I will sit around and be like,
[00:31:00.220 --> 00:31:01.220]   "Ah, how dare they?"
[00:31:01.220 --> 00:31:03.180]   And they're not gonna be able to like have,
[00:31:03.180 --> 00:31:06.300]   you know, vendor lock-in is gonna be terrible.
[00:31:06.300 --> 00:31:07.780]   But then you look at AWS and you realize,
[00:31:07.780 --> 00:31:10.180]   well, that's crazy, vendor lock-in.
[00:31:10.180 --> 00:31:12.140]   I did it off and stable there.
[00:31:12.140 --> 00:31:13.340]   So I agree with you.
[00:31:13.340 --> 00:31:15.260]   Like these rise of these new compute platforms,
[00:31:15.260 --> 00:31:17.020]   things at the edge and all these new use cases.
[00:31:17.020 --> 00:31:18.500]   And I think the biggest one is,
[00:31:18.500 --> 00:31:20.700]   if you think about like how organizations buy.
[00:31:20.700 --> 00:31:23.020]   So like every org in the world right now,
[00:31:23.020 --> 00:31:25.340]   enterprise, mid-market, doesn't matter.
[00:31:25.340 --> 00:31:28.020]   They're all saying 2025 is the year that we like
[00:31:28.020 --> 00:31:29.420]   do something with AI.
[00:31:29.420 --> 00:31:31.020]   What's our AI strategy, right?
[00:31:31.020 --> 00:31:32.940]   Like what's our LOM strategy?
[00:31:32.940 --> 00:31:33.900]   And the first thing they're gonna do is
[00:31:33.900 --> 00:31:36.140]   they're gonna go to their like VP of engineering
[00:31:36.140 --> 00:31:38.020]   and VP of, or VP of platform engineering,
[00:31:38.020 --> 00:31:40.340]   be like, what are you gonna do that's AI?
[00:31:40.340 --> 00:31:41.700]   And they're gonna say, well,
[00:31:41.700 --> 00:31:43.700]   I'm already pre-committed to this customer roadmap
[00:31:43.700 --> 00:31:45.260]   that you already pre-sold me on.
[00:31:45.260 --> 00:31:46.460]   And then the next thing they're gonna do is
[00:31:46.460 --> 00:31:48.780]   we're gonna spin up some RRD lab
[00:31:48.780 --> 00:31:50.700]   with some free flow budget and say,
[00:31:50.700 --> 00:31:53.060]   "You have to just build AI."
[00:31:53.060 --> 00:31:55.100]   They're not gonna build on top of the core platform.
[00:31:55.100 --> 00:31:57.340]   They're not gonna build inside the constraint
[00:31:57.340 --> 00:31:58.780]   because what's important to the business
[00:31:58.780 --> 00:32:02.260]   isn't actually to like drive real revenue yet from that.
[00:32:02.260 --> 00:32:03.980]   But it's the figure where revenue will come from
[00:32:03.980 --> 00:32:06.100]   in three, five, six years.
[00:32:06.100 --> 00:32:08.660]   And they're gonna remove all of the constraints from them
[00:32:08.660 --> 00:32:10.700]   just to figure out, like your job is to figure out,
[00:32:10.700 --> 00:32:13.460]   be top of funnel, think of what's thought leadership look like.
[00:32:13.460 --> 00:32:15.980]   And we already see that, like Cloudflare's the world,
[00:32:15.980 --> 00:32:18.500]   all the other ones, but the rise of like the RND lab
[00:32:18.500 --> 00:32:21.660]   in the big co is back and it's better than ever, baby.
[00:32:21.660 --> 00:32:25.540]   And I agree with you, the Gen Z Gen Z inference here to say
[00:32:25.540 --> 00:32:28.680]   and it's gonna, and all of those companies will be buyers
[00:32:28.680 --> 00:32:30.840]   of the modals and the Vercels
[00:32:30.840 --> 00:32:33.140]   and all the cool startups that we have on this pod
[00:32:33.140 --> 00:32:34.820]   and all the cool ones that we talk to
[00:32:34.820 --> 00:32:35.820]   and all the cool info trends
[00:32:35.820 --> 00:32:38.580]   because it's gonna make building easier.
[00:32:38.580 --> 00:32:39.420]   - Yeah, yeah.
[00:32:39.420 --> 00:32:41.260]   It doesn't necessarily have to be a Gen Z
[00:32:41.260 --> 00:32:43.820]   to build these startup, like Tigris data, right?
[00:32:43.820 --> 00:32:45.940]   Obvious, it's not Gen Z, obviously.
[00:32:45.940 --> 00:32:49.940]   But what he's able to capture is really focus on
[00:32:49.940 --> 00:32:53.660]   a set of new things, like try not to include
[00:32:53.660 --> 00:32:55.460]   the old world anymore, right?
[00:32:55.460 --> 00:32:59.420]   Have to really move on and focus on a new set of people.
[00:32:59.420 --> 00:33:02.300]   And I think his customers are mostly Gen Zs, right?
[00:33:02.300 --> 00:33:04.980]   (both laughing)
[00:33:04.980 --> 00:33:07.900]   So, I don't know, to me, like Gen Z inference
[00:33:07.900 --> 00:33:10.780]   really just encapsulates like the whole paradigm shift
[00:33:10.780 --> 00:33:12.620]   that's happening, it's, like I said, but yeah,
[00:33:12.620 --> 00:33:14.780]   the buyers of this will be, you know,
[00:33:14.780 --> 00:33:17.140]   people in our age are older either, but you know--
[00:33:17.140 --> 00:33:18.420]   - Absolutely.
[00:33:18.420 --> 00:33:20.500]   - All this is driven by something actually, you know,
[00:33:20.500 --> 00:33:22.180]   rather we spoke with the game of this episode,
[00:33:22.180 --> 00:33:24.740]   which is a change in an interface, right?
[00:33:24.740 --> 00:33:26.140]   That's kind of what Guy's talking about,
[00:33:26.140 --> 00:33:27.900]   but like, what does it all represent?
[00:33:27.900 --> 00:33:30.860]   It all represents new things we can compute upon.
[00:33:30.860 --> 00:33:32.060]   We couldn't compute upon before.
[00:33:32.060 --> 00:33:35.380]   We have indeterminism, so these statistical models
[00:33:35.380 --> 00:33:37.220]   now give us the ability to compute on things
[00:33:37.220 --> 00:33:38.460]   we couldn't compute on before.
[00:33:38.460 --> 00:33:40.100]   And that's pretty incredible.
[00:33:40.100 --> 00:33:43.080]   And all these new use cases, all this new spend,
[00:33:43.080 --> 00:33:46.220]   and vastly these new interfaces
[00:33:46.220 --> 00:33:48.220]   that will be very disruptive to businesses
[00:33:48.220 --> 00:33:49.940]   and drive, and it's a new stuff.
[00:33:49.940 --> 00:33:51.220]   Couldn't be more excited, honestly.
[00:33:51.220 --> 00:33:52.860]   2025 is going to be wild.
[00:33:52.860 --> 00:33:53.700]   - I know, I know.
[00:33:53.700 --> 00:33:54.540]   It's so fascinating.
[00:33:54.540 --> 00:33:56.820]   Just to leave me one last comment about this,
[00:33:56.820 --> 00:33:58.540]   I was looking at a pitch deck.
[00:33:58.540 --> 00:34:01.420]   Usually we talk about like, why my product is better.
[00:34:01.420 --> 00:34:03.740]   There's like one line that really strikes me,
[00:34:03.740 --> 00:34:06.180]   all the other product, I forgot what exactly what product
[00:34:06.180 --> 00:34:08.340]   is, but all the other products in this space
[00:34:08.340 --> 00:34:10.380]   are all following the Unix philosophy.
[00:34:10.380 --> 00:34:11.860]   There's like one single command,
[00:34:11.860 --> 00:34:13.060]   I could do one single function.
[00:34:13.060 --> 00:34:15.140]   And that's like why everybody is so bad.
[00:34:16.420 --> 00:34:18.380]   And I'm looking at it, I was like, Jesus Christ.
[00:34:18.380 --> 00:34:20.020]   'Cause I think for our age group of people,
[00:34:20.020 --> 00:34:23.660]   like Unix philosophy is supposed to be the modern
[00:34:23.660 --> 00:34:25.100]   and the right thing to do, right?
[00:34:25.100 --> 00:34:26.540]   It was like a religious thinking,
[00:34:26.540 --> 00:34:28.540]   like, hey, this is what best step tool
[00:34:28.540 --> 00:34:30.900]   or whatever a thing is, you do one thing well,
[00:34:30.900 --> 00:34:34.860]   you can pipe it, we all follow the same paradigm thinking.
[00:34:34.860 --> 00:34:37.580]   Not saying that this is necessary right or wrong,
[00:34:37.580 --> 00:34:39.260]   but like just able to put it out there
[00:34:39.260 --> 00:34:40.980]   is like, this is why things are bad.
[00:34:40.980 --> 00:34:45.220]   Because all these people, I think a certain way
[00:34:45.220 --> 00:34:48.580]   are still doing the same stuff in the same way.
[00:34:48.580 --> 00:34:50.260]   Anyway, it really just like fascinating,
[00:34:50.260 --> 00:34:52.580]   like to re-challenge thinking,
[00:34:52.580 --> 00:34:56.660]   willing to go back to the things we thought is ground truth,
[00:34:56.660 --> 00:34:58.420]   they're not really ground truth, really.
[00:34:58.420 --> 00:35:00.660]   They're just suitable for a type of era
[00:35:00.660 --> 00:35:04.180]   with a type of nature of compute and things that we know of.
[00:35:04.180 --> 00:35:07.500]   But things change, computers change all the time.
[00:35:07.500 --> 00:35:08.340]   - All the time.
[00:35:08.340 --> 00:35:10.220]   - I guess we don't really live through enough
[00:35:10.220 --> 00:35:12.700]   until we start realizing, oh, wow.
[00:35:12.700 --> 00:35:14.540]   Not just our phones looking different anymore,
[00:35:14.540 --> 00:35:16.860]   like actually a lot more things change in the middle.
[00:35:16.860 --> 00:35:19.140]   (laughs)
[00:35:19.140 --> 00:35:21.900]   Then like, oh, blackberries, whatever, yeah.
[00:35:21.900 --> 00:35:23.940]   - Yeah, I 100% agree, the only constant is change.
[00:35:23.940 --> 00:35:26.700]   I think like that old adage is true.
[00:35:26.700 --> 00:35:28.820]   Tim, I'm gonna smack on one last segment.
[00:35:28.820 --> 00:35:30.660]   I have one question for you, I'll answer it,
[00:35:30.660 --> 00:35:31.860]   but you're gonna go first.
[00:35:31.860 --> 00:35:34.540]   If you had a piece of advice for builders right now
[00:35:34.540 --> 00:35:36.140]   on this age, what they need to do,
[00:35:36.140 --> 00:35:38.100]   what they need to be thinking about over the holiday season
[00:35:38.100 --> 00:35:41.100]   to get ready for 2025, what would you be telling founders?
[00:35:41.100 --> 00:35:42.620]   What would you be telling other venture capitalists?
[00:35:42.620 --> 00:35:45.020]   What would you be telling engineers to work at companies?
[00:35:45.020 --> 00:35:46.660]   Like, what's something you're like,
[00:35:46.660 --> 00:35:48.540]   you really shouldn't be thinking about this right now.
[00:35:48.540 --> 00:35:51.700]   Like it's something that you think is important in 2025.
[00:35:51.700 --> 00:35:53.060]   It might not have been wonderful.
[00:35:53.060 --> 00:35:55.100]   - This is my opinion at least, you know?
[00:35:55.100 --> 00:35:56.900]   Especially infrastructure has been
[00:35:56.900 --> 00:36:01.740]   probably a bit more challenging place than before,
[00:36:01.740 --> 00:36:04.420]   just because everyone's attention is just AI everywhere, right?
[00:36:04.420 --> 00:36:05.500]   And everyone kind of just thought AI
[00:36:05.500 --> 00:36:06.980]   is gonna be solving everything.
[00:36:06.980 --> 00:36:08.700]   Like I told founders kind of the same thing
[00:36:08.700 --> 00:36:10.780]   or I just mentioned is that, you know,
[00:36:10.780 --> 00:36:12.180]   sometimes when we solve infrastructure,
[00:36:12.180 --> 00:36:14.580]   we're trying to always solve infrastructure to everybody,
[00:36:14.580 --> 00:36:16.300]   like solving infrastructure to old school,
[00:36:16.300 --> 00:36:17.580]   solving infrastructure to Kubernetes,
[00:36:17.580 --> 00:36:19.900]   solving infrastructure for like everyone, right?
[00:36:19.900 --> 00:36:22.980]   Treat them all the same and try to solve all problems.
[00:36:22.980 --> 00:36:27.220]   I think, you know, the biggest issue most founders think about
[00:36:27.220 --> 00:36:29.340]   that I always challenge them a little bit is like,
[00:36:29.340 --> 00:36:33.100]   hey, you know, any startup that works,
[00:36:33.100 --> 00:36:35.820]   you gotta be non-obvious the time you start.
[00:36:35.820 --> 00:36:37.700]   It's already too late if it's obvious, you know?
[00:36:37.700 --> 00:36:40.100]   So like, what's the point?
[00:36:40.100 --> 00:36:42.260]   And to be non-obvious and be huge,
[00:36:42.260 --> 00:36:44.820]   you're betting on one single thing that can be huge.
[00:36:44.820 --> 00:36:46.260]   And that thing isn't gonna just look
[00:36:46.260 --> 00:36:49.060]   like your Kubernetes or old school stuff.
[00:36:49.060 --> 00:36:50.460]   It will be somewhere in the middle
[00:36:50.460 --> 00:36:53.820]   and it probably way more gears towards the rights.
[00:36:53.820 --> 00:36:57.140]   And so I have to just debate with a lot of our companies
[00:36:57.140 --> 00:36:58.780]   and some newer ones like, hey,
[00:36:58.780 --> 00:37:00.540]   really try to do one thing well.
[00:37:00.540 --> 00:37:02.900]   And that one thing has to be non-obvious,
[00:37:02.900 --> 00:37:05.140]   not completely ludicrous, but like,
[00:37:05.140 --> 00:37:07.300]   it can't just be like, oh, 50 people doing the same thing,
[00:37:07.300 --> 00:37:08.660]   we're gonna do the exact same thing
[00:37:08.660 --> 00:37:09.980]   and just have a little bit different.
[00:37:09.980 --> 00:37:12.660]   People don't usually wanna admit that a little bit difference.
[00:37:12.660 --> 00:37:13.980]   They think it's everything.
[00:37:13.980 --> 00:37:15.460]   (laughs)
[00:37:15.460 --> 00:37:17.260]   But I always challenge like, hey,
[00:37:17.260 --> 00:37:19.900]   if you can't even find a different way
[00:37:19.900 --> 00:37:22.580]   or you have to like spend time to sit down with somebody
[00:37:22.580 --> 00:37:24.180]   and we'll do one-on-one chatter to find
[00:37:24.180 --> 00:37:26.620]   if you're able to convince somebody that's fully different,
[00:37:26.620 --> 00:37:28.860]   it's not gonna work for you, you know?
[00:37:28.860 --> 00:37:30.700]   And I think it's probably much more for founders
[00:37:30.700 --> 00:37:32.260]   than investors, right?
[00:37:32.260 --> 00:37:33.260]   I think that's largely true.
[00:37:33.260 --> 00:37:35.420]   Like founders, I think, you know,
[00:37:35.420 --> 00:37:37.260]   I was founder just not that long ago.
[00:37:37.260 --> 00:37:41.380]   And I think we all sort of play the same path where like,
[00:37:41.380 --> 00:37:44.020]   we just, we have a pre-made of mind
[00:37:44.020 --> 00:37:47.500]   of what is the best path forward all the time.
[00:37:47.500 --> 00:37:49.460]   But reality is like, the real thing
[00:37:49.460 --> 00:37:51.340]   isn't always set in stone yet.
[00:37:51.340 --> 00:37:52.580]   You have to go discover it.
[00:37:52.580 --> 00:37:54.060]   It has to be in a different place
[00:37:54.060 --> 00:37:56.380]   than you originally started with.
[00:37:56.380 --> 00:37:58.980]   - And it has to be something that like gets your buyer,
[00:37:58.980 --> 00:38:02.100]   your customer, whoever you're targeting, super excited.
[00:38:02.100 --> 00:38:04.560]   And so many times founders will like,
[00:38:04.560 --> 00:38:05.900]   do things that get some excited
[00:38:05.900 --> 00:38:07.820]   for customers, for those people excited.
[00:38:07.820 --> 00:38:09.220]   - Yeah, do you have the last advice
[00:38:09.220 --> 00:38:10.860]   to give to founders or?
[00:38:10.860 --> 00:38:13.900]   - I mean, my last minute advice that I'd give to founders,
[00:38:13.900 --> 00:38:15.900]   I just think to anyone thinking about infrastructure,
[00:38:15.900 --> 00:38:18.380]   building companies or building software is,
[00:38:18.380 --> 00:38:20.220]   there are so many assumptions that have been made
[00:38:20.220 --> 00:38:22.860]   in the last 10 years of cloud, like cloud 1.0,
[00:38:22.860 --> 00:38:25.180]   that made sense in the context of Kubernetes
[00:38:25.180 --> 00:38:27.460]   and made sense in the context of I only have one cloud vendor
[00:38:27.460 --> 00:38:29.780]   and made sense in the context of like one budget
[00:38:29.780 --> 00:38:31.140]   and all roles up to one person.
[00:38:31.140 --> 00:38:33.460]   I think like we have another wave coming
[00:38:33.460 --> 00:38:35.140]   and all those soft stuff is gonna change
[00:38:35.140 --> 00:38:37.540]   and that it's gonna be complete creative disruption.
[00:38:37.540 --> 00:38:39.060]   And so I think the question isn't,
[00:38:39.060 --> 00:38:41.220]   what made sense in cloud 1.0?
[00:38:41.220 --> 00:38:44.620]   It's more a question of what makes sense in cloud 2.0
[00:38:44.620 --> 00:38:46.420]   and what is cloud 2.0, what's that look like
[00:38:46.420 --> 00:38:47.460]   and how do you fit into it?
[00:38:47.460 --> 00:38:49.940]   And I don't think any of us have any inkling of what that is,
[00:38:49.940 --> 00:38:51.780]   but if I had one piece of advice,
[00:38:51.780 --> 00:38:54.580]   listen to the episode with Guy, that doesn't mean he's right.
[00:38:54.580 --> 00:38:57.060]   He could be completely wrong, but it's different.
[00:38:57.060 --> 00:38:58.740]   And it's really different.
[00:38:58.740 --> 00:39:01.020]   And so you should really understand
[00:39:01.020 --> 00:39:02.460]   and be following what that is.
[00:39:02.460 --> 00:39:04.020]   And you should have a thesis for why it is
[00:39:04.020 --> 00:39:06.500]   and should we back up for why, whoever your buyer is,
[00:39:06.500 --> 00:39:10.140]   why is this gonna be 10X better than what we had before?
[00:39:10.140 --> 00:39:13.940]   Because we're going to have a massive shift in infrastructure.
[00:39:13.940 --> 00:39:15.140]   The old stuff's still gonna be there.
[00:39:15.140 --> 00:39:18.460]   It's gonna get layered, but the new stuff is what matters.
[00:39:18.460 --> 00:39:20.020]   And that's where the spend is.
[00:39:20.020 --> 00:39:21.020]   That was good, sir.
[00:39:21.020 --> 00:39:23.600]   (upbeat music)
[00:39:23.600 --> 00:39:26.180]   (upbeat music)
[00:39:26.180 --> 00:39:28.760]   (upbeat music)
[00:39:28.760 --> 00:39:38.760]   [BLANK_AUDIO]

