--- Podcast Metadata ---
Show: Eye On A.I.
Episode: #261 Jonathan Frankle: How Datâ€¦
Host: Unknown
Guests: Jonathan Frankle
Source URL: https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862
------------------------

Here's a comprehensive summary of the podcast transcript, structured as requested:

**1. Podcast Overview & Key Segments:**

**Overall Summary:** 
This podcast episode features Jonathan Frankle, Chief AI Scientist at Databricks, discussing their innovative Test-time Adaptive Optimization (TAO) technique. TAO allows for continuous improvement of AI models using unlabeled data, potentially rivaling closed-source models while reducing costs and effort for enterprise customers.

**Key Topics:**

1. Introduction to TAO (Test-time Adaptive Optimization):
   TAO is a novel approach to fine-tuning AI models that doesn't require labeled data. It uses reinforcement learning techniques and a specialized reward model (DBRM) to improve model performance based on user inputs alone. This method has shown surprising effectiveness, sometimes outperforming traditional supervised fine-tuning.

2. Databricks' Role in AI and Data Intelligence:
   Databricks focuses on helping customers manage and understand their data using AI technologies. The company's "data lake house" concept combines structured and unstructured data management with advanced AI capabilities, aiming to make data intelligence accessible to a wider range of users.

3. Challenges in Enterprise AI Adoption:
   The podcast highlights the difficulties enterprises face in adopting AI, particularly in obtaining high-quality labeled data for fine-tuning. TAO addresses this by allowing companies to improve models using only input queries, significantly reducing the time and effort required.

4. Comparison with Other AI Techniques:
   The discussion compares TAO to other methods like supervised fine-tuning, reinforcement learning with human feedback (RLHF), and test-time computation techniques used by models like GPT-4. It explores the advantages and unique aspects of TAO in this context.

5. Future Directions for AI Development:
   Frankle discusses future plans for TAO and broader AI development at Databricks, including efforts to create reasoning LLMs that can provide insights into their decision-making process and expanding TAO's capabilities for multi-task learning.

**Conclusion:** 
The podcast concludes with Frankle emphasizing the goal of making AI development more accessible to enterprises, likening it to how website creation became accessible to everyone. He stresses the importance of allowing customers to focus on solving their specific problems rather than struggling with the technicalities of AI implementation.

**2. Key Themes, Technological Insights & Core Discussion Points:**

1. Synthetic Data Generation for Model Improvement:
   TAO uses a novel approach to generate synthetic training data based on user inputs.
   Quote: "The idea is that you can, if you really want to be reductive about it, this is a synthetic data generation and usage technique where you give us the prompts, we generate synthetic data based on those prompts, and then we integrate that synthetic data back into the model."

2. Reducing the Need for Labeled Data:
   TAO significantly reduces or eliminates the need for labeled data in fine-tuning AI models.
   Quote: "We're getting rid of labels, we're throwing out information, and it leads to better results."

3. Balancing Compute Costs Between Training and Inference:
   TAO shifts more compute to the training phase to reduce inference costs.
   Quote: "Let's spend some more compute at training time to generate better synthetic data. And then let's generate a normal model that when we use it at inference time has exactly the same inference cost as Lama had we not done Tau."

4. Continuous Model Improvement:
   TAO allows for ongoing model improvement as more user queries are collected.
   Quote: "You're seeing more of the set of possible inputs your users might provide, and that just gives Tau more data to work off of."

5. Challenges in Creating High-Quality Labels:
   The podcast highlights the difficulty and frustration involved in creating good labeled datasets.
   Quote: "Getting my team to do this for more than 20 minutes before they start checking their email and checking Slack is really hard."

6. Generalization to Multiple Tasks:
   TAO shows promise in improving models across a range of enterprise tasks.
   Quote: "We got general improvement across that family of tasks, which made us confident,, as people move toward agents, you're going to probably want one LLM that can handle many different kinds of tasks for you."

7. Focus on Practical AI Solutions:
   The discussion emphasizes the importance of creating working AI systems over purely novel academic research.
   Quote: "Getting stuff to work for real customers is hard."

**3. Actionable Investment Theses (Early-Stage VC Focus):**

1. AI Model Fine-Tuning Platforms:
   Problem: Enterprises struggle with the high costs and effort required to fine-tune AI models for specific tasks.
   Solution: Platforms like TAO that enable efficient, label-free model improvement.
   Quote: "Let's make sure their time is being spent carefully."
   Opportunity: Invest in startups developing innovative fine-tuning technologies that reduce data labeling requirements.
   Relevant Company: Databricks (though not a startup, their acquisition of MosaicML demonstrates the value in this space)

2. Enterprise-Focused AI Evaluation Tools:
   Problem: Difficulty in measuring AI model performance without extensive labeled datasets.
   Solution: AI-powered evaluation tools that can assess model quality without ground truth data.
   Quote: "It's a way to do evaluation without needing any label data either."
   Opportunity: Invest in startups creating AI model evaluation and benchmarking tools specifically designed for enterprise use cases.

3. Multi-Task AI Agents for Business:
   Problem: Enterprises need AI systems that can handle a variety of tasks without requiring separate models for each.
   Solution: AI platforms that enable the creation of versatile, multi-task models tailored to specific business domains.
   Quote: "As people move toward agents, you're going to probably want one LLM that can handle many different kinds of tasks for you."
   Opportunity: Invest in startups developing frameworks or platforms for creating and managing multi-task AI agents for business applications.

**4. Noteworthy Observations & Unique Perspectives:**

1. Labeled Data Quality Challenges:
   Even high-quality, academically-used datasets may have imperfections that limit model performance.
   Quote: "Wow, even for these world-class benchmarks, the labels may not be perfect."

2. Rethinking Parameter-Efficient Fine-Tuning:
   Techniques like LoRA, originally seen as compromises, may offer unexpected benefits in reducing overfitting and forgetting.
   Quote: "LoRA is a regularization technique as much as anything else. It learns less and forgets less."

3. Democratizing AI Development:
   Drawing parallels between AI development and the early days of web development, suggesting a future where AI creation is widely accessible.
   Quote: "I think a lot about the internet where everybody could have a website. And that led to just such an explosion of creativity, a lot of really bad ideas and a few absolutely brilliant ideas."

4. Balancing Innovation and Practicality:
   Emphasizing the importance of creating working systems over purely novel academic research in the AI field.
   Quote: "Getting stuff to work for real customers is hard."

5. Cautious Approach to AI Deployment:
   Recommending a conservative approach to AI deployment in enterprise settings.
   Quote: "I usually make recommendations to customers to be as conservative as possible when they're doing something with AI. They're already doing something with AI, and that's an aggressive choice to make."

**5. Companies & Entities Mentioned:**

1. Databricks (https://www.databricks.com/) - Jonathan Frankle's employer, focused on data and AI solutions
2. MosaicML (acquired by Databricks) - Frankle's previous startup
3. OpenAI (https://openai.com/) - Mentioned in comparison to TAO's capabilities
4. DeepMind (https://deepmind.com/) - Referenced for their work on Go AI
5. DeepSeek (https://deepseek.com/) - Mentioned in comparison to TAO
6. Google (https://www.google.com/) - Referenced for their recommender systems
7. Facebook (https://www.facebook.com/) - Referenced for their recommender systems
8. Oracle Cloud Infrastructure (https://www.oracle.com/cloud/) - Sponsor of the podcast

**6. Twitter Post Suggestions:**

1. "The idea that AI is similar and everybody has the chance to do creative things with it to solve their problems,, it'll change the world again." - J. Frankle on democratizing AI. Unexpected take: The next Wikipedia might be an AI application! https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862

2. "We're getting rid of labels, we're throwing out information, and it leads to better results." Databricks' TAO challenges conventional wisdom on AI training. Could less data actually lead to better models? https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862

3. "LoRA is a regularization technique as much as anything else. It learns less and forgets less." Rethinking efficiency in AI: Sometimes, learning less is actually more! https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862

4. "Getting stuff to work for real customers is hard." Why practical AI solutions matter more than academic novelty. Are we overlooking the true innovators in AI? https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862

5. "Wow, even for these world-class benchmarks, the labels may not be perfect." Is the foundation of supervised learning shakier than we thought? Databricks' TAO offers a provocative alternative. https://podcasts.apple.com/us/podcast/261-jonathan-frankle-how-databricks-is-disrupting-ai/id1438378439?i=1000712600862

**7. TomTunguz.com Style Blog Post Ideas:**

1. Title: "The Hidden Costs of Labeled Data: Why Unsupervised Fine-Tuning Might Be the Future of Enterprise AI"
   Thesis: Explore how the challenges and costs associated with creating high-quality labeled datasets are pushing the industry towards unsupervised fine-tuning methods like Databricks' TAO.
   Quote: "Getting my team to do this for more than 20 minutes before they start checking their email and checking Slack is really hard."

2. Title: "Rethinking AI Efficiency: When Less Learning Leads to Better Performance"
   Thesis: Analyze how techniques like LoRA and TAO are challenging traditional notions of model efficiency, showing that sometimes constraining a model's learning can lead to better generalization and reduced overfitting.
   Quote: "LoRA is a regularization technique as much as anything else. It learns less and forgets less."

3. Title: "The Next Wave of AI Democratization: From Data Scientists to Domain Experts"
   Thesis: Examine how new AI fine-tuning techniques are shifting the balance of power in AI development from data scientists to domain experts, potentially leading to a new era of specialized AI applications.
   Quote: "I think a lot about the internet where everybody could have a website. And that led to just such an explosion of creativity, a lot of really bad ideas and a few absolutely brilliant ideas."

HOST: Unknown
GUESTS: Jonathan Frankle