 Hey Matt.
 Hey, can you hear me Peter?
 Yeah, I can hear you.
 Can you hear me?
 Hey, what's up?
 Yeah.
 How's it going?
 Good.
 Good.
 We're working on some cool stuff.
 I was excited to meet you.
 Yeah.
 Nice to meet you.
 And your friends are Tom's?
 Yeah, yeah.
 TomÃ¡s is one of our angel investors.
 Nice.
 And, you know, he's been working, you know, we've known him for a couple of years now
 and he's been pretty interested in the stuff we're building, but we've been, you know,
 pretty early and so I think, you know, he's always just been really helpful in terms
 of providing like good advice and guidance and stuff and now we have a new product now
 that, you know, we think it's pretty cool and it's providing some value to like some early
 design partners and so we were kind of starting to like show it off a little bit more and
 just kind of get more focused on it.
 Okay.
 Awesome.
 And so I think you thought you would want to see it.
 I guess that's why you made the interest.
 100% super interested.
 And then just to ground ourselves, are you interested in becoming like in building using
 OpenAI or are you interested in selling to OpenAI or are you interested in like kind
 of all the above or like us helping find customers and share customer ideas with you?
 So I'm just curious like, well, the wording and email was a little vague, so I just want
 to make sure I understood.
 Yeah.
 You know, I don't actually know the answer to that yet.
 Because he didn't, you know, he didn't really give me much context either.
 He obviously thought, he obviously thought there's there's something, you know, we should
 connect.
 Sure.
 Absolutely.
 Let's explore that.
 Yeah.
 I can envision all of those realities.
 So I guess it's really more up to you.
 I guess.
 Okay.
 Cool.
 I would love to, you know, obviously just dive in and show you what we're doing, but just
 for just just for my contacts, like I just, what is your role at OpenAI and like, yeah,
 how do you focus on?
 Yeah.
 Yeah.
 There you go.
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 >> Result of one of those engagements in particular, we built this analytics tool that is really just kind of like a rethink of.
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 And we were able to ask really specific objective questions around the effects of a number of different variables on an objective variable.
 In this case, reading is the one we focused on, but it doesn't matter.
 You can pick any objective variable.
 You could pick session length, you could pick cost, you could pick token count, anything you're trying to optimize for, you put it as the Y variable.
 You want to know the effect of something on it, you put it as the X variable, and if you want to check in with the third variable, you put it as the confounder.
 And so because we've kind of unified this qualitative and quantitative paradigm, you can actually do this with not just LLM data.
 You can do it with any data that has natural language in it that's interesting, right?
 It just so happens that LLMs are like the most prolific producers of natural language data.
 So you could use this tool for analyzing data sets, like you could check, you could look for patterns in training data sets and synthetic data sets.
 You could use this for analyzing user sessions, right?
 So looking at chat sessions and understanding like where, where a chat session, you know, went off the rails, you could look for patterns and input prompts, output generation.
 And you can use it to obviously correlate between any of those natural language patterns and any other quantitative variable of interest.
 So I'll stop there and just see, you know, like how much, you know, is this making sense? Like, you know, see if you have any questions or, yeah.
 Absolutely makes sense to me. Super valuable tool. Really, really, really interesting.
 As I'm sitting in the, like, you know, from my, from my vantage point, meeting a lot of startups in this space.
 You know, you get to see a lot of like app layer stuff, but then you see some dev tool stuff.
 A lot of the tooling and infrastructure stuff sits on the, like, data manipulation side, like trying to get data into the models and then trying to get data out of the models.
 But you're kind of focused on the interaction between the model and the product, which I think is incredibly valuable.
 It feels to me like every startup building an app layer company with ALMs should be using your product. Like, this is like a great way to agree.
 A great way to like, you know, optimize and troubleshoot and improve.
 It's probably also really great for, I mean, it's basically like an eval tool that is not an eval, right? So it's like you're actually looking at what's going on inside the model and the interactions.
 And you could optimize for model, like which model you should be using. You could optimize for, you know, and then how if once you've chosen a model and you're all in on something, then you could actually just, you know, optimize within those parameters.
 It's super, super interesting. I mean, I feel I'm trying to think, my mind goes immediately to like how we can work together.
 Like, I think one of the, one of the ways is that feel like you should demo this to our technical success team.
 That is the team of folks that are going really deep with customers on technical issues and essentially like, just to give you some context of how we operate.
 There's a power law in terms of like the customers we're focused on.
 There's a lot of companies building with LLMs. There's about 15 or 20 of them that rise to the four and like are the most important ones driving the majority of the revenue at the.
 With them are account directors who are like kind of like, you know, like, like a is or whatever.
 Who have a pretty good knowledge of, but they're typically like bringing in solutions architects and sales engineers to look under the hood with the CTOs and heads of, you know, developer heads at these startups and help them optimize and fix and troubleshoot
 and adopt the new snapshots of the model and, you know, rip and replace when they need to.
 So they're the trusted to write a lot of prompt engineering, a lot of fine tuning support, you know, a lot of stuff like this.
 And so this and everyone's emails suck. And if they exist at all.
 So I want to talk about that. So let me, let me show you something. This is super interesting.
 So the company that we're working with right now for that has this data set, they have emails, they wrote their own email framework, they do online and offline emails.
 One of the first things that we use this tool for was actually checking to see if their emails even work.
 And what we found is that a whole bunch of them don't.
 And it's really easy to see this, because what we can do is we can pick. So here's all these columns are distributions of the results of their online emails.
 We have one called phrase similarity. If we look at the distribution of phrase similarity results.
 It's a weird distribution, right? This is supposed to be a metric between zero and one. It's normally distributed left skewed, almost to zero.
 And like this tail makes no sense. If we can found it against its own pass rate, we can see that it only passes.
 Or sorry, it only trips to falls in two fairly random sequences of the distribution, this little part over here, and like a tiny little part you can barely see over here.
 And most of the time it just passes true.
 So the crazier is when we plot, when we plot the same distribution for cases where it should, it should certainly pass or fail.
 It's almost always the opposite and we can do this when we look at my text mode, right? So for example, when text mode is preserved, this evaluator should always pass.
 Because the phrase similarity should be 100% because when text mode is preserved in the product, it's supposed to pass through the user input to the LLM and have the LLM.
 We're like, like, basically return that exact text with no rewarding changes.
 That's broken. We debugged that separately with a separate set of plots earlier this week.
 But then what we did was we plotted the distribution here of this evaluator for those cases. And what we found is that the cases, like this is effectively a 50-50 pass rate.
 It should either be 100% pass, meaning it always, like the text is always preserved as is, therefore, phrase similarities 100% or it should be zero, meaning it's broken all the time.
 Instead, it's basically random, right? And they had no idea, right? And so what's happening is people are creating all these eval metrics, but the fact is it's really hard to do automated evals.
 Like, it's just a hard problem. And the only thing that's actually a North Star for this kind of thing is user feedback.
 And so if you plot your evaluation, like if you plot your eval back, you can pretty much find out immediately whether your evaluators actually work, because they should either align with user feedback or not.
 And most people don't have a way to do this, because either the user feedback is coming in a natural language and there's no analytics tool that solves that, except for ours, obviously.
 Or they don't even have a rating system in place in the first place, right? So you need to put a rating system in place so that users can actually give that like thumbs up, thumbs down feedback.
 And then you need a way to actually measure that in conjunction with your automated evals.
 And no one's doing that. So it's like, you're right, the eval sucked to begin with, but the path to making them better is an analysis that no one's equipped to do.
 And that's why we built this product, because this product is so simple to see what is working and what's not working with respect to LLM user behavior, like user interactions.
 Yeah, yeah, I agree with you as well that the natural language is the only way and user feedback is the only way to evaluate.
 We have, you know, we call it like vibes based evals, right? It's like, that's how everyone kind of like bases everything right now.
 And so this is awesome. It quantifies it. And it looks at it from an academic standpoint, you know, or statistical standpoint.
 This is vibes modification. I think it is.
 It is modification exactly.
 Maybe that's what I should put on the home page is quantified vibes.
 Exactly. Quantified your vibes based evals. I mean, this is this is super powerful.
 I mean, let me know where I can invest.
 It's a great business.
 Absolutely. Yeah, we can definitely talk about that.
 But I think right now, yeah, right now, what we really want to do is get a few more solid design pilot, like pilot customers who can really use the tool, like, yeah, using the tool to great success.
 We, you know, we're working with them on a, on, you know, a contracting basis essentially as a design partner.
 We'd love to get kind of a few more, by the way, and that's fine. If that's open AI to if you think your technical success teams are actually potential users here, then that's great.
 Well, tell me, tell me, I think that depends on the data and like, it's like, if we could just, if so, oh, I do. So as a next step, you know, we have five minutes as a next step.
 I do want to put you in touch with someone or some folks on the technical success team.
 And they'll be the ones that can make the determination of whether they could, you know, be a buyer of this nine partner.
 I think that'd be an awesome outcome if we can make that work. And I'll make sure that I package it and couch it in, like, with the value that I see.
 I appreciate that. What are the hurdles you foresee them?
 I don't know how the procurement process works here. The hurdle I'm concerned about is like, you know, people are low to give open AI data, because there's just like a perception that we're training on the data.
 Like, we don't train on any of the LLM data, but there's just like a data, like, thing with customers. And so, you know, this is a third party tool.
 Like, you're, you're basically opening it up to all of the product data and all of the, and all of the LLM data. And like, I think that there's maybe, there may be concerns or hesitancy.
 Like, if we could just plug this in and just have it for everyone and be like, Hey, here's what's going wrong. And here's how you can build better and we can like, then that'd be awesome.
 But I feel I seem to think the hurdles in it.
 That's some really good news. This tool actually runs entirely locally.
 Oh, wow. Okay. So, so we can actually package it up. And you can, you can run it in-house on your developer machines, like with seats, basically.
 And you just, you just connect the tool to your own data warehouse and the tool just snapshots tables from your data warehouse.
 Okay. And then all the querying, all the, everything I just showed you, the reason it's so fast, like the clustering and like the plots, everything.
 The reason they're also fast is because everything is happening on the local machine.
 And so, it's a great user experience because you don't wait for anything, even for huge data sets.
 But also, it circumvents the data security problem because we don't, we're not hosting your data and we're not even hosting a software.
 Yeah. How would a customer success person at OpenAI access these charts on a customer's behalf and help them drive decision making?
 I mean, I think ultimately we want them to become self-reliant on your tool and not have to use our team.
 We're more like teaching the fish. Yeah. Exactly. So what you would do. So what we did was, we made the data model local, but the metadata, the metadata model can be synced to the cloud.
 Okay. So you can save plots and share plots and create plot groups and we're actually, we're building a teacher where you can actually like auto-generate a whole report over plots.
 So like you can, you can do a series of plots that basically pinpoint a problem in your product and then you can just export the whole group as a report.
 And like, it just, it just tells the story for you. And then you can just like ship that off to your product manager and be like, here's the problem, you know.
 I see.
 So I'd say there's a couple of ways you could go about this, which is pretty similar to how we're using the tool currently for our design partners is we drive the tool to produce the insights to show the customer how to use the tool and then we give them the tool.
 And then they do their, and then they do their own insights.
 But because the data is all coming from the data warehouse anyway, like we're pretty agnostic to where the data is.
 We really don't care where the data is because as long as you can run it on a, on a MacBook, like you're, you're good to go.
 Yeah, that makes sense.
 Yeah.
 Okay.
 All right.
 So, so I think, I think it's for sure worth all of the solutions architects, whether or not there's, I mean startups, it doesn't really matter if there's startups, this can be valuable across the entire landscape of companies using LMS.
 Actually, is he actually more valuable for the bigger companies because finding these distributions like finding these failure modes in bigger data sets is extremely hard.
 It's very.
 Yeah.
 So the more the bigger the data set, the more valuable the tool.
 So I'm going to try to set you up with someone on the technical success team.
 There's also like, there's something that we don't do yet, but we don't have one minute and I do have to jump, but the, we don't have like a co cell motion, yet we're not much sure enough.
 This would be something I would want to co cell like I would want to be like, Hey, were you using for emails, we're using brain trust, you know, it's like, okay, well, maybe you should take a look at this as well, you know, in addition to like as a compliment.
 I think I think that's the right cell because we don't replace an eval platform, but we do.
 We do fill what I think is a uniquely missing gap, which is we are a product we are a product analytics tool that understands natural language that does not exist.
 And, and we can evaluate your evaluators. So like, I see eval platforms as CI platforms or test suites almost for these sort of like synthetic metrics that people build.
 That's great. But the problem with eval platforms in their current form is they completely ignore the rest of your product data and they completely ignore the like the natural language feedback from users.
 And there's no way to contextualize that.
 So, you know, I don't and I think the metrics are, you know, a lot of them are like, they're usefulness is also questionable. So I think, yeah, that's the right pitch. The right pitch is to say, look, you have like you're using an eval platform over here.
 So it basically do CI for your prompts. That's great.
 You're not doing product analytics, like you're not doing, you're not doing end to end BI, all the way from your session variables, all the way to your, what your user actually said about their experience.
 Yeah, I also want, I'm also wondering if I should bring in our product team, or data, data science team as well, to have you, you know, meet with them because we could maybe use this internally to think you could.
 Maybe be a favorite, Peter, can you send me an email, just a blurb describing the company and your own words, you know, paragraph or two.
 And just so I can use that kind of as an artifact to front run in here.
 And then let me set something up for you in the next couple of weeks to talk with some folks internally. And I think this is really awesome.
 And yeah, hopefully we can unlock some opportunities together.
 And Matt, I really thank you so much for the time. I'll send you that email and we'll take it from there. Of course, great meeting you.
 Bye. Take care. You too. Bye.
