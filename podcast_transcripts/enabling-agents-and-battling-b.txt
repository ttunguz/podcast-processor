--- METADATA START ---
Show: AI + a16z
Episode: Enabling Agents and Battling Bâ€¦
Host: Joel De LaGarza 
Guests: David Mitten
Source URL: https://podcasts.apple.com/us/podcast/enabling-agents-and-battling-bots-on-an-ai-centric-web/id1740178076?i=1000712750758
--- METADATA END ---

1
If 50% of traffic is already bots, it's already automated, and agents are only really just getting going.
2
Most people are not using these computer use agents because they're too slow right now.
3
They're still at previews, but it's clear that's where everything is going, then we're going to see an explosion in the traffic that's coming from these tools.
4
And just blocking them just because they're AI is the wrong answer.
5
You've really got to understand why you want them, what they're doing, who they're coming from, and then you can create these granular rules.
6
Thanks for listening to the A16Z AI podcast.
7
If you've been listening for a while, or if you're at all plugged into the world of AI, you've no doubt heard about AI agents and all the amazing things they theoretically can do.
8
But there's a catch.
9
When it comes to engaging with websites, agents are limited by what any given site allows them to do.
10
If, for example, a site tries to limit all non-human interactions in an attempt to prevent unwanted bot activity, it might also prevent an AI agent from working on a customer's behalf, say making a reservation, signing up for a service, or buying a product.
11
This broad strokes approach to site security is incompatible with the idea of what some call agent experience, an approach to web and product design that treats agents as first-class users.
12
In this episode, A16Z InfraPartner Joel De LaGarza dives into this topic with David Mitten, the CEO of ArcJet, a startup building developer-native security for modern web frameworks, including attack detection, sign-up spam prevention, and bot detection.
13
Their discussion is short, sweet, and very insightful.
14
And you'll hear it after these disclosures.
15
As a reminder, please note that the content here is for informational purposes only, should not be taken as legal, business, tax, or investment advice, or be used to evaluate any investment or security, and is not directed at any investors or potential investors in any A16Z fund.
16
For more details, please see a16z.com/slash disclosures.
17
It seems like what once was old is new again.
18
And would love to get your thoughts on this new emergence of bots and how, while we know all the bad things that happen with them, there's actually a lot of good and really cool stuff that's happening and how we can maybe work towards enabling that.
19
Right, well, things have changed, right?
20
The DDoS problem is still there, but it's just almost handled as a commodity these days.
21
The network provider, your cloud provider, they'll just deal with it.
22
And so when you're deploying an application, most of the time you just don't have to think about it.
23
The challenge comes when you've got traffic that just doesn't fit those filters.
24
It looks like it could be legitimate, or maybe it is legitimate, and you just have a different view about what kind of traffic you want to see.
25
And so the challenge is really about, well, how do you distinguish between the good bots and the bad bots?
26
And then with AI changing things, it's bots that might even be acting on behalf of humans, right?
27
It's no longer a binary decision.
28
And as the amount of traffic from bots increases, like in some cases, it's the majority of traffic that sites are receiving is from an automated source.
29
And so the question for site owners is, well, what kind of traffic do you want to allow?
30
And when it's automated, what kind of automated traffic should come to your site?
31
And what are you getting in return for that?
32
And in the old days, I mean, I guess the old providers, we'll say the legacy providers in this space, like it was very much using a hammer, right?
33
So they would say, hey, if this IP address is coming in, it's probably a bot.
34
Or they would say, if this user agent is coming in, it's probably a bot.
35
Very imprecise.
36
And I think the downside of that is that you probably blocked a lot of legitimate traffic along with illegitimate traffic.
37
And now there's very real consequences because some of these AI bots could be actual users they're acting on behalf of who are looking to purchase your products This is the challenge.
38
So a volumetric DDoS attack, you just want to block that at the network.
39
You never want to see that traffic.
40
But everything else needs the context of the application.
41
You need to know where in the application the traffic is coming to.
42
You need to know who the user is, the session, and to understand in which case you want to allow or deny that.
43
And so this is the real issue for developers, for site owners, for security teams is to make those really nuanced decisions to understand whether the traffic should be allowed or not.
44
And the context of the application itself is so important because it depends on the site.
45
If you're running an e-commerce operation, an online store, the worst thing you can do is block a transaction because then you've lost the revenue.
46
Usually you want to then flag that order for review.
47
A human customer support person is going to come in and determine based on various signals about whether to allow it.
48
And if you just block that at the network, then your application will never see it.
49
You never even know that that order was failed in some way.
50
There's been a lot of media releases about companies that have released solutions in this space, but largely they were based on sort of those old kind of approaches using network telemetry.
51
Is that generally how they're working now?
52
Or is there some other capabilities that they've released?
53
Because they give them AI names and you just immediately assume that they're doing something fancy.
54
That's right.
55
Yeah.
56
So blocking on the network is basically how the majority of these old school products work.
57
They do analysis before the traffic reaches your application, and then you never know what the result of that was.
58
And that just doesn't fly anymore.
59
It's insufficient for being able to build modern applications, particularly with AI coming in, where something like OpenAI has four or five different types of bots, and some of them you might want to make a more restrictive decision over, but then others are going to be taking actions on behalf of a user search.
60
And we're seeing lots of different applications getting more sign-ups, businesses actually getting higher conversions as a result of this AI traffic.
61
And so just blocking anything that is called AI is too blunt of an instrument.
62
You need much more nuance.
63
And the only way you can do that is with the application context, understanding what's going on inside your code.
64
I mean, I'd say we're seeing across the industry that AI is driving incredible amounts of new revenue to companies.
65
And if you use an old-world tool to just block any of that traffic, you're probably doing your business.
66
That's right.
67
Or you're like putting it into some kind of maze where it's seeing irrelevant content.
68
And then by doing that, you are kind of downranking your site because the AI call is never going to come back.
69
It's kind of like blocking Google from visiting your site.
70
It's like, yeah, Google doesn't get you in, you're no longer in Google's index, but then you're no longer in Google's index.
71
And so anyone searching is not going to find you as a result.
72
Well, and I believe we had sort of standards in the old days that developed your quasi-standards like robots.txt, right?
73
Which would tell you, like, it'd tell the crawlers, hey, don't crawl these directories.
74
Are we doing something similar for this new agentic world?
75
So, robots.txt is still the starting place.
76
And it's kind of a voluntary standard.
77
It evolved over several decades ago now.
78
It's been around a long time.
79
Bots have been a problem for a long time.
80
And the idea is that you describe the areas of your application and tell any robot that's coming to your site whether you want to allow that robot to access that area of the site or not.
81
And you could use that to control the rollout of new content.
82
You could protect certain pages of your site that you just don't want to be indexed for whatever reason.
83
And you can also point the crawler to where you do want it to go.
84
You can use the sitemap for that as well.
85
But the robot's text file format has evolved over time to provide these signals to the likes to crawlers like search engines from Google and so on.
86
The challenge with that is it's voluntary and there's no enforcement of it.
87
And so you've got good bots like Googlebot that will follow the standard and you'll be able to have full control over what it does.
88
But there are newer bots that are ignoring it or even sometimes using it as a way to find the parts of your site that you don't want it to access and they will just do that anyway.
89
And so this becomes a control problem for the site owner.
90
And you really want to be able to understand not just what the list of rules are, but how they are enforced.
91
Totally.
92
Maybe it'd be great to walk through what these agents are, maybe get some more understanding of sort of how they operate, what people are using them for, perhaps go through a couple of the use cases.
93
And then it'd be great to understand sort of like how you do control it, because it seems like a far more complicated problem than just bad IP addresses.
94
Right.
95
So if we think about OpenAI as an example, because they have four or five different crawlers, there's one and they all have different names and they all identify themselves in different, if in different ways.
96
So one actually is crawling to train the OpenAI models on your site.
97
And that's one that probably everyone is thinking about when they're thinking about I want to block AI, the training, and you have different philosophical approaches to how you want to be included in the training data.
98
The others are more nuanced and more require more thought.
99
So, there's one that will go out when a user is typing something into the chat and has asked a question, and OpenAI will go out and search.
100
It's built up its own search index, and so that's equivalent to Googlebot.
101
You probably want to be in that index because, as we're seeing, sites are getting more signups, are getting more traffic.
102
The discovery process is being part of just another search index is super important.
103
Gotcha.
104
So, like when I ask OpenAI when is John F.
105
Kennedy's birthday, if it doesn't know the answer, it goes out and searches the web.
106
Yeah, that's right.
107
Or if it's trying to get open hours for something, it might go to a website for a cafe or whatever and pass it and then return the results.
108
So, that's really just like a classic search engine crawler, except it's kind of happening behind the scenes.
109
The other one is something that's happening in real time.
110
So, you might give the agent a specific URL and go and ask it to summarize it or to look up a particular question in the docs for a developer tool or something like that.
111
And then that's a separate agent that will go out, it will read the website, and then it will return and answer the query.
112
For both of these two examples, OpenAI and others are now starting to cite those sources.
113
And you'll regularly see, and this is kind of the recommendation: is you get the result from the AI tool, but you shouldn't trust it 100%.
114
You go and then verify, and you look at the docs.
115
And maybe it's like when you used to go to Wikipedia and you'd read the summary, and then you look at the references, and you'd go draw the references and check to make sure what had been summarized is actually correct.
116
But all three of those examples, you clearly could see why you would want them accessing your site.
117
Right.
118
Like, blocking all of OpenAI's crawlers is probably a very bad idea.
119
Yeah, it's too blunt.
120
It's too blunt an instrument.
121
You need to be able to distinguish each one of these and determine which parts of your site you want them to get into.
122
And this then comes to the fourth one, which is the actual agent.
123
This is the end agent, the computer operator type feature.
124
Hadless web browsers, those headless web browsers, yeah, but even a web browser, a full web browser operating inside a VM.
125
And those are the ones that require more nuance because maybe you're booking a ticket or doing some research and you do want the agent to take actions on your behalf.
126
Maybe it's going through your email inbox and triaging things.
127
From the application builder's perspective, that's probably a good thing.
128
You want more transactions, you want more usage of your application.
129
But there are examples where it might be a bad action.
130
So, for example, if you're building a tool that is going to try and buy all of the concert tickets and then sell them on later, that becomes a problem for the concert seller because they don't want to do that.
131
They want the true fans to be able to get access to those.
132
And again, you need the nuance.
133
Maybe you allow the bot to go to the home page and sit in a queue.
134
But then when you get to the front of the queue, you want the human to actually make the purchase and you want to rate limit that so that maybe the human can only purchase, let's say, five tickets.
135
You don't want them to purchase 500 tickets.
136
And so this gets into the real details of the context, each one about what you might want to allow and what you might want to restrict.
137
That's incredibly complicated.
138
I mean, if I remember back, why we made a lot of the decisions we made in blocking bots was strictly because of scale.
139
So, you know, you've got 450,000 IP addresses sending you terabits of traffic through a link that only can do gigabit, and you've got to just start dropping stuff, right?
140
And you take, you know, it's the battlefield triage of the wounded, right?
141
It's like some of you aren't going to make it.
142
And it becomes a little brutal.
143
That sounds incredibly sophisticated.
144
How do you do that sort of fine-grained control of traffic flow at internet scale?
145
So, this is about building up layers of protections.
146
So, you start with the robots.txt, just managing the goodbots.
147
Then, you look at IPs and start understanding where's the traffic coming from.
148
In an ideal scenario, you have one user per IP address, but we all know that that doesn't happen.
149
That never happens.
150
And so, you can start to build up databases of reputation around the IP address, and you can access the underlying metadata about that address, knowing which country it's coming from or which network it belongs to.
151
And then you can start building up these decisions, thinking, well, we shouldn't really be getting traffic from a data center for our sign-up page, and so we could block that network.
152
But it becomes more challenging if we have that agent example.
153
The agent with a web browser or a headless browser is going to be running on a server somewhere.
154
It's probably in a data center.
155
And then you have the compounding factor of the abusers will purchase access to proxies which run on residential IP addresses.
156
So, you can't easily rely on the fact that it's part of a home ISP block anymore.
157
And so, you have to build up these patterns, understanding the reputation of the IP address.
158
Then, you have the user agent string that is basically a free text field that you can fill in with whatever you like.
159
There is kind of a standard there, but the goodbots will tell you who they are.
160
It's been surprising getting into the details of this how many bots actually tell you who they are.
161
And so, you can block a lot of them just on that heuristic combined with the IP address.
162
Or allow them.
163
Or allow them.
164
Yeah, I'm the shopping bot from OpenAI.
165
Right.
166
Come on in and buy some stuff.
167
Exactly.
168
And Googlebot, OpenAI, they tell you who they are.
169
And then you can verify that by doing a reverse DNS lookup on the IP address.
170
So even though you might be able to pretend to be Googlebot, you can check to make sure that that's the case or not with very low latency lookups.
171
So we can verify that, yes, this is Google.
172
I want to allow them.
173
Yes, this is the OpenAI bot that is doing the search indexing.
174
I want to allow that.
175
The next level from that is building up fingerprints and fingerprinting the characteristics of the request.
176
And this started with the JA3 hash, which was invented at Salesforce and has now been developed into a JA4.
177
Some of them are open source, these algorithms, some of them are not.
178
So essentially, you take all of the metrics around a session and you create a hash of it, and then you stick it in a database.
179
Exactly.
180
And you look for matches to that hash.
181
You look for matches.
182
And then the idea is that the hash will change based on the client.
183
So you can allow or deny certain clients.
184
But if you have a huge number of those clients all spamming you, then they all look the same.
185
They all have the same fingerprint and you can just block that fingerprint.
186
So this is almost like, if you think of, you know, I always think of things in terms of the classic sort of network stack, like, you know, layer zero up to layer seven.
187
Like this is almost like layer two level identity for devices, right?
188
Right.
189
It's looking at the TLS handshake on the network level.
190
And then you can go up the layers.
191
There's one called JA4H, which looks at the HTTP headers.
192
And the earlier versions of this would be working on the ordering of the headers, for instance.
193
So an easy way to work around it is just to shift the headers.
194
The hashing has improved over time so that even changing the ordering of the headers doesn't change the hash.
195
And the idea is that you can then combine all of these different signals to try and come to a decision about whether you think this is or who it is basically making the request.
196
And if it's malicious, you can block it based on that.
197
And if it's someone that you want to allow, then you can do so.
198
And this is before you even get into kind of the user level, what's actually happening in the application, right?
199
That's right.
200
Yeah.
201
So this is the logic on top of that.
202
Because you have to identify who it is first before you apply the rules about what you want them to do.
203
Gotcha.
204
So it's almost like you're adding an authentication layer or an identity layer to sort of the transport side.
205
That's right.
206
Yeah.
207
And the application side, I guess you should say.
208
Yeah, the application.
209
Yeah.
210
But it's throughout the whole stack, the whole OSI model.
211
And the idea is you have this consistent fingerprint that you can then apply these rules to.
212
And identity kind of layers on top of that.
213
And we've seen some interesting developments in fingerprinting and providing signatures based on who the request is coming from.
214
So a couple of years ago, Apple announced Privacy Pass, which is a hash that is attached to every request you make.
215
If you're in the Apple ecosystem using Safari on iPhone or on Mac, then there is a way to authenticate that the request is coming from an individual who has a subscription to iCloud.
216
And Apple has their own fraud analysis to allow you to subscribe to iCloud.
217
So it's a very, it's an easy assumption to make that if you have a subscription and this signature is verified, then you're a real person.
218
There's a new one that Cloudflare recently published around doing the same thing for automated requests and having a fingerprint that's attached to a signature inside every single request, which you can then use public key cryptography to verify.
219
These are all emerging as the problem of being able to identify automated clients increases because you want to be able to know who the good ones are to allow them through whilst blocking all the attackers.
220
Yeah, and it's just like the old days with Kerberos, right?
221
Every large vendor is going to have their flavor.
222
And if you're a shop and you're trying to sell to everybody, you've got to kind of work with all of them.
223
That's right.
224
And you just need to be able to understand: is this a human and is our application built for humans?
225
And then you allow them.
226
Or is it that we're building an API?
227
Or do we want to be indexed and we want to allow this traffic?
228
It's just giving the site owner the control.
229
Yeah, I mean, I think it's what's really interesting to me is that in my own use and in my own life, like I interact with the internet less and less directly, like almost every day.
230
And I'm going through some sort of AI type thing.
231
It could be an agent, it could be an large language model, it could be any number of things, but I generally don't query stuff directly as much as I used to.
232
And it seems like we're moving to a world where almost the layer you describe, the agent-type activity you describe, will become the primary consumer of everything on the internet.
233
Well, if 50% of traffic is already bots, it's already automated, and agents are only really just getting going.
234
Most people are not using these computer use agents because they're too slow right now, or they're still at previews, but it's clear that's where everything is going.
235
Then we're going to see an explosion in the traffic that's coming from these tools.
236
And just blocking them just because they're AI is the wrong answer.
237
You've really got to understand why you want them, what they're doing, who they're coming from, and then you can create these granular rules.
238
I mean, I hate to use the analogy, but these things are almost like avatars, right?
239
They're running around on someone's behalf.
240
And you need to figure out who that someone is and what the objectives are, right?
241
And control them very granularly.
242
And the old school methods of doing that assume malicious intent, which isn't always the case.
243
And increasingly is going to be not the case because you want the agents to be doing things.
244
And the signals just no longer work when you're expecting traffic to come from a data center or you're expecting it to come from an automated Chrome instance.
245
And being able to have the understanding of your application to dig into the characteristics of the requests is going to be increasingly important in the future of distinguishing how criminals are using AI.
246
What we've seen so far is either training and people have that opinion of whether they want to train or not, or it's bots that maybe have got something wrong.
247
They're accessing the site too much because they haven't thought about throttling, or they're ignoring robots.txt rather than looking at agents.txt, which is distinguishing between an agent you want to access your site and some kind of crawler.
248
And the examples that we've seen are just bots coming to websites and just downloading the content continuously.
249
There's no world where that should be happening.
250
And this is where the cost is being put on the site owner because they currently have no easy way to manage the control, control the traffic that's coming to their site.
251
Directionally, things are improving because I have looked back 18 months and the bots have no rate limiting.
252
They're just downloading content all the time.
253
Today, we know that these bots can be verified.
254
They are identifying themselves.
255
They are much better citizens of the internet.
256
They are following, starting to follow the rules.
257
And so over the next 18 months, I think we'll see more of that, more of the AI crawlers that we want following the rules, doing things in the right way.
258
And it will start to split into making it a lot easier to detect the bots with criminal intent.
259
And those are the ones that we want to be blocking.
260
So with the transition of bots from being these entities on the internet that represent third parties and organizations to this new world where these AI agents could be representing organizations, they could be representing customers, they could be representing any number of people.
261
And this is probably the wave of the future.
262
It seems to me like detecting that it's AI or a person is going to be an incredibly difficult challenge.
263
And I'm curious, like, how are you thinking about proving humanness on the internet, right?
264
Right?
265
With proofing is a tale as old as time.
266
There's a NIST working group on proofing identity that's been running, I think, for 35 years and still hasn't really gotten to something that's implementable.
267
There's 15 companies out there, right?
268
The first wave of ride-share services and gig economy type companies needed to have proofing, right?
269
Because you're hiring these people in remote places where you don't have an office.
270
And it's still not a solved problem.
271
I'm curious, like, it feels like maybe AI can help get us there, or maybe there's something that's happening in that space.
272
Right.
273
Well, the pure solution is digital signature, right?
274
But we've been talking about that for so long.
275
And the UX around it is basically impossible for normal people to figure out.
276
And it's why something like email encryption, no one encrypts their email.
277
You have encrypted chat because it's built into the app and it can do all the difficult things like the key exchange behind the scenes.
278
So that solution isn't really going to work.
279
But AI has been used in analyzing traffic for at least over a decade.
280
It's just, it was called machine learning.
281
And so you start with machine learning.
282
And the question is, well, what does the new generation of AI allow us to do?
283
The challenge with the LLM-type models is just the speed at which they are doing analysis, because you often want to take a decision on the network or in the application within a couple of milliseconds.
284
Otherwise, you're going to be blocking the traffic and the user is going to become annoyed.
285
And so you can do that with kind of classic machine learning models and do the inference really quickly.
286
And where I think the interesting thing in the next few years is going to be is how we take this new generation of generative AI using LLMs or other types of LLM-like technology to do analysis on huge traffic patterns.
287
I think that can be done in the background initially, but we're already seeing new edge models that are designed to be deployed to mobile devices and IoT that use very low amounts of system memory and can provide inference responses within milliseconds.
288
I think those are going to start to be deployed to applications over the next few years.
289
I think you're exactly right.
290
Like, I think so much of what we're seeing now is just being restricted by the cost of inference.
291
And that cost is dropping incredibly fast, right?
292
We saw this with cloud where like S3 went to being the most expensive storage you could buy to being free, essentially free.
293
Glacier is essentially free, right?
294
Free as beer, right?
295
Whatever.
296
And so we're seeing that even at a more accelerated rate for inference.
297
Like the cost is just falling incredibly.
298
And then when you look at the capabilities of these new technologies to drop a suspicious email into Chat GPT and ask if it's suspicious, then it's like 100% accurate.
299
If you want to find sensitive information, you ask the LLM, is this sensitive information?
300
And it's like 100% accurate.
301
It's amazing.
302
As you squint and look at the future, you can start to see these really incredible use cases.
303
To your point of inference on the edge, do you think we all end up eventually with an LLM running locally that's basically going to be clippy, but for CISOs?
304
It pops up and says, hey, it looks like you're doing something stupid.
305
Is that kind of where you think we land?
306
That's what we're working on is getting this analysis into the process so that for every single request that comes through, you can have a sandbox that will analyze the full request and give you a response.
307
Whereas now you can wait maybe two to five seconds to delay an email and do the analysis and decide whether to flag it for review or send it to someone's inbox.
308
Delaying an HTTP request for five seconds, that's not going to work.
309
And so I think the trend that we're seeing with the improvement cost, the inference cost, but also the latency in getting the inference decision, that's going to be the key so we can embed this into the application.
310
You've got the full context window so you can add everything you know about the user, everything about the session, everything about your application alongside the request and then come to decision entirely locally on your web server, on the edge, wherever it happens to be running.
311
As I listen to you say that and describe this process, all I can think is that advertisers are going to love this.
312
It just seems like the kind of technology built for sort of like, hey, he's looking at this product, showing this one, right?
313
Yeah, super fast inference on the edge, coming to a decision.
314
And for advertisers, stopping click spam, that's a huge problem.
315
And being able to come to that decision before it even goes through your ad model and the auction system.
316
Who would have ever thought that non-deterministic, incredibly cheap compute would solve these use cases?
317
Right.
318
We're in a weird world.
319
That's it for this episode.
320
Thanks again for listening.
321
And remember to keep listening for some more great episodes.
322
As the AI space matures, we need to start thinking more practically about how the technology coexists with the systems and platforms we already use.
323
That's what we try to do here.
324
And we'll keep examining these questions in the weeks to come.