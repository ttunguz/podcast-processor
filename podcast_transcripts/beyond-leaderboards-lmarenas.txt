--- METADATA START ---
Show: AI + a16z
Episode: Beyond Leaderboards: LMArena’s…
Host: Anjane Midha
GUESTS: Anastasios N. Angelopoulos, Wei Lin Cheng, Jan Stoika 
Guests: Anastasios N. Angelopoulos, Wei Lin Cheng, Jan Stoika
Source URL: https://podcasts.apple.com/us/podcast/beyond-leaderboards-lmarenas-mission-to-make-ai-reliable/id1740178076?i=1000710577136
--- METADATA END ---

1
Thanks for listening to the A16Z AI podcast.
2
We have a fascinating and lengthy discussion for you today, so we'll keep the introduction brief.
3
If you're familiar with the world of generative AI models, you're likely familiar with LM Arena, the leaderboard and competition space created and managed by a team at UC Berkeley.
4
What began with a focus on language models has since expanded to cover vision models, coding models, and more.
5
And very recently, the team behind LM Arena announced they're starting a company to scale the project's reach and its impact.
6
They want to amass a global community of AI users and use their collective experiences and ratings to make AI models more reliable and to help everyone find the right model for the right use case.
7
So, without further ado, here are LM Arena founders Anastasios N.
8
Angelopoulos, Wei Lin Cheng, and Jan Stoika discussing the state and future of AI evaluation with A16Z general partner Anjane Midha.
9
They kick off the discussion discussing the importance of mass-scale real-time testing and evaluation right after these disclosures.
10
As a reminder, please note that the content here is for informational purposes only, should not be taken as legal, business, tax, or investment advice, or be used to evaluate any investment or security, and is not directed at any investors or potential investors in any A16Z fund.
11
For more details, please see a16z.com/slash disclosures.
12
Yeah, that's a good way.
13
Sometimes I get asked, what's the last exam that AI should take for humanity?
14
And it seems like that's the wrong question to ask.
15
We should be asking, what's the real-time exam you want your AIs to be taking before they get deployed every hour, every second of the day, especially as we start to get like, I think one of the things that's emerging for me is that the arena is misunderstood partly because we're just early in AI.
16
And so while benchmarks like MMLU and the idea of these static exams were useful three years ago, the future is about real-time evaluation, real-time systems, real-time testing in the wild.
17
Now, one thing that concerns a lot of people is the reliability of these systems.
18
When we start going from chatbots that are good at, let's say, companionship and more consumer use cases to mission-critical systems, defense, healthcare, financial services.
19
How will Arena have to evolve as we go beyond companionship or web dev to those kinds of mission-critical use cases?
20
I think that's one of the very reasons we wanted to create a company to support this project to further scale the platform.
21
So, right now, we are at million months of user now, but if we scale it to five, to ten, or even more, to capture even more diverse user base across different industries, and then in that case, we'll have the ability to really like zoom in into all these different kinds of like areas that people really care about for critical mission tasks that it will be used to.
22
Yeah, you can imagine we are going to, when we are going to scale, we can have microsites for nuclear physicists, radiologists, and so forth, right?
23
And these experts are going to come there to get the best answers to their research questions.
24
So, that's interesting.
25
Is there a future where now that Arena is becoming a company, you could see a scientific lab or a shipping company or a defense company deploy their own arena on their own infrastructure for their own users or their own prompts?
26
Essentially, many people have asked us for this already.
27
So, these would be sort of private arena.
28
Private evaluation.
29
And it's worth saying, I think when people have these mission-critical industries in mind, they often are thinking about the factual nature of the responses and so on and so forth.
30
But in reality, even in such industries, the majority of questions that people ask are subjective.
31
Okay, so the mythology that in hard sciences or in mission-critical industries, people just have like cut-and-dried questions and they just need like a retrieval and a lookup, that's completely false.
32
That's the very reason why these models are useful is because they allow you to sort of like interpolate between these weird questions and answer questions that are not fully specified and give responses that are sort of like geared to answer the question, but might not have a fully factual basis, right?
33
They might incorporate factual elements through rag, let's say, but there's a subjective nature to the response.
34
And that's a reality that everyone's going to have to live with.
35
If these systems are going to be deployed in medicine and defense and so on and so forth, they're going to be deployed in places where the data is messy because that's where they're useful.
36
Okay, given that fact, how are you going to make sure that they're reliable?
37
Well, we need something like Arena.
38
You know, at this point, Arena is hard to miss in the AI space, whether it's Grok3 coming out and Elon putting it up for the bulk of the keynote or Demis using the WebDev Arena scores to kind of demonstrate how good Gemini is.
39
It's sort of become the standard bearer for evaluation and testing at all the big labs.
40
Right.
41
But does that mean you guys have been helping them more than open source labs or smaller labs?
42
No, we work with model providers, small and large.
43
Okay, so we work with basically anybody who wants to work with us within our constraints, try to be as helpful as possible.
44
The fact is that part of the reason to build a company is because we use.
45
And in fact, one of the things that we help everybody to do is pre-release testing their models.
46
Okay, so it's not just that we work together to evaluate the models or release, but we also try to be their release partners and say, hey, can we help you guys pick the models that do best on our user base and use that as a guideline for which models they should actually release to the world?
47
So that's the way our platform works.
48
And that's getting us closer to something, as we've talked about, like reliability is so important, these subjective measurements are so important.
49
How are we going to get to a world where there's a CI CD pipeline where people can test their models pre-release and make sure that they're doing well for all sorts of different diverse people?
50
Well, you need something like Arena to do that, and that's part of what the company is geared to do.
51
So what people do is they can come, they can test a bunch of different models.
52
We do this with basically every provider that comes to us.
53
And they can see, oh, which one's doing better or worse on the distribution of arena users?
54
And then we can use that information to help them decide which model to release.
55
After they decide to release a model, it gets continually evaluated forever.
56
So that's where you get the freshness of the data.
57
The model continues to be tested.
58
And we're pushing towards a world where these subjective and human considerations are part of model developers' final release pipeline.
59
So if I'm hearing you right, the more testing, the more reliable we should expect AI systems to get.
60
So we should be like Kuzeni self-hosted stuff.
61
This is one of the fundamental debates in the space, right?
62
Is what is the measure of progress?
63
And there's a body of work that tries to create exams.
64
They call them harder and harder exams.
65
And for two reasons, I've always found Elementary Renaissance.
66
One, it's the opposite approach, which says, let the wisdom of the crowd guide us.
67
And two, let open source actually define the examination.
68
I think this is quite important.
69
Where if you get a group of experts in a room who decide what the right exam is for humanity, inevitably, then if it turns out that group's values get encoded in that, we have no way for the rest of the world to use AI systems that are measured by a different set of values.
70
So it's great that people do these expert evals.
71
It's totally fine.
72
It's orthogonal from what we do.
73
I'm glad we have it.
74
But at the same time, you have to ask yourself, what makes somebody an expert?
75
What are they an expert on?
76
I think the whole world is moving in the direction against like experts being the be-all, end-all of everything.
77
And everybody actually has their own opinions, and everybody has their own point of view.
78
And in fact, there's so many natural experts in the world on all sorts of topics that they don't necessarily need a PhD in order to be really intelligent and high-taste and have valuable opinions.
79
And I think that's one of the things I'm proud of with the arena: it allows us to actually go and say, hey, where are the natural experts?
80
And actually, can we find data-driven ways to identify them?
81
What if we can go look and say, hey, this person here in this random part of the world is actually incredible at coding and math?
82
Their vote actually means so much.
83
And we hope to be able to scale it further.
84
Okay, but I'm going to push back a little bit on this.
85
I'm going to channel a few of the criticisms I've heard from experts, which is that: look, if you're an expert, you've been blessed with the brains, the resources, and so on to be a highly educated individual in your field.
86
We have a responsibility to guide humanity.
87
It's our job to actually guide the masses.
88
We should be defining what good human preferences is versus not, because the masses actually don't know what's good for them.
89
The everyday users, the lay person, prefers slop.
90
Right?
91
We've heard these arguments.
92
Yeah.
93
Is there a grain of truth, or how do you think about this?
94
So I think a few things here.
95
So, one, like Anastasio said, it's this alternative to have hard exams, to create a hard exam, is valuable, no question about that.
96
I think the other thing I want to point out, and I took that at heart, is this kind of criticism and about kind of having expert labeling.
97
Now, I went to quite a few experts I know and I respect and ask them, would you label it?
98
Would you label?
99
Almost everyone told me, No, I don't have time.
100
So there is a question there about: Are you going to get really the experts?
101
Right?
102
Right?
103
And I don't know, I don't think so.
104
You're getting some people from that area who are willing to.
105
But these people, if we offer to them, and we are not doing now, I'm talking about the future, a platform where their community comes there and ask questions to push the boundaries to help with their research and things like that.
106
They are going to be, first of all, you are going to get these people, right?
107
Because they do that in order to advance, again, their research.
108
And we are going to get also their votes.
109
So, fundamentally, I do think that we can get in arena.
110
Again, right now, I'm not talking about today, I'm talking about the future.
111
It even happens today.
112
It even happens.
113
It even happens.
114
So, you get real experts, right?
115
The top ones.
116
Okay, the answer.
117
Unhireable people.
118
That's a great way to say it.
119
The other thing I want to say about the layman and so forth, Ange, you fund a lot of companies.
120
You are funding a lot of companies, AI and A16Z, and you are on the board of many companies.
121
What do they build it?
122
What do these companies build?
123
What is their product?
124
Who uses their product?
125
Right?
126
Right?
127
He's not the top experts, he's a layman.
128
These are their users.
129
That's how OpenAI makes money and so forth, many others.
130
So then, wouldn't the evaluation take into account the preference of these users?
131
Right?
132
The answer is obviously yes.
133
So, how can something again like these exams are very important to understand?
134
Again, no question about that.
135
But they are not going to reflect, MMLU is not probably as good as reflecting the preferences of the users of these AI products and services.
136
I want to just dig one level deeper here, which is that one of the things that we are really excited about is the question of why do people vote the way they do?
137
Who's voting left-right?
138
Why are they doing it?
139
On what kind of prompts do they vote for one model or another?
140
On what kind of topics or models better.
141
Basically, can we decompose human preference into its constituent components?
142
Let's say you have a criticism.
143
You say people vote based on slot.
144
Emojis are driving votes, and response length is there's a huge response-length bias, which is it's true that people vote for longer responses preferentially over shorter responses, even given the same contents or well-known human bias.
145
This gets back into RL.
146
Can we learn this bias and actually adjust for it and correct for it?
147
And the answer is yes.
148
That's why we're making style control default.
149
So, what we developed is this method called style control, which allows you to run not just, let's say, a Bradley terror aggression, but also includes certain covariates that model the effect of style and sentiment on how people vote.
150
And what you get when you fit this model is not just a prediction of preference, but also an understanding of why people are voting the way they do.
151
We're trying to target a causal quantity, which is the causal effect.
152
Okay, there's always more work to do to get things closer to the actual causal estimate that we want.
153
But if we continue to decompose human preference into its constituent components, what we're building is an ever-richer evaluation that can tell us all the factors that go into response, how you can optimize people's preferences for keeping style fixed.
154
Let's say I want to remain concise, but I want to maximize your preferences.
155
Okay, how am I supposed to do that?
156
Well, not a lot of people have that information, but we're building the methodology that allows you to do that.
157
Okay?
158
And the question is: can we disentangle style versus substance?
159
That's it.
160
There is an effect and you want to know about it.
161
Period.
162
The platform helps you.
163
And what is the impact?
164
And what is the impact?
165
Yeah.
166
So this is another, I think, important area to dig into.
167
There was a moment where you guys decided that it was insufficient to keep measuring the progress of the models on coding with the base design of the platform, which is just chatbot arena.
168
And I remember seeing a launch which was WebDev, I think you called it WebDev Arena, and showing up to a completely different interface.
169
And then realizing that this was a pretty big change for you guys, right?
170
Why did it, why did we need a new kind of arena to correct for that effect?
171
Why was that necessary?
172
I think that comes back to, I think, Jan's point, but like when you view a product, an AI product, you want to know about how people use it, you want to understand.
173
And in order to collect that kind of data, you have to build sort of like a product first.
174
And then, as we know, over the past few years, people have been building very different kinds of applications on top of AI beyond just chatbot.
175
So, chatbot is one of the widely used interface right now for humans to interface with AI.
176
But these days, people are applying these models to coding and then more like tool use, agenda, behavior, that kind of stuff, right?
177
So, that's the first step.
178
We were thinking, like, okay, how can we capture all these use cases?
179
And then the answer to that is we have to build something that people can use again, the same environment, that people can test in real time to give us real-world feedback.
180
So, that was the original idea that was kind of like last summer, just at the beginning of this kind of text-to-web, text-to-app trend.
181
At the beginning, was very beginning with cloud artifacts.
182
That was the first.
183
And we saw that we were amazed by that.
184
And then, how can we do eval for that?
185
We have to give credit to Arian on the team.
186
Yeah, and then Arian basically was like just joined the team, and then he was interning actually.
187
So, we were saying, okay, why don't we build something new?
188
And at the time, cloud artifacts, how can we do evaluation for that kind of applications, text-to-web?
189
And then that was the idea.
190
Once more, financially, so far, we talked the serial files, but eventually, the team goes.
191
And right now, I don't know, it's like almost like 20 people, and both graduate and especially undergraduate students, and doing a lot of very exciting and interesting work to expand the abilities and capabilities and the reach of a chatbot arena.
192
So I just want to make sure that other colleagues of mine are involved here, and Flag Joey and others.
193
And I think the credit goes much more beyond the 30 fast.
194
But that's what we want to provide.
195
You want to, whatever you want, we hope that you'll find the answer.
196
Fundamentally, if you're looking at the leaderboard, there's only one thing really that matters, which is: do you care about the preferences of the community people that come to vote on our platform?
197
That's it.
198
That's what we measure.
199
It's the only thing that we claim to measure.
200
We don't claim to be an AGI benchmark.
201
We are faithfully representing the preferences of our community.
202
So that's why it's so important to us that we continue to grow our community.
203
And we get a diverse community of all different people, experts, non-experts, artists, scientists, different languages.
204
Everybody under the sun.
205
We want to come into this platform to express their preferences.
206
Because if we can get that to happen, it already happens to some extent.
207
But if we can continue to grow it, what's going to happen is, again, in order for a model to do well, what needs to happen is new people need to come in and vote for it.
208
And so if we can provide this lens into the preferences of the world, it seems not like we have the study, you want to talk about the freshness, right?
209
About we always see fresh prompts, right?
210
It's not like, oh, we are going to see all these prompts over and over again, like kind of saturated.
211
That's not the case.
212
That was the very beginning of why we believe Arena is fundamentally different.
213
Related to this contamination.
214
The very beginning of ARENA is to try to solve the contamination problem, the overfeeding problem, which is like people test models on a static benchmark.
215
Or what people call it overfeeding.
216
Yeah, what people call overfeeding.
217
So, how do you overcome overfeeding?
218
You collect new data, right?
219
That's how you overcome overfeeding.
220
And an arena is designed to collect new data every second.
221
So, all the questions are asked, are new, like all the votes are new.
222
And then we measure basically how different, what's the difference between all these problems, right?
223
What's the distribution look like, and so on.
224
And then we conservatively estimate like over 80%, something like that from the same thing.
225
If I say something different, correctly.
226
And this study was done by another member of our team, Lisa, and basically measures out how many more fresh prompts you have in one day compared to what you've seen in the past three months.
227
Right.
228
And by a similarity score of something like 70, 75%, you have over 70 of these prompts are fresh.
229
So you mean by 80%.
230
Or over 80%.
231
Right.
232
So it's a large number of prompts are fresh.
233
It's not like, oh, they're identical.
234
No.
235
Yeah, just to dig in one click deeper into what Wei Lin said.
236
What is overfitting?
237
Static benchmarks overfit.
238
Why?
239
It's because, as Jan said earlier, you're giving the student the same test over and over.
240
You have a model, you test it, you look at whether or not it's improved on a static data set, then you find another model, you test it, and you pick the one that does better and better.
241
And what ends up happening is that the test becomes meaningless.
242
Because you've seen it so many times that you've memorized the answers.
243
That's what overfitting is.
244
Jobbot Arena is immune from overfitting by design.
245
Because you're always getting fresh questions.
246
In order to do well on the arena, new users need to come and vote for your model.
247
That's it.
248
That means that users like it.
249
One thing I've noticed is that the same researchers who often argue with me that Arena is a terrible evaluation system, the leaderboard is not to be trusted, the scores are rigged, they're gameable, have, one, tended to also celebrate when they're on top of the leaderboard.
250
And the second is I find actually increasingly, especially for specialist arenas like WebDev, there's a natural tendency to just accept that this is a really good indicator of the underlying capabilities.
251
Why is that?
252
Why is WebDev Arena such a good proxy for actual performance improvements when it comes to a capability like coding, which is quite general purpose?
253
It's very counterintuitive.
254
Programming is actually a very general purpose discipline and skill, and yet it seems like the capabilities in a very general way are still being able to are captured well in a specialist arena like WebDev Arena.
255
Why is that?
256
Yeah, first, let me just say, I think that all of these arenas have signal in them.
257
It's not just WebDev Arena.
258
It's just people have more opinions on language.
259
So WebDev is a little bit more objective.
260
It's easier to see the website you build it, and this one's better than the other.
261
There's a lot of signal in it.
262
Also, it shatters the models.
263
And what I mean by that is it can be very clear that one model is way better than another on WebDev Arena immediately.
264
Bam, you see it, and it's like.
265
Why is that?
266
Just the capabilities of the models.
267
Maybe William can say more.
268
I think it's just a much, much harder task because it's like from text, a description of a website.
269
And you have to first understand the request and then build, like, write code, right?
270
And then the code has to fit, maybe satisfy certain requirements, let's say a style requirement or like component, that kind of stuff.
271
And then it has to compile, right?
272
It has to be, we basically run it live in browser and then with connect to a sandbox, that kind of stuff.
273
So there are a lot of like parts the model has to get right in order to build a website that people can really interact with.
274
So fundamentally discriminates much better across the models because it's a much harder problem.
275
So very few get it right.
276
So it's in reality, for critics who say these static exams are hard tests and arena is an easy-to-game benchmark.
277
In fact, WebDev Arena is a great example that shows it's a really hard.
278
It just proxies the real world better than some static multiple-choice question test.
279
Is that roughly right?
280
Yeah, for sure.
281
And then a lot of like every input from user is for a real-world task.
282
They are trying to build some website real.
283
And then that also measures something that's like beyond just academic benchmark that we imagine what user would do.
284
Just like really trying to approximate the user intent, user preferences directly.
285
I have to say, I also just like completely disagree with the foundation of the question.
286
The like implicit assumption is that like chat is easy or that it's even easier than web dev.
287
That's completely false.
288
It's a completely naive perspective that people have on this.
289
Because it's hard to build something that people love.
290
People are good at chat, but you like some people way better than others.
291
It's subjective and everybody has their own opinions.
292
And the landscape is very rich.
293
You might like a very different model than me.
294
Somebody else, let's say a musician, might like a much different model than I do.
295
And understanding all of those differences is really hard.
296
And anybody who thinks that is gamable is deluding themselves.
297
To take that strongest form argument then isn't the right way to allow me to evaluate whether the model is good or not, to allow me to generate my own leaderboard?
298
You as a person?
299
Absolutely.
300
Yeah, and we should be giving you the tools to do that we're currently building them.
301
So this is quite profound.
302
You see the world going where everybody has their own personal arena?
303
Absolutely.
304
I was supposed to.
305
It should be personalized, just for you.
306
You should understand which models are better.
307
And it's going to be for your task, it's person and task, right?
308
Because for a different, if you want to do different things, you may have a different leaderboard, right?
309
If you have a question about tax a day, you go to different people than if you have a question about programming or whatever.
310
So that's kind of also going to depend on what task you want to accomplish.
311
And I really want to go back to one thing.
312
I think that because it's, I try to think quite a bit about all the criticisms because on the face of it, intuitively, for some people, makes sense, right?
313
That's why many people make the same criticism.
314
And I think there is another thing going on.
315
We as humans, we believe that why do people say arena is not good because people are fooled.
316
Right?
317
That's fundamentally their kind of argument.
318
They are fooled because long answers, more emoji, and things like that.
319
And when I look from that perspective as a human, what I in my mind is that I am not going to be fooled.
320
That's what it's not relevant.
321
These guys are going to be fooled by these things.
322
No, I am not.
323
That's why it's not good.
324
That's why it's not a good proxy.
325
The problem is that you are, we are fooled.
326
Everyone is fooled.
327
And this is actually, that's why the chatbot arena, it's like Anastasios said, it's a proxy.
328
It provides a magnifying glass.
329
But, you know, all of us, we have our own peculiarities, our own culture, our own history built on interaction with different people, right?
330
That we have different perspectives.
331
That's fundamentally what it is, right?
332
And these preferences are not fully objective, right?
333
Because people say it's only on objective answers, but all of us are different.
334
That's kind of the fundamental disconnect between the criticism and actually what you provide and actually what everyone we believe needs, right?
335
To double-click on that a little bit, you said we all have our own culture, right?
336
And Ben Horwitz, we all know well, has a quote I love, which is, he says, culture is not a set of beliefs, it's a set of actions.
337
Right?
338
So let's say our belief, the philosophy that Arena has is that the set of actions a user takes when using an AI model is the best source of truth for whether that model is good or not for them, versus some third-party, closed-source evaluator telling you what is good for you.
339
And that's why, again, it's like going back to the previous that, you know, I think what you hear is that capturing the human preference is fundamental because we are building these AIs to interact with humans.
340
Right?
341
I think that's kind of the foundation.
342
But if you, like we have in the previous discussion, say, people say, well, like, yeah, but these other people are fooled.
343
This is not me.
344
Right?
345
Although I am fooled as well, but I don't believe, right?
346
You always believe about yourself that you are better than you are.
347
But then we can provide like style control, okay, so forth.
348
You can remove that.
349
Adjust for that effect, right?
350
And we are going to provide more and more.
351
You can adjust for that effect and that effect, right?
352
So you get your answers as well.
353
LM Arena started as a research project to take us back to how it began.
354
So it was started around two years ago in late April 2022.
355
And then at that time, before Arena, we were working on a project called Vikuna, which is like one of the first open models that's been released, like the ChatGPT kind of core.
356
Yeah.
357
And then at that time, LLAMA1 was just released, which is a base model, doesn't really know how to chat with humans, only gone through like pre-training process.
358
At that time, I don't think people call this pulse training yet.
359
People call it instruction fine-tune, that kind of stuff.
360
So we were like in the lab exploring how do we reproduce this, right?
361
How do we make an open source version of ChatGPT?
362
And then, credit to Liam Ming, he was having kind of like an idea that we could use some of the open data published on the internet.
363
That's kind of like user-Chat GPT conversation.
364
It was called ShareGPT.
365
And it was like a high-quality set of dialogues that users shared.
366
So basically, we come as a group, a bunch of PhD students in the lab, set an ambition goal, which is like we try to release this model, trend this model in two weeks, that kind of stuff.
367
And then during that process, the result was surprisingly.
368
At that time, we were like playing with the model, and then we thought we gotta demo this to the world.
369
So we basically just set up a website and then put that model on the website and then release it.
370
At that time, it was like there's like a huge debate, like internally, like how should we, when we release it, how should we evaluate this model?
371
How good is this model really?
372
It vibed well, right?
373
Because when we compare it to Lama, the base one, you can feel the difference, right?
374
Lama just learned how to chat, learn how to speak like a chat GPT.
375
So, there was a huge debate, like how do we evaluate this?
376
At that time, we didn't have much time.
377
So, we were like, okay, we either do this kind of like labeling, come up with questions ourselves, and label the data and then compare it with other models, or we do something like something automatic.
378
And then at that time, there was like GPT-4 just came out in March.
379
People were like wondering what can they do, right?
380
And then we were like, okay, why don't we just use it for evaluation?
381
We use GPT-4 as a judge to do this automatic evaluation.
382
At that time, no one believed it.
383
And then it was like, again, huge debate, but we didn't have time.
384
So we just ended up doing it.
385
And it worked surprisingly well again.
386
And we release it.
387
But the huge problem after that is like, still, there's an open problem, which is like, how do we evaluate these chatbots?
388
So soon after, we come up with that idea that why don't we let everyone in the community vote which model is better?
389
Because at the time we serve that model and then we also serve some of the other open source model at a time.
390
At a time, every week there's a new finding.
391
So we register a website, we demo them, all of them, and then we come up with kind of like a side-by-side UI that people can compare them.
392
And then soon after, we say, okay, why don't we come up with a battle mode, which is like we anonymize their identity and let people vote.
393
So that was the original arena, which is basically trying to solve our problem, which is how do we evaluate this model, understand this model's difference.
394
Actually, the first time we did start, we tried to have some students buy some pizza, get them in a room, and label the replies from Vicunia and other models to compare them.
395
And then obviously they didn't scale.
396
And then it was this LLM as a judge.
397
We tried and it worked surprisingly well.
398
And then it was quite a bit of debate.
399
Okay, we are going to build a platform to scale because the question is still, okay, it seems that anecdotal, like GPT-4, it was released just two weeks before, before we started to use it as a judge.
400
Still, the question, okay, yeah, it seems that it's doing well, but still it's like, how about how does it compare with humans, right?
401
And that was a question about, okay, how do we scale the human evaluation?
402
And we discussed quite a bit about how to do it, because it was not clear, because if you think about before, you just ask people, I have a prompt, and the prompt is answered by all models, and then you label it, right?
403
Good and bad.
404
That's kind of the typical way that how would you scale up the process.
405
Then you need to kind of to rank them, right?
406
And you have n choices, answers of the same prompt from n models.
407
It's very hard to rank them, right?
408
We get slightly different in tone and so forth, try to rank them.
409
And then we saw quite a bit, I think, the inspiration about was how humans in real life rate, say, players or teams in games, right?
410
And obviously, you have the tournament, that's one way to do it, where you have, so to speak, the players play with each other head to head, and then based on that, you are going to have some number of points to win or lose or to end or die.
411
And then you are going to have a leaderboard, right?
412
And but that again, the problem is that if you have a tournament, typically the assumption in a tournament is that the number of players do not change across during the tournament.
413
And also in general, most of the tournament, each player has to play with everyone, so it's kind of an n-square problem or n is the number of players.
414
And then we thought about, okay, there are other ways in real life how players or teams are ranked when they don't play with each other.
415
They don't have a chance, either because the number of players is too large, or you need to also accommodate new players entering the game.
416
And that's why we were about then we thought about, okay, there are disciplines where this is done, like chess, that tennis, ATP rating, and many others.
417
And that was the idea.
418
And we said, okay, why don't you do something like ELO score?
419
Okay, and for that, what do you need?
420
Well, you need only head-to-head.
421
And not everyone is to play in the same tournament.
422
And that's how you adopted.
423
And that's why Arena has this battle mode, and you have a prompt and so forth.
424
And when was the moment when you joined the conversation and brought the, from what I understand, the Bradley-Terry approach?
425
Yeah, it turned out it was like deeper technically than we thought.
426
So at that time, Jan was like, we need to find someone to back this up more like on the theory side, to have a solid foundation to rank all these models.
427
At that time, it was no longer really a fun project anymore.
428
It was started as a fun project.
429
And people started to pay attention to it.
430
So you better do something.
431
And I went to Michael Jordan, my colleague, very famous machine learning AI researcher and faculty here.
432
And I've been working with him actually when we built this kind of labs at Berkeley, cross-disciplinary labs.
433
We were working with him in 2005, 2006.
434
He was joining the system people, database people to work together and exciting projects.
435
And he told me, oh, I know exactly.
436
I have the guy for you.
437
It's Anastasio.
438
I saw what was being built.
439
At the time, Arena was not so close to what it is today.
440
It was, I think, not that much usage.
441
I saw it and I thought, wow, what a great opportunity to do some interesting statistical modeling and theory.
442
Like being able to understand how do we optimally sample models?
443
How do we perform this estimation?
444
Okay, let's move from ELO to Bradley-Terry because we're actually performing an estimate here instead of just like, and the ELO score moves over time.
445
It doesn't converge, but Bradley-Terry models converge.
446
And how do we then construct confidence intervals properly for this estimand and so on and so forth?
447
All that stuff was super interesting to me.
448
We were meeting like just next two doors down.
449
And we wrote on the whiteboard like five or six different topics.
450
Yeah.
451
I mean, we started working on them and the rest is history, right?
452
In many ways, I feel like this is the birth of Arena.
453
Couldn't have happened anywhere else other than an interdisciplinary lab at a university research, fundamental research university like Berkeley.
454
Is that true or do you think?
455
Well, it certainly would have been worse if it came out somewhere else.
456
And the reason is because the fact that we come from Berkeley and from a university really speaks to our scientific approach and neutrality.
457
I think if it came from an industrial lab, people would always have questions about, oh, well, these people are also training a model and what's their incentive and so on and so forth.
458
But the reality is we were just students.
459
We were doing this in order to evaluate models and it came from a scientific perspective.
460
That's it.
461
And I think that's something that people can see when they look at us and builds a lot of trust in our business.
462
The other angle here is that in a lab like this one, what you get, you can get, maybe in industry, you can also get maybe interdisciplinary teams, but these are going to be large teams, right?
463
Because you go, okay, you are going to do the team which is doing AI, the team is doing systems, work together.
464
These are already large teams.
465
But here, what you get are small teams, a few people, which everyone can come from different areas.
466
We have people who are kind of systems.
467
Airbion, we have to build the systems to serve these open source models.
468
We have to serve this virunia, like Royne mentioned.
469
Actually, when we use this Channel GPT data, we are doing quite a bit of data pre-processing in order to pick some data curation and so forth.
470
Then when Anastasio joined, we have now machine learning experts.
471
But the team still is like four or five people.
472
And early on, small teams move very fast.
473
So I think that's kind of the difference.
474
You have a very small team, but interdisciplinary and small.
475
So I think you may get in, industry may get in interdisciplinary things, but they're going to be large.
476
If you could just teleport back in time to that moment in early 2023, correct me if I'm wrong, but if I had to kind of summarize the research environment in the Bay Area at the time, most people basically were extolling the death of AI in academia, right?
477
The idea that, oh, you can't really do any serious research.
478
You can't contribute to the frontier of computer science or in AI from a research institution.
479
It was quite common, actually, if you remember that.
480
And there's nothing more satisfying than proving these people wrong.
481
Right, right.
482
And so where do you think people got wrong?
483
I think that if I may take a step back here, because I'm old enough, so I've seen a few of those.
484
I remember when I was a student, I was doing system work and networking.
485
That was the internet days.
486
And I was going to these conferences, and there were panels.
487
Are the operating systems dead?
488
Research in the operating system.
489
Is that dead?
490
That was the topic of the panel.
491
And the reason for that was at that time, it was at the end of the previous century, it was Microsoft was dominating, and then Apple, and then, of course, there were some free BSD and so forth.
492
But then, although it didn't come from Academia, it was Linux.
493
Actually, we did, Linux was preceded by Minix, which came from Academia, from the Netherlands.
494
So that's one.
495
Then in 2004, like when we came here and we started this lab, there was a question also that was this kind of distributed system, because a lot of researchers on distributed systems, what can Academia do?
496
Right?
497
Because it was Google was doing all the research of Q systems, membrane, Google File systems.
498
All of that happened at Google.
499
Best people going at Google and so forth.
500
And then we've done here, then it comes Spark, which also comes from Academia.
501
And I think that's when this started.
502
It was actually the question people were surprised about Bikunia.
503
Just a bunch of students and actually their own initiatives.
504
I knew almost after the fact this happened.
505
Pick this kind of data set from the internet, which was high quality, and use it.
506
And people were so surprised about the quality.
507
That was where people were asking, is this real?
508
So I want an evaluation.
509
I'm also kind of, you just show me some stuff anecdotal, right?
510
And okay, it's looking good, but how about is that?
511
Some people didn't believe it.
512
Yeah.
513
And then say, this is a GPT or wrapper.
514
Yeah.
515
I remember that service.
516
I remember we were at Europe later that year, right?
517
And I remember Elen was sitting at a table next to me, and a really well-known, famous researcher who's still at OpenAI asked me, oh, is that the team that worked on that Vikuna bot?
518
And I said, yes.
519
And he said, oh, yeah, I've been wanting to have a conversation with them because we think they're violating our terms of service.
520
Because they're just reselling our GPT-4.
521
And I don't know if he came and confronted you, but that was very much the default assumption people had, right?
522
It was a disbelief.
523
So because for a game, for a while, those are the best open source models, I don't know, three months, four months, or whatever.
524
So that's why the evaluation was so important back then, right?
525
Because it's disbelief.
526
So we tried to support with some evaluation, which seems more objective, that indeed it's a good model.
527
Again, since then, there are many other things we've done here and open source, like inference, LLM inference, like BLLM and HGLAN.
528
But I do think that what happens in this kind of sense in industry, I was actually on a panel yesterday, was saying this kind of discussion.
529
Okay, it's like, you know, what can maybe academia should do this thing.
530
And the industry should do this thing, right?
531
Like, let's, as the industry just can't do anything like training and so forth.
532
I think at the end of the day, it is about what resources you have and what problems you solve.
533
And over again, I think through the example I gave, if academia has the resources, it's going to surprise you.
534
Clearly, it's going to be at the very edge of innovation and creativity.
535
And so that's always.
536
In this case, of course, we had, we didn't need huge resources, right?
537
It's just a group of smart, passionate students.
538
So when Chatbot Arena started, it was a lot of excitement and so forth.
539
But then it was this, okay, it was a feeling, at least for some people in the group, that we are done here.
540
We publish the paper and so forth, right?
541
For a while, even if you look at the usage, it's kind of dropping a little bit and it's almost almost like.
542
And then Waylin, at that time, his main thrust of research was different.
543
Some graph neural networks, distributed graph neural networks, and things like that.
544
And I remember at some point in one of our one-on-one meetings, Euelin came to me and said, look, I really like, and I want to, instead of doing this kind of work, I really am passionate about shared waterna.
545
I want really to do it, right, and to focus on it.
546
And then when kind of then it started, and Waylin is like one-man back-end starting and marketing.
547
And he started to add more models to the leaderboard, market it and so forth.
548
And very soon after that, Anastasio came, and then it was kind of magical, right?
549
You have these people who are so passionate and they are working so well together, they are so complementary in skills and even personalities.
550
Then it started to shoot up, right?
551
And I'm mentioning that because without that kind of inflection point, which came long after it started as a project, we wouldn't be here.
552
I think there's a chart I saw recently that showed that compared the number of models being released and tested on LM Arena per year over the last two years.
553
And if you look at Q1 of 2023, it was, I think, two models.
554
And if you look at just this past quarter, I think there were 68 models or something like that.
555
In total, that first year there were about 12 models or so.
556
And today it's over 280 or something on the platform.
557
So at some point, it took, it sounds like it took the two of you realizing that this deserved to be more than a one-off paper.
558
When was that?
559
I still remember when we worked on the paper for Chatbot Arena, it was like a couple weeks of really hard work, and we were like pushing all the way until the deadline.
560
And afterwards, I turned to my girlfriend at the time.
561
I was like, you know what?
562
I think this is going to be a pretty good paper.
563
You'll still have the pay-the-stage.
564
And yeah, Wayland and I were talking at the time, but I think we started very early thinking about this, what this could become, and trying to de-risk it in various ways and trying to build it and seeing, hey, is this growing?
565
Can we keep building on it?
566
Another feel that really drives the growth is competition.
567
The competition of AI has become much more intense in early 2021 cloud three.
568
So let me tell you, to answer your question, because I think that it's very interesting, and maybe I have a more kind of unique view.
569
I start with other companies which are based on projects coming from this lab, like Database with Spark and or NISCAR with Ray.
570
And there, the notion was pretty clear.
571
You have a successful open source project which gets more and more popular.
572
And then there are some companies which start to use the project.
573
And then you get to the point and say, okay, if I'm going to bet on this project, be part of my infrastructure, what happens when the students who build it, like Matthew and so forth, graduate?
574
Who is going to maintain it?
575
Who is going to evolve it?
576
So in that particular case, it's kind of natural.
577
Okay, if really this gets very, to get even more successful, you have to have a company backing it.
578
Whether it's a new company or an existing company, and if there is no existing company, it's people who are on that project, if they want to push it further, almost like you have to start a company to have enough resources to push it.
579
But this was different for the reasons Anastasio said.
580
It's kind of, we are very clear, it's kind of a trust neutral.
581
And Whelan, I remember, mentioned to me like one year ago, he's like, I think maybe we should have a company.
582
And I told him, man, what you're talking about, this has to be neutral, maybe we do have kind of foundation and so forth.
583
And this discussion actually went back and forth for a while.
584
I was even frustrated.
585
I'm telling this guy what I think this should happen, and we should be just kind of foundation and so forth.
586
And it comes to me, it's like not hearing, it's like telling me the same thing, right?
587
So we were trying to convince you, basically.
588
They were trying to convince me, right?
589
And then for me, it's like it was, I talked with some of this foundation and so forth.
590
But when it was very clear for me, is that when you started to get more and more demand and so forth, and there is no way you can, you need so much funding to build such a platform, right?
591
Because you need to save the models and you need to build an entire backend, scalable backend, and things like that to do it.
592
And then you are UX, right?
593
So when you look at that sheer amount of work, in order to push them to the next level, there is no way you can do it without having significant funding.
594
So that's kind of for me was a kind of inflection point.
595
But these guys can say more because they were convinced about this long before I was.
596
Yeah, another thing I think we were discussing last year was like when we were trying to discuss whether this can be really like a business that solves more fundamental problems in this space.
597
I think Anastasia at that time was like giving some perspective on ever more granular evaluation that we can provide with the data.
598
So you want to say more about that?
599
Chopout really, when you look at the leaderboard, runs like a marginal regression, which means that the leaderboard sort of ranks models on average across all users and all the prompts that they ask.
600
But there's a vision where you take this to the logical extreme where there's the overall leaderboard, then you can calculate.
601
But, you know, the real value is in: well, what if I can tell you which model is best for you?
602
What if I can tell you which model is best for you and your question for your business?
603
There's so many interesting methodological questions to ask there, and actually, they require a lot of resources to answer.
604
So, one thing that we've been working on recently is called prompt to leaderboard.
605
Prompt to leaderboard asks the following question: You give me your prompt.
606
Can we tell you which models are best for that prompt specifically?
607
Now, the problem is we've never seen that prompt before, we've only seen any prompt once or zero times, right?
608
Because most people don't ask every question under the sun.
609
Fundamentally, it's a hard question because the thing that you're trying to estimate is: what if infinitely many people came to me and asked the same question and then voted?
610
That's the thought experiment you're trying to run in your head that you can't really answer that question by running a standard regression.
611
So, instead, what we came up with was a strategy for training language models that can output leaderboards.
612
And it's actually a deep question because what essentially you're doing is you're training LLMs to output these Bradley-Terry regressions that we were talking about earlier.
613
And how do you do that?
614
Well, you have to make sure that as you train the model, the regression sort of naturally emerges from the data.
615
And the only thing you're getting is binary preference.
616
But nonetheless, it turns out that you can do it.
617
This has so much utility, and it requires so many resources in order to really scale up.
618
It converts the problem of testing.
619
You think about it as like, okay, how am I going to evaluate ML?
620
Well, I'll just like calculate the accuracy.
621
But the reality is that really doesn't reflect the heterogeneity of the performance of the model for different settings and for different people.
622
But instead, what Prompt Leaderboard teaches us is that you can convert the problem of evaluation into the problem of learning.
623
What if I learn something that can tell me how my models are performing in all different parts of the space?
624
It turns out that you can do that by training big language models.
625
And that because language models are sort of the intermediary that gets you to this evaluation, there's also a scaling law that comes along with it, which is to say that the more data you get, the bigger you build the platform, the better you can make your evaluations, the more granular you can make them, the more personalized you can make them.
626
And that's a very powerful idea.
627
And I think that's part of the reason why we're convinced, hey, this deserves to be a company of its own.
628
So fundamental technical innovation that's going to change the way people approach the space.
629
And let me try to follow that with a more, a less accurate explanation, but I think it drives home the point why the data is so important.
630
So with the pronto leaderboard, it's basically when you give your prompt.
631
And again, like Nanastasio said, we may have never seen the prompt, more likely.
632
However, what we have seen may be a lot of other prompts which are similar with your prompts.
633
So in Treaty, you can think that you can use the votes to the similar prompts as a proxy to compute how good are the models for your prompt.
634
Now, from this kind of maybe not as accurate analogy or explanation, you can see that the more data I have, the more prompts similar to your prompt I have.
635
So, the more accurate I can be.
636
There is another thing we didn't touch on, and what actually for me was I was so excited about the project early on.
637
And if you think about outside Vikuna and our own story, how people and still evaluate these models, you have this kind of benchmarks.
638
MMLU, Helm at that point, Sweetbench, all of these models.
639
The problem with that is that they are static.
640
So, you can overfeed them.
641
And at that time, if you remember, there are already starting to be discussions.
642
I'm talking about one year and a half ago or two years ago, about contamination.
643
There are some very high-profile examples.
644
And why?
645
Because these lag language models are going to train, as we know now, the data is elementary with a bottleneck.
646
So, they train on all the possible data they are going to get their hands on, right, in the internet, right?
647
And many of these benchmarks are also out there.
648
So, it's not intentionally, probably many, but they are going to train on some of them on the very benchmarks they are going to be evaluated on.
649
So, this is kind of another fundamental problem.
650
And I think that the unique thing about Sharwat Arena is kind of evolves over time.
651
We are thinking that, we are thinking that the way people typically evaluate these models is like.
652
Certainly, we don't do that, or at least we try not to do that, right?
653
I'm talking as a faculty now, right?
654
For each class, for each year, we need to give different exams, right?
655
So, that's kind of again, with a human the same thing, right?
656
To evaluate humans, to evaluate which kind of learn over time, like these models, you need to come with something, you need to evolve the benchmarks, right?
657
The exam, right?
658
So, I think that's kind of unique part and unique value of childbaut arena.
659
And probably these guys can say more about the kind of freshness and the evolution of the benchmark over time.
660
What are the biggest differences between benchmarking and evaluation?
661
So, let me just zoom out for a second.
662
Benchmarks, how are they collected?
663
What happens is that you ask a question or a given input, and then a human grades the output.
664
And then, what is the benchmark supposed to be?
665
There's an answer key.
666
A benchmark is like a test with an answer key.
667
A human has to look at it and tell you what's right or wrong.
668
The fundamental insight of the arena is that by virtue of the fact that we built this platform, we can do something closer to reinforcement learning.
669
Benchmarks are like supervised learning.
670
Arena is like reinforcement learning.
671
And supervised learning, you can only do as well as the best human that you have.
672
Because what's happening is that you're learning from the teacher.
673
In reinforcement learning, you're learning from the world.
674
You're able to learn things better than the best human could ever teach you.
675
Why?
676
Because you're only getting these preferences.
677
You're getting, was this good, was this bad?
678
Nobody needs to tell you.
679
Oh, your writing style needs to improve in XYZ way, and you should edit the sentence.
680
Forget all of that.
681
For the same reason why reinforcement learning has been so powerful in training language models, it is also powerful in evaluation.
682
It can capture things that you and I, if we were looking, could never understand how to encode.
683
It is the open world nature allows you to go back and mine the data in order to extract insights that are much more profound than we could come up with ourselves.
684
So this seems to be the fundamental tension, right?
685
If you, let's say you are a leader in the AI industry, your product lab or your product company, and you say, we believe the most valuable thing for us to do is build useful AI products.
686
We're not interested in benchmark hacking.
687
We're interested in making truly useful products.
688
If that is actually true, you should be strictly supportive of testing your systems more and more on arenas like WebDev Arena.
689
Let's say you want to build a useful web development AI experience.
690
Then you should want your teams to be testing more and more on this product, right?
691
You want to do well on the distribution of natural use.
692
Let's say then we expect anybody who's serious about building useful AI products to want to use testing environments like WebDev Arena more.
693
Why are people complaining that some labs are testing more than others?
694
And why are they saying that's a bad thing?
695
So first of all, I think it's worth saying that we offer the same level of service to all labs.
696
There's nobody that we treat preferentially or anything.
697
It's a neutral platform.
698
We want to help the ecosystem advance.
699
But second of all, addressing your question more directly, people do not yet fully understand the arena.
700
I think people still think about the arena as a benchmark.
701
People still think about it as something like, oh, people can overfit on this thing.
702
But what hasn't sort of permeated, and it's because it's just such a new way to approach evaluations, is when you have fresh data, you can't overfit.
703
It just means you're doing well.
704
Period.
705
There's no overfitting that can occur.
706
What can happen is you can do well, and you can argue with me about whether doing well is a good thing.
707
Okay, that's perfectly fine.
708
That's not where people's heads are at.
709
I think people's heads are still at, oh, you tested so much, and that must mean that you're, because people are used to it.
710
Because people, oh, it's stat 101, so on and so forth.
711
I know statistics.
712
If you're doing well on this distribution, that's a strictly good thing, all else equal.
713
And then people can choose, how much do I want to tune my model for chat?
714
That's your choice.
715
You can choose how much you care about this signal, and that's okay, too.
716
So I think it's a fundamental misunderstanding.
717
But I think as we go, as we continue building this, as it grows, people will become more educated on this topic.
718
And then I expect that the world will understand it.
719
And again, just to make sure, because we had the discussions with Sastasia early on.
720
So overfitting refers to the same data.
721
Right.
722
Okay.
723
Okay.
724
But when you do, like, you do supervised learning or something like that, then what?
725
You have data, trained data, and then you have test data, which you don't show during the training.
726
And you hope that it's going to do well on the train data.
727
So overfitting means it's doing well on the test data, but only on the train data.
728
If you think from that perspective, there cannot be overfitting because we have continuously fresh data.
729
The one thing can people say that it's a particular domain which is given by the set of users and so forth, and you are going to learn to do better with this domain, in this domain.
730
Which is perfectly fine.
731
Probably you should care about that because the domain is a group of people you care about.
732
But it's very different.
733
Overfitting, it's very particular meaning.
734
And what people think about here, oh, I'm going to do, when they use the term overfitting, I'm going to do well on, I'm going to learn how to do well on arena audience.
735
Right?
736
That's what I have in mind.
737
But it's again, that's fundamentally different.
738
Well, actually, so let's talk for a second for the arena audience, because you mentioned that's a critical part, right?
739
As opposed to continuing to train your model to perform well on a static distribution.
740
One of the things that shocked me when between the first time Wayland and I chatted in the beginning of last year to the end of last year was that arena traffic had grown by 10x.
741
The user base of the community had gone up by 10 times.
742
Why is that?
743
That feels like something people don't it certainly wasn't visible to me.
744
What's going on under the hood?
745
Why are more and more people using arena?
746
And in your mind, is that one of the reasons why people don't realize how hard it is to actually overfit?
747
Why overfitting is almost not possible on medians of people's preference?
748
And I think one of the reasons why people are kind of like surprised to see usage grow is because when they think about arena, they think about the leader.
749
They think about, again, a benchmark.
750
How would a million people use a benchmark?
751
That's strange.
752
But in reality, Arena is basically real-world testing.
753
And not just real world testing, the best AI from all the frontier labs.
754
Does the demand grow over time for people to test the best AI user best?
755
Yes.
756
So that's the very foundation of ARENA, which is like this is like an open space where everyone can come here to compare all the AIs for their own use cases for free.
757
And this demand we've been seeing has been growing, and we believe it has a very strong potential to continue to grow.
758
And in the same time, we collect all sorts of like comparison data that we can use for evaluation for all sorts of tasks.
759
So one thing that I want to point out, because we have been talking through this discussion a lot about votes, right?
760
The vote is a fundamental construct which allows us to evaluate this model and so forth.
761
It's the votes have to be sort of high quality.
762
If they don't have high quality, it's like you said, garbage and garbage out.
763
And we do believe, and there are two things, at least two things, we believe that the votes on ARENA are high quality.
764
One is that the people who ask questions are the people who evaluate the answers.
765
So presumably they are going to have the context for that question and for that answers.
766
As opposed to I have one question and two answers and I'm asking someone random labeler.
767
This is known from the information retrieval field for decades, and it's called gold standard when people evaluate the answer to their own questions.
768
When an expert evaluates someone else's questions, and the answer is called the silver, if I remember correctly.
769
But the second thing, people who give votes, who vote, in our case, are intrinsically motivated.
770
We are not asking them to vote, right?
771
They can choose not to vote.
772
Only people who want vote.
773
Relative to companies that pay human beings to vote and provide other kinds of incentives, like, oh, if you vote more, we can give you more resources or something like that, right?
774
Because you can imagine, you can easily imagine how they can get their incentive, wrong incentives, which are not necessarily aligned.
775
When I say wrong incentives, they are not necessarily aligned with increasing the voting quality.
776
One of the things that strikes me as I hear you guys talk about the design of the platform is that unlike these other paid services where you can just essentially hand out cash or incentives, when you have somebody intrinsically testing, the usage of the quality of a model, that starts to look more and more like software testing.
777
So, 15 years ago, when software systems were starting to be deployed to the internet, they were bugs, they were insecure, they were unstable, they were unreliable.
778
And so, as an industry, we developed the idea of unit tests and CI CD and A-B testing.
779
And today, software systems go through a sort of fairly reliable set of checks before they get deployed to production.
780
Am I wrong, or should I think about that as a pretty good analogy that we should want, if we'd like the progress of AI, the arc of AI progress to head towards more and more reliability, then we actually want model developers and AI developers to be testing their systems more before we actually get released to the world.
781
So I think that's kind of when it started, and this is another thing about exciting.
782
It's about, we do believe, and you can see right now, one of the main challenges of adopting AI in a wide area of scenarios, it's actually reliability.
783
Especially if you look at enterprises, right?
784
Is this answer correct or not?
785
That's kind of fundamental, right?
786
And that's, like you said, it's very similar with software systems.
787
And for software systems, we develop, like you said, this kind of long and sophisticated testing processes, right?
788
CI C D and so forth.
789
So you should think about that.
790
You need something like similar for these models.
791
Right now, are basically, tell the truth, like almost static benchmarks, right?
792
This is what we are doing, right?
793
You start training your model, and when the loss rate clutters, you start testing checkpoints, right?
794
And you have a 60, 70, 80 kind of benchmark, and you look at that in a spreadsheet, see which checkpoint is doing better, whatever, then you can measure them.
795
This is what happens, right, today, right?
796
But like we discussed, if you really are going to build your application for humans, okay, you can still test on your static benchmarks, nothing wrong with that, very valuable.
797
But you also want to test.
798
For all the reasons we mentioned during this discussion.
799
Yeah, so ideally, you want Arena to be the limit part of your CI CD for training the models.
800
We spent a bunch of time talking about how Arena was born and how the big idea, at least the theoretical idea, is that to unlock more reliability in AI, we need more testing of AI.
801
So let's spend a little bit of time going deeper on the practical realities of making that possible.
802
What are the hardest challenges when it comes to actually building the best testing platform to make AI more reliable?
803
Arena is a very interesting platform.
804
It's unique and it's kind of like N of one at the moment.
805
And so there's a number of like technical challenges that are actually quite exciting.
806
We're always looking to improve the platform, both from the methodological side and from the infrastructure side.
807
And what makes it unique is that it's this combination of AI, machine learning, converting evaluations into learning algorithms, like reinforcement learning side of things, plus like pretty large-scale infrastructure.
808
A lot of people don't know this, but Chatbot Arena is used by like a million plus monthly users.
809
We get like tens of thousands of votes on a daily basis.
810
We have like over like 150 million conversations that have been had on the platform.
811
It's massive.
812
And it's like the leading platform for these kind of like subjective real-world evaluations continuing to grow.
813
So the infrastructure side is actually quite challenging.
814
And then the question is, we have this like unprecedented data set.
815
How can we use it and leverage it maximally in order to actually target what we want, which is like the most granular possible evaluations and measurements of model performance?
816
Why is that hard?
817
Why is granularity hard?
818
Well, granularity is challenging because fundamentally, the questions you're asking when you talk about granularity is: how does it work for this specific individual, this specific prompt, or this specific use case?
819
That is a hard question to answer.
820
Why?
821
It's because you onj come to the platform where you ask three questions and you vote on one of them.
822
How am I supposed to tell which model is best for you?
823
It's like a sparse problem where what happens is that there's a big matrix of users and queries, and the number of queries is infinite that the user could possibly ask.
824
And the number of users is very large, and they've only asked three of them.
825
How are you supposed to learn which model is best for that specific user?
826
Well, you have to do something creative.
827
And the methodology for that, it relates to all these sort of like core topics that are very like deep in machine learning statistics, recommendation systems, so on and so forth.
828
But they come into kind of a new light when you think about language.
829
So one example of a problem that we're working on towards the future is personalization.
830
How am I supposed to create a personalized leaderboard for you?
831
Let's say I have your prompt history and a few votes.
832
Well, in order to run a regression that's just for you, I probably need hundreds of votes.
833
It's just going to be too high variance unless I have that much data.
834
But I'm never going to collect that much data on a user.
835
Or like only for the most power users am I going to collect that much data at the moment.
836
So we need a way that we can train in models that look at your interaction history and then can compare you to other users and pool between users so that you can create leaderboards for specific people, categories of people, so on and so forth.
837
That is a challenging and interesting problem.
838
And you need to do it using only this sort of limited information that we have, which is binary preference data.
839
How do you do that?
840
Well, it's a cool problem.
841
It's a hard problem.
842
And it's one that we like to have taken steps towards solving.
843
And it's not just personalization.
844
What about if I want to value the data?
845
What if I want to tell you which data points are high signal, which users are high taste?
846
What if I want to say, Anj, he's fantastic at bioinformatics, but when you ask him about history, this guy doesn't know what he's talking about.
847
Or what if you want to say, hey, this person right here, they're a local expert in this particular topic, and I really should upweight their opinions, let's say.
848
Or this person's just voting noise.
849
How do I take them out?
850
We need to be able to do tasks like these.
851
And they're fundamentally hard because of the structure of the data that we collect.
852
But they're also very exciting methodologically.
853
And we keep making progress on them, which is part of the reason why it fuels us.
854
And it's all enabled by this massive infrastructure and platform.
855
It needs to be done at scale.
856
It needs to be done very quickly.
857
And Wayland is kind of the expert on this, and he should speak to more of that.
858
Yeah, so before we go into infrastructure, I think one related note, and all sorts of problems we are looking at, like ML problem, which also related to recommendation systems in early days where people try to figure out the cold star problem, right?
859
You only have very few data points per user, but you are trying to do something, personalized recommendations for them.
860
Or Netflix, Netflix.
861
And as we lean toward like a more personalized world where like companies try to build AI products for consumer, everyone, that leverage all these user histories, prompts, the model has memories now.
862
So there's quite a new methodology needs to be developed, and in particular in this kind of like evaluation context.
863
It seems like there's two or three emerging frontiers of AI progress, right?
864
Relative to two or three years ago, where models were pretty simple, the vast majority of questions people had about the quality of performance of the models were mostly about in-context learning.
865
I give the model a couple of examples.
866
How good is it at predicting the next token or word in that sequence?
867
And it was a pretty simplistic measure.
868
Fast forward two, three years now, models have gotten extraordinary.
869
Models clearly look more and more like systems.
870
And one of the systems improvements that you've described is memory.
871
So relative to five, six months ago, when most AI assistants like ChatGPT didn't have memory, but now do, people are starting to notice a discernible verticalization of the model and the systems layer.
872
So famously, OpenAI has spent a ton of time both straining their latest model, 4.1 or 4.5 or whatever it was, with the assumption built in that the model has access to the user's memory and context.
873
When you have, how do you solve the problem of evaluating a model that, where the lines are blurring between model, system, application, this is turning into a full stack sort of product experience, relative to a model that, let's say, doesn't have all of those, right?
874
Because these, now these, relative to two, three years ago, the side-by-side base test was naively looked easier to do because it looked like Coke versus Pepsi or whatever, right?
875
Now it looks like a dessert versus an entree versus whatever.
876
I'm doing a terrible job of the analogies, but you get what I'm saying, right?
877
Chat GPT today, for example, has memory.
878
Claude doesn't.
879
These are two consumer apps that look very similar on the surface, but under the hood, fairly different.
880
The implementations are diverging.
881
And yet, on Arena, they are evolved side by side.
882
So, what does that future look like?
883
How do you guys disentangle the fact that the stack is becoming more and more verticalized and integrated across model, system, interface, application?
884
But Arena today is largely side-by-side evaluation of models that people are used to seeing, thinking of as basically symmetric systems.
885
Yeah, I think it's a combination of, again, evaluation would ever become more challenging and more specific to your applications.
886
Just like all software systems need its own CI-CD pipeline, that's a very different branch other.
887
I think the same thing would happen to all the AI products as well.
888
So, our belief is like, in order to collect data or evaluation that really means something that matters to us, to the app user or to the user, we have to build a real-world environment for everyone to test, to use.
889
That's also why we are like, and it's a combination of challenges of ML, product design, and engineering infrastructure.
890
Because ultimately, we are going to serve.
891
We are already serving millions of users.
892
We are going to serve tens of millions of users, right?
893
How can we design a product that people really love to use?
894
And then in the same time, that's the most organic feedback that we could collect for different kinds of users, including memory.
895
So, what if we have memory in the arena?
896
That kind of like applications testing, like really like the long-context capability of the model to reason about the past, and then to have the potentially a rack system to retrieve relevant information from the user's past history in order to create a more personalized content for users or more personalized leaderboards for users that help them to choose what's the best AI for their use cases.
897
When ChatGPT has memory built in, but Claude doesn't, how would that actually work in production when I show up to the site and I'm trying to evaluate these models side by side?
898
What are serving the model be an API?
899
Does that mean on the arena side, you have to recreate memory and then abstract that away as a shared service that all the models consume?
900
How would that implementation work?
901
Yeah, so I think increasingly we are going to go beyond just a single model.
902
That the model has the capability to connect to different sources of information, you will say, like context.
903
The search arena is one example of this.
904
So, search arena, like we launched a couple months ago is basically an arena specific to evaluate models that have internet access, web data access, right?
905
And in that case, model is not just model itself, it has to be in combination of other components.
906
And the same thing happened to memory, right?
907
You have another component which is retrieving relevant information from user history, and then this history is actually richer, like not just prompt, that has all the battle between different models, comparison data, and then users express preference.
908
So, that kind of like more like, and then it could be also multimodal, right?
909
It can be like image, can be like video, all the, or PDF, right?
910
People will upload long documents, that kind of stuff.
911
So, all these kind of like different contexts, different modality of data, how can we leverage them in order to create more personal experience and then evaluate them?
912
That would be like a very interesting challenge.
913
Yeah, I would say there's basically two ways that we're moving forward.
914
The first is the platform is going to continue to evolve, for sure.
915
We're going to keep creating new arenas.
916
We're going to keep improving the arena to integrate things like an artifacts component and things like memory and so on and so forth.
917
And the second is integrations.
918
At the end of the day, if someone wants to evaluate their app, we should be able to provide them a toolkit that integrates with our services to do that.
919
So, if let's say I'm building a code editor, but I'd like to understand which one of the 17 models out there are best for my users.
920
Exactly.
921
What does that look like?
922
I use an arena SDK.
923
Exactly.
924
Got it.
925
That's exactly right.
926
And so, what would that look like?
927
My users would generate a bunch of interactions that then the Arena SDK is serving on the Arena site to run side by side, or is that eval actually happening on my app?
928
I think it can happen in context.
929
So what you can do is you can have some kind of a gateway.
930
that allows people to access all sorts of different models, even maybe the ones that they didn't handpick themselves, but the cutting edge ones that maybe they don't even have access to them.
931
Maybe they're even pre-release, right?
932
And then what we can do is on our side, use all the experience that we've built on sampling, data tools, training models, this huge data set that we've collected that has all these multi-provider comparisons to do things like choose what the best model is for your users, understand how all the different models perform, all the cost-benefit trade-offs, the Predo-Prito curve of cost versus performance of different models.
933
All that stuff is stuff that we can instrument, and we can do it using in-context feedback.
934
Let's say somebody says, hey, let's hook into a thumbs up, thumbs down button, pass that back to the Arena SDK.
935
Well, we can look at that.
936
And using that information, we can produce leaderboards for that organization.
937
We're the experts in doing that, right?
938
We've been doing this for years.
939
Things like prompt a leaderboard and various technologies.
940
D3?
941
D3, yeah.
942
So we're building a project now that we call data-driven debugging D3.
943
It's a little farther out.
944
It's a little come in a couple months.
945
But the fundamental premise of that is that pairwise comparison feedback is not the only kind of feedback that we can use to construct leaderboards.
946
We can construct leaderboards with any form of feedback.
947
And because of that, we can hook in not just to pop up pairwise preference comparisons for whatever company, which is, of course, something that we can do.
948
But instead, what if I want to rank code models in part on how many times the code is copied or accept.
949
Or code changes accepted.
950
How many, like, what's the edit distance between the code that they're saying we're moving from a world where the primary signal that is used to figure out whether to improve an AI model is sort of very explicit, thumbs up, thumbs down, binary preference.
951
You see a future where every interaction I have within a product, engagement, retention, down to a GUI interaction, can help tell the model what things.
952
Absolutely.
953
So that's exactly the kind of stuff that we can loop into our methodology that we've been developing and generate useful feedback for people to continue improving their models.
954
If you want to create code that people are going to use, make sure that people are using it and that the edit distance is low and that people accept your changes.
955
If you want to build an agent like a Devin that's going to be your software engineer, how many of these PRs are getting merged?
956
This is the sort of stuff that we're building the technology that gives you very rich insights into.
957
And I think by virtue of the fact that we're developing this new methodology, we think we have an edge to be able to provide people that kind of service.
958
You talked earlier about prompt to leaderboard.
959
One of the things that surprised me when I looked at the repo, it's an open source repo, is how well the model performed on Arena.
960
Can you actually just walk through what happened when you guys recapped what it is and then what happened when you actually deployed it on Arena.
961
So I'll get a little bit into technical detail here because I think it's cool.
962
So prompt to leaderboard, what does it do?
963
If you look at the chop autoina leaderboard, it's Bradley-Terry coefficients.
964
Prompt to leaderboard is a technology that we built that allows you to take a prompt and then produce Bradley-Terry.
965
So, what's the natural next step from producing a leaderboard?
966
Well, let's make a router.
967
Anj asks me a question.
968
I'm going to produce a leaderboard just for that question.
969
And then, how about I route his question to the model that's on the top of the leaderboard?
970
It turns out that when you do this, when you train a prompt-to-leaderboard model, which is like let's say a 7 billion parameter model, and then you use it to route Anj's questions on the arena and everybody's questions, that model does better than any of the constituent models that were used in the router by a pretty substantial margin.
971
Now, here's another thing that's yet more interesting.
972
Because the Bradley-Terry coefficients have a particular parametric form and a statistical meaning, you can use them in downstream optimization problems.
973
So, one example of an optimization problem is a router.
974
Maximize performance subject to cost constraint.
975
So, the router can be, for example, a randomized router that chooses between different models.
976
It has like a random policy that chooses, hey, Ange asked me a question.
977
With 50% probability, I'm going to route here, with 50% probability, I'm going to route here.
978
And I'm going to do so in such a way that my average cost is one cent.
979
And I'm going to maximize my performance subject to that.
980
Now, if you trace the performance, the best performance that any individual model can give you as part of the router as a function of cost, that's like 2x worse than the router.
981
In other words, the router is giving you double the bang for your buck in terms of performance per cost.
982
If you want to achieve an arena score of 1280 using the router, it'll cost you half as much as it costs you to use.
983
What it means is that you're taking advantage of the heterogeneity and performance of these models across different parts of prompt space in order to properly route them.
984
And by virtue of the fact that it has the statistical interpretation, you can cost constrain it too.
985
And that's why Prompt to Leaderboard is so interesting, is that because we believe it's like a fundamental first step towards addressing this routing problem in a principled way.
986
And from our perspective, it's like the right way to do routing.
987
If you want to do routing to maximize preference, like even internally at OpenAI, they're doing these A-B tests, right?
988
If you want to maximize the sort of feedback that you get there and the engagement, then you should be using a strategy like Prompto Leaderboard.
989
And so our hope is that this sort of thing would make it easier for them to avoid the dropdown and that they can actually implement it in their own product.
990
I'm sure that they have strategies of their own, but maybe this can be helpful to them too.
991
Let's talk a little bit about, you said that experience will look different over time.
992
The arena experience will look different than ChatGPT.
993
Let's talk a little bit about the roadmap.
994
What are the biggest things that you guys are working on over the next few months?
995
And then let's go longer term.
996
Yeah, well, two that we've already mentioned are personalization and a leaderboard of users.
997
Can we get people, first of all, to figure out which models they like best and sort of lean into that experience, incentivize them to give us better votes, come here for their personal leaderboards and their personal metrics, and then give them a lot of them to drill down really deep in that.
998
And in that case, we align the interests of individuals and the platform as a whole, because you don't want to mess up your personal.
999
Just like how people these days, when they use social media, they don't like a random post because if they do that, then their feed will be messed up.
1000
So it's like, oh, I will be more careful voting.
1001
I'll be more careful looking at all these different models, denser, and so on, which we believe collectively will create a better, even more higher-quality arena.
1002
Absolutely.
1003
Yeah.
1004
And then on the note of a user leaderboard, can we value the data in such a way that allows people to know where they stand in terms of what kind of questions they're asking, how useful they are?
1005
We think people are going to love that.
1006
It's such a fun thing to be able to see that in terms of math, I'm asking the best questions.
1007
I would love that.
1008
I would love if I was asking the best statistics questions in the world.
1009
And I think people will use that and think, hey, I want to be on the top.
1010
And so, can we continue to align the incentives?
1011
And by the way, once we do that, it'll make it much more valuable, but we'll make the leaderboard much more valuable because it'll mean that we sort of start removing the noise from people that might be sort of, oh, I don't know what these buttons are, click.
1012
And instead, people are getting really intentional, really high-taste votes, identifying who those people are, and maybe even being able to personalize so carefully that we can produce leaderboards for different types of people.
1013
That would be incredible.
1014
And then on the flip side of it, it's like we as a platform have a bit more visibility into who are those users and how do we even customize the distribution that on the flip side is model developer care or developers or developer at large cares.
1015
I wanted to say, oh, I want to test my AI.
1016
And then, can we have the ability to customize that kind of distribution to target what are the most meaningful distributions that reflect your use cases?
1017
One of the things that you guys have been pretty vocal about is open source.
1018
I think from day one, LM Arena has open sourced prompts, votes, chunk of the data that's being generated on the platform.
1019
I think we do every week probably updates on the leaderboard, and then all the code infrastructures that we process the data is published as open source, and also research, blog, paper.
1020
And then, including prompt leaderboard, we publish the paper and open source the models, the code, and everything.
1021
Because we believe that this is critical in terms of building trust with the community and also really build the foundation of this that we can like enable more and more value on top of it.
1022
So, for adoption reasons, for trust, and then for collaborations.
1023
As you guys have made the transition from being a research project to not being a company, what are the most important values do you guys create and hold at the company as you guys grow out the team, as the project grows?
1024
Absolutely.
1025
Well, we are very focused on neutrality, innovation, trust.
1026
We come from an academic background and yeah, we want to maintain the culture of this is a project, it's a community-focused project, it's going to continue to grow.
1027
Yes, it's going to be a company, the company is going to support the project that we've already built and allow it to grow.
1028
Yes, it's going to continue to change, it's going to change for the better.
1029
We're going to keep improving it, we're going to keep publishing papers, we're going to keep releasing open source, we're going to keep releasing open data.
1030
That's all going to be part of our culture.
1031
And part it goes both ways because that's the way that you recruit.
1032
The best people don't want to hold up at a company and develop a bunch of proprietary technology that is never going to be released and it's just going to sort of stay in the annals of their nearest neighbors within the company, and they're the only ones that are going to know.
1033
We want the world to know what the best ways are of evaluating these models and accelerating the ecosystem.
1034
And releasing this data is also a big part of our trust.
1035
If people want to ask the question, hey, how are models performing on the why are they performing well?
1036
Go look at the data.
1037
That's what we did with Llama.
1038
Right?
1039
When people have questions about Llama, we just release the data.
1040
Easy, right?
1041
Just go look.
1042
And we plan on doing things like this for the lifetime of our company, right?
1043
That's how we're going to recruit the best researchers that are going to help us develop the methodology.
1044
That's how we're going to develop the best engineers who care about the whole ecosystem, not just one company.
1045
And ultimately, that's how we're going to develop the best products.
1046
That's how we're going to become central to the space.
1047
We already are, but we're going to cement it, is by remaining open and neutral.
1048
And how would you resolve the tension that often exists when there are people who are concerned that as AI gets more and more prevalent, right, as AI systems, that in fact there's an argument to be made that these systems should be closed source and evaluated in a fairly locked-down environment as opposed to being openly tested in this manner and this is actually irresponsible.
1049
How do you think about that cultural tension?
1050
Listen, I'm not an expert in national security, but I think an evaluation platform like ours has many different ways of being used.
1051
If they want to evaluate it publicly, they can.
1052
If they do a private deployment, we can probably also do that for them.
1053
It just depends on the sort of level of national security risk, which is way above my pay grade.
1054
But for any of these things, you're going to need sort of these subjective community-driven evaluations, that's for sure.
1055
If things are going to be deployed in the real world, you're going to need real people testing them.
1056
Yeah, and also, like, there's a point when you develop the model and this model is going to be used by broadly the public.
1057
There has to be like a phase of testing it, right?
1058
And then we're trying to, what we are building is like to bridge this gap between the lab building something that's like the latest frontier research and the world would use it as a large.
1059
You need an environment for you to test.
1060
In the sense that it's a more controlled environment with the people that the distribution you want to customize, you want to understand the preference.
1061
There's a need for a platform like this to exist.
1062
And we want to serve that.
1063
Yeah, could you talk a little bit about Red Team Arena?
1064
Yeah, so for example, this real-world testing idea of Arena can be applied to different modality, image, that kind of stuff, and as well as red teaming.
1065
Because red teaming at its core is like a bunch of people try to drill break the models to see if it's really faithfully following what the model has been like instructed to do or graded to do, right?
1066
So these days, many frontier labs in publishing kind of like models back, that kind of idea, like how models should behave in this way, in that weight, right?
1067
And then, but how do you make sure model follows that instructions?
1068
You need real-world testing again.
1069
You need red teaming, you need a group of people knowledgeable in this space to help, right?
1070
So again, this can be community-driven too, because there's a group of vibrant community of job breakers that want to help.
1071
And then they wanted to also like, they want, they tested for fun as well.
1072
Like, so in Red Team Arena, we have a leaderboard, not just for model, but for user, for job breaker, who is the best job breaker that can like identify issues for all different models.
1073
So that very particular, the very idea of real-world testing still applies here and it still can deliver value to the ecosystem that we believe.
1074
So is it fair to say that if I wanted to understand the security or the safety sort of risks in a model, I could go to Red Team Arena and look at the evaluation that the models are generating over there.
1075
How does Red Team Arena actually work in practice to improve the security and reliability?
1076
Yeah, for sure.
1077
So, I think the same as how we understand chatbot, web that kind of thing, there will be many different applications people are trying to build on top of the models.
1078
That's a customer services or like retriever systems, that kind of stuff, right?
1079
You want models to behave in certain way and you want control.
1080
And then in Red Team Arena, the idea will be like, why don't we build an environment to simulate that applications?
1081
So, for example, can we build an environment to simulate customer services where the model is instructed to not give certain, not take certain actions?
1082
And then you are as a job breaker trying to break the model.
1083
So, that kind of like signals that we will be getting in terms of like real-world testing, job breaking, will be reflected to the particular use cases that people can.
1084
By the way, Red Team Arena right now is still a little bit of a prototype.
1085
We're continuing to work on it.
1086
But it's interesting to see.
1087
People can, it's not necessarily the model that's like most like refuses the most to answer these like queries that people ask necessarily better.
1088
Some people want a model that's more controllable, some people want a model that's going to say whatever they want.
1089
Some people want a model that's going to be completely safe and you can use it in PG-13 or rated G.
1090
That's okay, as long as people have the choice.
1091
So, as we start to wrap up here, one question that a lot of people ask is: what does the world look like, especially the world of evaluation and testing, as we go from a pre-training world to a post-training world and a world of the curve, where Arena has always been an environment for agents more than a set of static EAPs.
1092
So, as people start, as agents get better at long horizon tasks and dual calling and so on, this future where a ton of work in the economy is done largely by fully end-to-end automated systems.
1093
Does Arena have to change in any fundamental way for that future?
1094
Or does this largely look the same?
1095
Yeah, I think I'm talking about what's the fundamental is organic real-world testing with feedback.
1096
That's not going to change.
1097
I can tell you that is not going to change.
1098
Will we have to adapt the UI?
1099
Yes.
1100
Will we have to improve the product?
1101
Yes.
1102
Will we have to launch new products for evaluation?
1103
Yes.
1104
Will we have to develop new methodology?
1105
Yes.
1106
Does the fundamentals change?
1107
I think no.
1108
I think the reality is: if you want to test your model for real-world use, you have to subject it to real-world use, to collect feedback from real-world use, and that's it.
1109
So, we're really excited about what the future has to hold there.
1110
We don't actually even know ourselves where the product is going to evolve over the next five to 10 years.
1111
Why?
1112
The ecosystem is moving so quickly.
1113
But wherever it goes, we're excited to follow.
1114
Awesome.
1115
Thanks, guys.
1116
Thank you.
1117
Thank you.
1118
If you made it this far, thanks so much for listening until the very end.
1119
And keep listening in the weeks to come as we have some great discussions lined up.
1120
Finally, if you enjoyed this discussion or anything else you've heard on this podcast, please do share it far and wide and rate the show on Apple Podcasts.