--- METADATA START ---
Show: The AI Daily Brief (Formerly The AI Breakdown): Artificial Intelligence News and Analysis
Episode: The Push to Productize AI
Host: Unknown 
Guests: None
Source URL: https://podcasts.apple.com/us/podcast/the-push-to-productize-ai/id1680633614?i=1000709365825
--- METADATA END ---

1
Today on the AI Daily Brief, Google's Quest to Make AI Real.
2
The AI Daily Brief is a daily podcast and video about the most important news and discussions in AI.
3
Thanks to today's sponsors, KPMG, Blitzy.com, and SuperIntelligent.
4
And to get an ad-free version of the show, go to patreon.com/slash AI Daily Brief.
5
Quick note, friends, as I kind of imagined would happen with some of these events, we are once again doing a main episode only today.
6
There is a lot to discuss from Google I.O., so much that it's going to crowd out the headline section.
7
So, what was the theme from Google I.O.?
8
Earlier this week, we talked about the different vectors of AI competition.
9
All the Frontier labs are competing around not only the benchmarks which make headlines, but also for consumer affiliation, for enterprise adoption, for specific use cases around coding, and of course to build the infrastructure and applications that bring to life agents.
10
Microsoft Build was the first event to kick off this week, and our big takeaway from that was just the radical shift in how comprehensive, deep, technical, and embedded agents were in every part of the Microsoft stack.
11
For Google, Google summed up their I.O.
12
event with the phrase, from research to reality.
13
And it's pretty clear watching this that their attempt is to productize all of their AI technology as fast as possible.
14
In his keynote address, CEO Sundar Pichai said: We're now entering a new phase of the AI platform shift where decades of research are becoming reality.
15
Normally, you wouldn't have heard much from us in the weeks leading up to I.O., because we'd be saving up our best models for the stage.
16
But in our Gemini era, we're just as likely to ship our most intelligent model on a random Tuesday in March or announce a really cool breakthrough like Alpha Evolve just a week before.
17
We want to get our best models into your hands, ASAP, so we're shipping faster than ever.
18
And as a sub point, while this is a far less important story than what was actually going on, it is notable that the AI labs all seem to be kind of giving up the idea of holding on to your best announcements for your big conferences.
19
The rate of competition is so punishing, you just have to push things out when they become available.
20
And so, in many ways, instead, what these events are is a way to gauge and see all of the organization's strategy all come together.
21
And indeed, there certainly wasn't a lack of announcements for I.O.
22
There were, in fact, dozens of announcements, large and small, addressing every aspect of Google's business.
23
There were, in fact, way more features and products revealed than I can cover.
24
So, what I'm going to do instead is try to share what this event says about Google's AI strategy and how they seem to be stepping up the competition.
25
The big picture is that Google is reinventing their entire product experience end-to-end to be AI first.
26
Across a multitude of vectors, AI is becoming the very core of what Google offers.
27
More than anything, the conference was about going broad with a huge volume of new ways to use AI.
28
Pichai noted that the volume of tokens being produced across Google's platforms is up 50x from this time last year.
29
And the sharpest increase has actually happened over the past few months.
30
AI is very clearly hitting an inflection point, and Google wants to have every possible option available to users.
31
As Pichai put it, the company's entire goal is to make more intelligence available for everyone, everywhere.
32
One notable example of Google's productizing AI is their new ultra subscription.
33
Until now, Google hasn't really been monetizing AI in a meaningful way.
34
Yes, they've had a $20 pro tier and some enterprise add-ons for workspace, but no top-end coherent AI product that they're pitching primarily to users.
35
And that may be where this new ultra-subscription comes in.
36
It's a $250.
37
Like rival premium subscriptions, the idea is to give users extremely high usage limits and early access to new features.
38
It also comes bundled with increased storage for Google Drive and YouTube Premium as a clear upsell for existing users.
39
One really interesting point here is that with Google bundling together their most advanced AI features as a package, it will allow Google to be the first public company to report a direct AI revenue line item in their financials.
40
Until now, public tech companies have sort of buried AI revenue within cloud or advertising divisions.
41
The actual package itself is replete with new features.
42
On the model side, users will have access to a deepth mode for Gemini 2.5 Pro.
43
That's an enhanced reasoning mode that allows the model to consider and evaluate multiple answers before responding.
44
DeepMind CEO Demis Isabe says, Deepthink pushes model performance to its limits.
45
It uses our latest cutting-edge research and thinking and reasoning, including parallel techniques.
46
Now, as you can probably tell, of these vectors of competition, the one I care least about is the benchmarks, but this new Deep Think mode does seem to push them significantly, allowing Gemini 2.5 Pro to purportedly leapfrog OpenAI's O3 on the multimodal reasoning MMMU test.
47
It also boosts coding performance on the live code bench, opening a wider gap with O3.
48
The Ultra subscription also comes with exclusive access to the new VO3 video model, as well as early access to a web agent, both of which we'll cover in a little more detail later in the show.
49
One of the big value propositions is increased limits to make full use of Project Astra, which is Google's real-time AI interface.
50
Astra allows the model to accept streaming video and audio through a smartphone and respond to users' questions with very low latency.
51
The technology is part of a bet on what the future of AI interfaces will look like, although for now, Google is more positioning it as an interesting smartphone feature for early adopters.
52
Now, if you've just heard one thing about Google and AI recently, it might be around the way that they're letting AI change search.
53
Just two weeks ago, testimony from an Apple executive that the company was seeing a decline in searches as a result of AI caused Google's stock to tumble.
54
This angle was pretty much the only one the financial press cared about, and Google seemed to have hit it out of the park.
55
The announcement was that AI mode would be rolled out to all US search users this week.
56
Users can switch to an AI mode tab and conduct searches using natural language prompting, including follow-up questions.
57
The output can include generated charts and website references.
58
There's also a deep search feature to provide outputs closer to a deep research report.
59
Now, the features aren't novel, but the rollout does say something about how Google is positioning itself.
60
Honestly, Wall Street Jitters gave the company license to put AI search front and center without fear of backlash.
61
Even The Verge, who are pretty critical of big tech these days, reported the news under the bold headline that, quote, AI mode is obviously the future of Google search.
62
And it was clear from the prominence of this announcement that Sundarbury Chai wanted to press home that Google is not going to lose their search revenue to AI.
63
Indeed, in his keynote, Pichai shared that internal testing showed AI overviews caused a 10% increase in search queries per user.
64
During an accompanying interview with All In, Pichai explained that search revenue was completely unchanged with the introduction of AI overviews.
65
He commented that, quote, Empirically, we see that people are engaging more and using the product more.
66
Effectively, his argument is that AI search doesn't change much about the business model of monetizing e-commerce advertising.
67
It just improves the user experience.
68
Now, that will be a debate that will be very fiercely fought, especially based on some of the reactions I got in press releases in my inbox.
69
But that's the narrative from Google's point of view.
70
Another hot topic, of course, at the moment is the agentic web.
71
And along those lines, Google has introduced their own web agent.
72
Called Project Mariner, the agent allows users to opt.
73
It operates in the background of a cloud-based browser, so users can keep going with other tasks while it works.
74
The feature set is pretty similar to OpenAI's operator as well as other early web agents.
75
Mariner can handle tasks like online shopping, web-based research, and form filling.
76
Frankly, it seems a lot more interesting as a building block towards an AI-native interface than it does as a standalone product.
77
Google is beta testing Mariner as an add-on for its AI mode in search.
78
It also already seems to be powering a shopping-focused feature being introduced to AI mode within the standard Google search experience.
79
The feature promises price tracking, virtual try-ons, and agentic checkout.
80
Finally, there's a dedicated agent mode that combines web browsing with deep research features and app integrations as well.
81
Early access to Project Mariner will be exclusive to the Ultra subscription going back to that, so that could be a big selling point.
82
Now, in terms of what this all says about how Google feels about agents, it's actually in some ways a little bit muted.
83
It doesn't feel to me like Google has exactly come to a conclusion around how agentic web assistants should work, and so they're rolling out several narrow and generalized options to test product market fit.
84
The other big AI UX announcement involved the return of, believe it or not, Google's smart glasses.
85
An entire decade ago, Google was first to market with Google Glass.
86
A product so bad or so early, depending on your take, it salted the earth and deterred anyone from pursuing the concept for years.
87
Co-founder Sergei Brin was present at I.O.
88
and actually discussed the product disaster.
89
He commented that he, quote, didn't know anything about consumer electronic supply chains or how difficult it would be to deliver at a reasonable price point.
90
To that end, the new Android XR smart glasses will be produced in collaboration with consumer eyewear company Warby Parker.
91
In both style and strategy, then, this is definitely Google lifting a page from Meta with their successful Ray-Band.
92
But honestly, for people who are excited about this technology and don't want them to look like, well, dog-doo, my strong feeling is to copy away and find the right partners to partner with.
93
In any case, Google is investing up to $150 million into Warby and is taking an equity stake contingent on hitting milestones.
94
There are some reasons to like where Meta is relative to Google in this competition.
95
For example, they have a significant multi-year head start on things like battery and electronics miniaturization.
96
But at the same time, more than any sort of competitive dynamics, I think that this move just suggests that this is actually going to be a ubiquitous category.
97
I don't think when it comes to glasses, we're going to have a winner-take-all.
98
I think we're going to have a radically commoditized experience where everyone has some version of this attached to any glasses they wear.
99
Today's episode is brought to you by KPMG.
100
In today's fiercely competitive market, unlocking AI's potential could help give you a competitive edge, foster growth, and drive new value.
101
But here's the key: you don't need an AI strategy.
102
You need to embed AI into your overall business strategy to truly power it up.
103
KPMG can show you how to integrate AI and AI agents into your business strategy in a way that truly works and is built on trusted AI principles and platforms.
104
Check out Real Stories from KPMG to hear how AI is driving success with its clients at www.kpmg.us/slash AI.
105
Again, that's www.kpmg.us/slash AI.
106
Today's episode is brought to you by Blitzy, the Enterprise Autonomous Software Development Platform with Infinite Code Context.
107
Which, if you don't know exactly what that means yet, do not worry, we're going to explain, and it's awesome.
108
So, Blitzy is used alongside your favorite coding co-pilot as your batch software development platform for the enterprise, and it's meant for those who are seeking dramatic development acceleration on large scales.
109
First documenting your entire code base, then deploying more than 3,000 coordinated AI agents working in parallel to batch-build millions of lines of high-quality code for large-scale software projects.
110
So, then, whether it's code-based refactors, modernizations, or bulk development of your product roadmap, the whole idea of Blitzy is to provide enterprises dramatic velocity improvement.
111
To put it in simpler terms, for every line of code eventually provided to the human engineering team, Blitzy will have written it hundreds of times, validating the output with different agents to get the highest quality code to the enterprise in batch.
112
Projects then that would normally require dozens of developers working for months can now be completed with a fraction of the team in weeks, empowering organizations to dramatically shorten development cycles and bring products to market faster than ever.
113
If your enterprise is looking to accelerate software development, whether it's large-scale modernization, refactoring, or just increasing the rate of your STLC, contact Blitzy at blitzy.com, that's B-L-I-T-Z-Y dot com, to book a custom demo, or just press get started and start using the product right away.
114
Today's episode is brought to you by Super Intelligent and more specifically, Super's agent readiness audits.
115
If you've been listening for a while, you have probably heard me talk about this, but basically, the idea of the Agent Readiness Audit is that this is a system that we've created to help you benchmark and map opportunities in your organizations where agents could specifically help you solve your problems, create new opportunities in a way that, again, is completely customized to you.
116
When you do one of these audits, what you're going to do is a voice-based agent interview where we work with some number of your leadership and employees to map what's going on inside the organization and to figure out where you are in your agent journey.
117
That's going to produce an agent readiness score that comes with a deep set of explanations, strengths, weaknesses, key findings, and of course, a set of very specific recommendations that then we have the ability to help you go find the right partners to actually fulfill.
118
So, if you are looking for a way to jump-start your agent strategy, send us an email at agent at bsuper.ai and let's get you plugged into the agentic era.
119
What about then this AI coding theme, obviously, a big one for many of us?
120
Well, there are a couple things around here.
121
First up, we have Jules, Google's AI coding agent.
122
Google says that Jules will be able to fix bugs, test code, and consult documentation while running in the background.
123
Josh Woodward, the VP of Google Labs, said that the design brief asked: What if you created a way where you could assign tasks to this agent for the things you didn't want to do?
124
Social media is awash with developers comparing Jules to OpenAI's Codex, which was announced on Friday as a way to front-run these announcements.
125
And honestly, it matters way less, I think, which company is out in front, and more than in the span of less than a week, autonomous AI coding agents that complete routine tasks in the background have become table stakes.
126
We also saw Microsoft introduce this feature with Copilot Agent earlier this week, and it's clear that this is just something that every major lab is going to have to do now.
127
Maybe more interesting, though, are a couple of AI coding tools that are a little bit more focused.
128
The first is called Stitch, and it's designed to quickly turn rough UI ideas into fully functional front ends ready for deployment.
129
It can take text and image inputs as references thanks to Google's multimodal capabilities.
130
And so, if you imagine somewhere between AI-enhanced Figma and a Vibe coding app, you've got the right idea.
131
The other product is a feature inside of Android Studio called Journeys.
132
It allows Android developers to automate product testing with the help of Google's agents.
133
Users can describe a particular user interface flow or journey using natural language to test for bugs and crashes.
134
Now, what's interesting and a little bit different about this is that while mostly we've seen AI coding tools being gigantic sandboxes that can plausibly do anything, these tools are aimed instead at very specific tasks.
135
I will be super interested to see whether this type of narrow tooling is actually a better thin edge of the wedge when it comes to adoption.
136
And even after all this, there are still several other features that are worth mentioning as part of this broader idea of productizing AI.
137
Notebook LM, which was such a pop at the end of last year, is getting an exciting new feature in video overviews.
138
The output is a slide deck to accompany audio commentary with the model capable of generating infographics and drawing on image content from its input data.
139
The podcast generated by audio overviews have already become a popular tool for internal corporate communications, and adding a slide deck is a very natural next step.
140
Another product in this category is Flow, which is Google's new AI video making tool.
141
Flow is geared towards filmmakers, allowing them to generate video clips based on imported reference material.
142
Users can execute camera control and use a scene builder to direct the action.
143
The product is a combination of the VO video model, ImageN for image generation, and Gemini for text and prompting.
144
And one of the trends here is users no longer needing to understand how to connect various models together to achieve an outcome.
145
The AI itself will be able to make those connections for us.
146
Now, while we're on the subject of video, it is worth mentioning quickly VO3.
147
The model is now capable of generating sound to accompany its video with pretty impressive results.
148
Devin Sabis posted a teaser video of onion sizzling in a pan, and an even bigger reveal was that VO3 can now generate dialogue with believable accents and even serviceable acting.
149
We can talk its cartoons.
150
This is amazing.
151
Imagine all the narrative possibilities.
152
Now, the breakthrough could be an entire episode in and of itself, and honestly, I have this very much flagged to come back to, but it might be summed up by Beth Jezos, who posted, this will actually eat Hollywood.
153
So, that was a bit of a summary of what's been going on at I.O., and yet, even with all that, there were still a bunch of things that I didn't get to cover.
154
We didn't touch, for example, on the live translation technology coming to web conferencing, Google's proprietary AI chips, or like a dozen other new products.
155
The big takeaway from most seems to be that Google is competing on every angle and seems determined to win them all.
156
AI search engineer Archie Singupta commented: Google just killed OpenAI, Sora, Suno AI, Perplexity, Metaglass's Claw, ChatGPT in two hours.
157
Now, obviously, the assertion that those things have been killed is hyperbolic for the sake of the X algorithm.
158
But what's notable is that just a year ago, there were active discussions as Gemini was recommending putting glue on pizza that Sergei might have to return to take over for Sundar, but instead, a year on, Sundar has set a course to try to dominate the AI era.
159
The most complimentary thing I can say about all of this is that while this barrage of product announcements isn't new, this did feel a bit more like a whole greater than the sum of its parts than in previous versions of this we've seen.
160
Google has definitely passed the stage of making AI models where they just leave it up to users to figure it out.
161
They're clearly creating a wide range of AI products that are supposed to excel at specific functions.
162
We got a lot of glimpses into the future, but also a lot that's for right here and right now.
163
Back at the end of 2024, Sundar told his company: I think 2025 will be critical.
164
I think it's really important we internalize the urgency of this moment and need to move faster as a company.
165
The stakes are high.
166
These are disruptive moments.
167
In 2025, we need to be relentlessly focused on unlocking the benefits of this technology.
168
And so far, at least, people seem to be responding.
169
For now, though, that is going to do it for today's AI Daily Brief.
170
Appreciate you listening, as always.
171
And until next time, peace.