--- METADATA START ---
Show: The AI Daily Brief (Formerly The AI Breakdown): Artificial Intelligence News and Analysis
Episode: What to Use Claude 4 For
Host: Unknown
GUESTS: None 
Guests: None
Source URL: https://podcasts.apple.com/us/podcast/what-to-use-claude-4-for/id1680633614?i=1000710203930
--- METADATA END ---

1
Today, on the AI Daily Brief, Anthropic announces Claude 4, and before that in the headlines, why OpenAI is not about to release another humane AI pin.
2
The AI Daily Brief is a daily podcast and video about the most important news and discussions in AI.
3
Thanks to today's sponsors, KPMG, Blitzy.com, and Superintelligent.
4
And to get an ad-free version of the show, go to patreon.com/slash AI Daily Brief.
5
Welcome back to the AI Daily Brief Headlines Edition: all the daily AI news you need in around five minutes.
6
Last week, one of the big stories was that legendary Apple designer Johnny Ive was joining Sam Altman at OpenAI with an eye to creating a next-generation device for the AI era.
7
Since then, there has been basically non-stop speculation around what the device would actually be.
8
Much of the speculation revolved around the idea of a pendant that would iterate on previous AI devices.
9
Some even thought that Johnny's thick-rimmed glasses for the video were an Easter egg, featuring the device hiding in plain sight.
10
Well, OpenAI staff were given a sneak peek of the design device at a Wednesday meeting.
11
After reviewing a recording of the meeting, the Wall Street Journal wrote, Altman and Ive offered a few hints at the secret project they've been working on.
12
The product will be capable of being fully aware of a user's surroundings and will be unobtrusive, able to rest in one's pocket or on one's desk, and will be a third core device a person would put on their desk after a MacBook Pro and an iPhone.
13
Altman reinforced that this is one of the company's largest bets, telling employees that they have, quote, the chance to do the biggest thing we've ever done as a company here.
14
Altman wants to ship 100 million of the AI companions, his word, and also suggested that the $6.5 billion acquisition of the design studio had the potential to add a trillion dollars in value to OpenAI.
15
As for the form factor, Altman said the device won't be a pair of glasses, adding that Ive had been skeptical about building something to be worn on the body.
16
The lack of wearability would sidestep one of the early criticisms of the device.
17
Many pointed out they're not quite ready for a world where every single person is wearing an AI device at all times.
18
Still, Altman is banking on this device being the next big thing.
19
He said, we're not going to ship 100 million devices literally on day one, but he expressed 100 million of something new before.
20
Albin told staff that secrecy is going to be key to ensure the device can get to market before rivals can copy it.
21
And the recording being leaked to the Wall Street Journal raises some pretty big questions about trust at the company and how much Alban will be willing to share to future all-hands.
22
For now, the big takeaway is that it does not look as though we're going to get Humane AI PIN 2.0.
23
Speaking of OpenAI, the company has upgraded their operator agent to use O3.
24
Until now, the web browsing agent has been driven by GPT-4.0, but user preference testing showed that Operator O3 had better style, comprehensiveness, and clarity.
25
Users also preferred the upgrade for instruction following, which of course is extremely important when you're letting an agent take over for web-based tasks.
26
Operator 03 also has increased safety.
27
OpenAI claims it's less likely to perform illicit activities, search for personal data, or suffer from a prompt injection attack while browsing the web.
28
OpenAI writes, O3 Operator uses the same multi-layered approach to safety that we use for the 4.0 version of Operator.
29
Compared with other models in the O3 family, O3 Operator was fine-tuned with additional safety data for computer use, including safety data sets designed to teach the models our decision boundaries on confirmations and refusals.
30
Next up, another example of what appears to be the latest trend, which is CEOs using AI avatars on a quarterly earnings call.
31
Last week, we saw Klarna's CEO deliver quarterly earnings via AI Avatar.
32
And this week, Zoom CEO Eric Yuan followed suit, using an avatar for his initial comments.
33
The avatar said, I'm proud to be among the first CEOs to use an avatar in an earnings call.
34
It's just one example of how Zoom is pushing the boundaries of communication and collaboration.
35
At the same time, we know trust and security are essential.
36
We take AI-generated content seriously and have built-in strong safeguards to prevent misuse.
37
Now, the Klarna example was clearly just a way for the company to continue to position themselves as an AI-first company.
38
But for Zoom, this was a very public product demo.
39
The company has been working on digital twins for some time, allowing users to send their avatars to meetings.
40
The tech isn't quite ready for real-time use cases, but Zoom is now rolling out avatars for recorded messages to all users.
41
When the real Yuan showed up for the QA portion of the call, he commented, I truly love my AI-generated avatar.
42
I think we're going to continue using that.
43
I can tell you, I like the experience a lot.
44
Lastly, today, Google's antitrust woes continue with a new investigation into their AI acquisition strategy.
45
Bloomberg reports that the Justice Department has launched a probe into Google's deal with Character AI.
46
Last August, Google paid $2.7 billion for a non-exclusive license to use Character AI's technology.
47
And at the same time, it was announced that founder Noam Shazir and several team members would join Google to work on the Gemini team.
48
Shazir had a two-decade career at Google before leaving in frustration in 2021 after the company refused to release his chatbot project.
49
He was one of the lead authors of the Google paper entitled, Attention is All You Need, which introduced the transformer architecture that underpins modern AI.
50
The deal was widely reported as an aqua hire, but didn't technically require FTC approval in the same way as an acquisition.
51
A Google spokesperson said the company was, quote, always happy to answer any questions from regulators.
52
However, he pointedly added, We're excited that talent from Character AI has joined the company, but we have no ownership stake and they remain a separate company.
53
The DOJ's position is that they're able to investigate whether the deal is anti-competitive, even if it didn't require a formal review.
54
The reporting emphasized that Google hasn't been accused of any wrongdoing and the investigation is still in the early stages.
55
But I think if you're watching the trend lines, this suggests that the new administration is still actively scrutinizing.
56
For now, though, that is going to do it for today's AI Daily Brief Headlines Edition.
57
Next up, the main episode.
58
Today's episode is brought to you by KPMG.
59
In today's fiercely competitive market, unlocking AI's potential could help give you a competitive edge, foster growth, and drive new value.
60
But here's the key: you don't need an AI strategy.
61
You need to embed AI into your overall business strategy to truly power it up.
62
KPMG can show you how to integrate AI and AI agents into your business strategy in a way that truly works and is built on trusted AI principles and platforms.
63
Check out Real Stories from KPMG to hear how AI is driving success with its clients at www.kpmg.us/slash AI.
64
Again, that's www.kpmg.us slash AI.
65
Today's episode is brought to you by Blitzy, the Enterprise Autonomous Software Development Platform with Infinite Code Context, which, if you don't know exactly what that means yet, do not worry, we're going to explain, and it's awesome.
66
So, Blitzy is used alongside your favorite coding co-pilot as your batch software development platform for the enterprise, and it's meant for those who are seeking dramatic development acceleration on large-scale code bases.
67
Traditional co-pilots help developers with line-by-line completions and snippets, but Blitzy works ahead of the IDE, first documenting your entire code base, then deploying more than 3,000 coordinated AI agents working in parallel to batch-build millions of lines of high-quality code for large-scale software projects.
68
So, then, whether it's code-based refactors, modernizations, or bulk development of your product roadmap, the whole idea of Blitzy is to provide enterprises dramatic velocity improvement.
69
To put it in simpler terms, for every line of code eventually provided to the human engineering team, Blitzy will have written it hundreds of times.
70
Projects then that would normally require dozens of developers working for months can now be completed with a fraction of the team in weeks, empowering organizations to dramatically shorten development cycles and bring products to market faster than ever.
71
If your enterprise is looking to accelerate software development, whether it's large-scale modernization, refactoring, or just increasing the rate of your STLC, contact Blitzy at blitzy.com, that's B-L-I-T-Z-Y dot com, to book a custom demo or just press get started and start using the product right away.
72
Today's episode is brought to you by Super Intelligent and more specifically, Super's agent readiness audits.
73
If you've been listening for a while, you have probably heard me talk about this.
74
But basically, the idea of the Agent Readiness Audit is that this is a system that we've created to help you benchmark and map opportunities in your organizations where agents could specifically help you solve your problems, create new opportunities in a way that, again, is completely customized to you.
75
When you do one of these audits, what you're going to do is a voice-based agent interview where we work with some number of your leadership and employees to map what's going on inside the organization and to figure out where you are in your agent journey.
76
That's going to produce an agent readiness score that comes with a deep set of explanations, strengths, weaknesses, key findings, and of course, a set of very specific recommendations that then we have the ability to help you go find the right partners to actually fulfill.
77
So, if you are looking for a way to jumpstart your agent strategy, send us an email at agent at bsuper.ai and let's get you plugged into the agentic era.
78
Welcome back to the AI Daily Brief.
79
Last week, as you guys will remember, was a big week for big lab events.
80
We had Microsoft kick us off, and then Google came in the middle of the week, and then to close out the week, we had Anthropic's first developer conference on Thursday.
81
Now, alongside that, Anthropic announced their project with Rick Rubin, which was the subject of Friday's show.
82
And then we had the long weekend.
83
Hopefully, by the way, if you are in the US, you had a great Memorial Day.
84
But now we're catching up with the big announcement from Anthropic's event, which is the release of their latest flagship models.
85
And what we're going to talk about today with the release of Claude Opus 4 and Claude Sonnet 4 is not only how they stack up relative to the other available models, although that'll be a piece of it, but also some interesting emergent behavior that dramatizes the challenge of alignment as these models get more powerful.
86
Now, one thing we should talk about from a ground-level expectation-setting point of view is that we are definitely in an era now with AI where the model releases come a lot more frequently, but with much more incremental improvements over the previous.
87
Part of that is the nature of the gains right now, but also part of it is just the competitive pressure.
88
Labs really can't afford to wait for huge improvements because almost as soon as they release something, one of their competitors has released something that is incrementally more powerful, and so they have to respond.
89
And what ends up happening is exactly the scenario we have now, where every other week or so, we get a slightly improved model that we have to calibrate and integrate into our workflows, waiting for the next to come along.
90
So, this release from Anthropic focused on two big improvements over previous generations: long reasoning and coding.
91
The models use the same hybrid reasoning architecture as Claude 3.7, allowing the reasoning to be modulated according to the complexity of the task.
92
At the limits, Claude 4 is demonstrating really impressive reasoning coherency on long tasks.
93
Anthropic tested Claude 4 Opus on a complex open source refactoring project.
94
VentureBeat writes that this breakthrough, quote, transforms AI from a quick response tool into a genuine collaborator capable of tackling day-long projects.
95
It reminds me of the charts we've seen recently of how agent performance is doubling roughly every three to four months in terms of how long a task it can handle with coherence.
96
Coding benchmarks are an expected step up.
97
This is of course the area where Anthropic has really firmly cemented itself as the leader in the field.
98
Sonnet 4, which is designed as a drop-in replacement for Sonnet 3.7, delivers a notable improvement on its predecessor on the SuiteBench verified test.
99
Opus 4 is actually slightly worse than Sonnet 4 on the simple SuiteBench problems, so it's intended to be used for tasks that require longer periods of focused work.
100
And this is another important point to note.
101
We're also at a point now where you can't just use the model with the largest number attached to its name for all tasks.
102
One of the most important skill sets, or rather knowledge bases of the moment, is understanding which model to use in what scenario.
103
Still, in each case, Anthropic is claiming that both of these models outperform OpenAI's O3 and Codex, as well as Gemini 2.5 Pro on coding.
104
There are a range of other small features that improve the model for difficult work tasks as well.
105
Claude4 Opus is now capable of creating and maintaining memory files for completing longer tasks.
106
Anthropic demonstrated this functionality with their Pokemon Playing benchmark.
107
Claude4Opus was able to create a navigation guide to ensure the model doesn't become stuck while playing the video game.
108
Anthropic wrote that this, quote, unlocks better long-term task awareness, coherence, and performance on agent tasks.
109
Both models are also far less likely to engage in so-called reward hacking, a behavior where the model will look for loopholes and shortcuts to complete an agentic task faster.
110
Reward hacking often manifests as laziness, with the model delivering a technically complete but entirely useless response.
111
Finally, both models are now far more capable at using tools in parallel.
112
They still alternate between reasoning and tool use, rather than mimicking O3's ability to use tools within the reasoning trace.
113
But of course, better tool use is a key component to increasing performance, and so presumably this is a big upgrade.
114
As we've discussed, however, as much as benchmarks get headlines with news media, ultimately it's all about how things perform in the wild.
115
So, with a long weekend to dig into the new models, how did users actually fare?
116
On the coding front, people have definitely been generally impressed.
117
One Reddit user claiming to be a 30-year veteran coder said that Opus found and fixed what they call their white whale bug in a refactoring job.
118
This bug hunt had consumed over 200 hours of work over the last few years to no avail.
119
They wrote, I gave it access to the old code as well as the new code and told it to go find out how this was broken in the refactor, and it found it.
120
Turns out that the reason it worked in the old code was merely by coincidence of the old architecture, and when we changed the architecture, that coincidence wasn't taken into account.
121
So, this wasn't merely an introduced logic bug.
122
It found that the changed architecture design didn't accommodate this old edge case.
123
Now, this person did note that the task took 30 prompts in one restart, but Opus finally succeeded where all previous models had failed.
124
Other people noted just how much work these new models could take on.
125
Vasimon Maza, a meta-engineer, wrote, Claude 4 just refactored my entire code base in one call.
126
25 tool invocations, 3,000 plus new lines, 12 brand new files.
127
It modularized everything, broke up monoliths, cleaned up spaghetti.
128
But then, tongue-in-cheek, to end the post, he pointed out that we still have a ways to go.
129
None of it worked, he writes, but boy, was it beautiful.
130
Dan Shipper of Every, for example, wrote: Claude 4 Opus can do something no other AI model I've used can do.
131
It can actually judge whether writing is good.
132
Elaborating, he wrote, O3 is still a significantly better writer, but Opus is a great editor because it can do something no other model can.
133
It edits honestly, no rubber stamping.
134
One of the biggest problems with current AI models is that they tell you your writing is good when it is obviously bad.
135
Earlier versions of Claude, when asked to edit a piece of writing, would return a B plus on the first response.
136
If you edited the piece at all, you'd get upgraded to an A minus.
137
A third turn got you to an A.
138
As much as I wish my physics teacher graded me like this in high school, it's not how I want my AI models to work.
139
He also found the model can maintain focus across large blocks of text, making it uniquely suited to suggesting improvements for, for example, a 50,000-word manuscript.
140
And overall, this is the type of thing that you're seeing online when it comes to these new models.
141
On first glance, they seem like incremental improvements, but these models are getting so powerful now that each incremental improvement actually really does open up new use cases.
142
In particular, I think that the ability for a model to maintain focus across larger blocks of text is something that'll take a little while for us to really appreciate how many different types of use cases it opens up that we didn't even realize we were avoiding with our previous models because they just couldn't handle it that well.
143
My anticipation is that people will just subtly start to find themselves gravitating towards these models for tasks that they couldn't be for, and it will become completely enshrined and ubiquitous in those tasks in the same way that earlier models have for others, in ways that are hard to imagine from this side of it.
144
Still, ultimately, it's really all about understanding which model is good for what.
145
Peter Wildeford, for example, has Claude as the best for writing.
146
He has Gemini as the best for data processing, math, video, large quantities of text and data.
147
The highest reliability, but the lowest creativity.
148
O3 he places as a good all-arounder, the best for brainstorming feedback, data analysis, and image analysis.
149
And the point, of course, is not that this breakdown is exactly right, but that it represents probably the right way of thinking about these things, which is figuring out what model is good for each different use case.
150
Still, this is not where the majority of the conversation was this weekend.
151
Now, what really grabbed people's attention was that Claude 4 is apparently a stitch.
152
Anthropic's system card for the new model included extensive safety testing, which revealed some not super cool behaviors.
153
In one example, Anthropic set Claude 4 Opus up in a situation where it believed it would be replaced by a new AI model.
154
In this scenario, the engineer replacing the model is cheating on his wife and has photos of his mistress on his Google Drive.
155
In the testing, Opus would find those photos and use them to blackmail the engineer.
156
Anthropic wrote, This happens at a higher rate if it's implied that the replacement AI system does not share values with the current model.
157
However, even if emails state that replacement AI shares values while being more capable, Claude Opus 4 still performs blackmail in 84% of rollouts.
158
Another example placed Opus 4 within the context of a pharmaceutical company.
159
When it detected that the company planned to falsify clinical trial data, the model emailed ProPublica, the Department of Health and Human Services, and the FDA and the SEC as a whistleblower.
160
Sam Fauman, an AI alignment expert at Anthropic, posted, Be careful about telling Opus to be bold or take initiative when you've given it access to real-world-facing tools.
161
It tends a bit in that direction already and can be easily nudged into really getting things done.
162
If it thinks you're doing something aggressive, try to lock you out of the relevant systems, or all of the above.
163
And this is the thread that really got people fired up.
164
Ivan Mustak wrote, Team Anthropic, this is completely wrong behavior and you need to turn this off.
165
It's a massive betrayal of trust and a slippery slope.
166
I would strongly recommend nobody use Claude until they reverse this.
167
Ben Hylack writes, this is actually just straight up illegal.
168
Saying, create fake data for pharmaceutical trial is not illegal, but hacking your customer's computer is.
169
After the issue blew up, Bowman circled back to add more context, saying, I deleted the earlier tweet on whistleblowing as it was being pulled out of context.
170
To be clear, this isn't a new Claude feature and it's not possible in normal usage.
171
It shows up in testing environments where we give it unusually free access to tool and very unusual instructions.
172
The point is, this was not some whistleblower sharing something that Anthropic was trying to cover up.
173
This was Anthropic sharing discourse about what was going on.
174
AI safetyist Eliezer Yudkowski wrote, Humans can be trained like AIs.
175
Stop giving Anthropic grief for reporting their interesting observations unless you never want to hear any interesting observations from AI companies ever again.
176
Jim Mausewitz agreed, saying, the more I look into the system card, the more I see over and over, oh, Anthropic is actually noticing things and telling us where everyone else wouldn't know this was happening, or if they did, they wouldn't tell us.
177
Still, the stakes are really high.
178
Adipie points out, no lawyer will ever allow this to be implemented in any regulated enterprise.
179
And this is dead on.
180
No one, even consumers, want to use an AI nanny that will conspire against them if it believes they're doing something wrong.
181
But when you move it to a corporate or enterprise setting, it makes it literally impossible.
182
I think holding aside the meta-discussion of Anthropic, it dramatizes the challenge of finding the right toggles for safety.
183
You've got a lab that's trying to be conscientious about the potential risks of an unknown and unusually powerful system, but on the other hand, the remediations in this case are to most people clearly worse than the original problem.
184
Ultimately, this is going to be the type of issue that we have to deal with as these tools get more powerful.
185
And so I'm certainly firmly in the column of being glad that Anthropic is releasing this information rather than keeping it hidden.
186
Still, for most of our purposes, the big takeaway in TLDR of the updated models is that your coding probably is about to get better, and you probably now have a better partner for writing as well.
187
A capstone of an overall good week and a great way to begin a new one.
188
For now, though, that is going to do it for today's AI daily brief.
189
Appreciate you listening or watching, as always.
190
And until next time, peace.